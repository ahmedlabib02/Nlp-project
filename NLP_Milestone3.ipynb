{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyMnw9EYMRvtCAVvsCBCKgcK","include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers\n!pip install evaluate","metadata":{"id":"dgLirjz0Dd0T","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:30:30.886225Z","iopub.execute_input":"2025-05-17T16:30:30.886427Z","iopub.status.idle":"2025-05-17T16:30:39.003870Z","shell.execute_reply.started":"2025-05-17T16:30:30.886408Z","shell.execute_reply":"2025-05-17T16:30:39.002955Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments, Trainer, pipeline\nfrom datasets import DatasetDict\nimport evaluate\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:32:24.611212Z","iopub.execute_input":"2025-05-17T16:32:24.612268Z","iopub.status.idle":"2025-05-17T16:32:24.616093Z","shell.execute_reply.started":"2025-05-17T16:32:24.612241Z","shell.execute_reply":"2025-05-17T16:32:24.615460Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def preprocess(examples, tokenizer: PreTrainedTokenizerBase):\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=256,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized.pop(\"offset_mapping\")\n    start_positions = []\n    end_positions = []\n    for i, offsets in enumerate(offset_mapping):\n        sample_idx = sample_mapping[i]\n        answers = examples[\"answers\"][sample_idx]\n        seq_ids = tokenized.sequence_ids(i)\n        cls_index = 0\n        start_positions.append(cls_index)\n        end_positions.append(cls_index)\n        if len(answers[\"answer_start\"]) == 0:\n            continue\n        start_char = answers[\"answer_start\"][0]\n        end_char = start_char + len(answers[\"text\"][0])\n        context_tokens = [idx for idx, s_id in enumerate(seq_ids) if s_id == 1]\n        if not context_tokens:\n            continue\n        chunk_start_char = offsets[context_tokens[0]][0]\n        chunk_end_char = offsets[context_tokens[-1]][1]\n        if not (chunk_start_char <= start_char and end_char <= chunk_end_char):\n            continue\n        token_start_index = cls_index\n        token_end_index = cls_index\n        for idx in context_tokens:\n            off_start, off_end = offsets[idx]\n            if off_start <= start_char < off_end:\n                token_start_index = idx\n            if off_start < end_char <= off_end:\n                token_end_index = idx\n        start_positions[-1] = token_start_index\n        end_positions[-1] = token_end_index\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:32:38.199537Z","iopub.execute_input":"2025-05-17T16:32:38.199811Z","iopub.status.idle":"2025-05-17T16:32:38.206760Z","shell.execute_reply.started":"2025-05-17T16:32:38.199790Z","shell.execute_reply":"2025-05-17T16:32:38.206102Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def print_metrics(ds, custom_pipeline):\n    outputs = custom_pipeline(question=ds[\"question\"], context=ds[\"context\"])\n    \n    preds = [\n        {\"id\": ex[\"id\"], \"prediction_text\": out[\"answer\"]}\n        for ex, out in zip(ds, outputs)\n    ]\n    refs = [\n        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n        for ex in ds\n    ]\n    \n    results = metric.compute(predictions=preds, references=refs)\n    print(f\"EM: {results['exact_match']:.2f}, F1: {results['f1']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:32:46.298180Z","iopub.execute_input":"2025-05-17T16:32:46.298691Z","iopub.status.idle":"2025-05-17T16:32:46.303333Z","shell.execute_reply.started":"2025-05-17T16:32:46.298666Z","shell.execute_reply":"2025-05-17T16:32:46.302582Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Dataset loading","metadata":{}},{"cell_type":"code","source":"raw = load_dataset(\"squad\")\nsub = raw[\"train\"].shuffle(seed=42).select(range(20000))\nsplit = sub.train_test_split(test_size=0.2, seed=42)\n\ndata = DatasetDict({\n    \"train\":      split[\"train\"],\n    \"validation\": split[\"test\"],\n    \"test\":       raw[\"validation\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:33:05.740973Z","iopub.execute_input":"2025-05-17T16:33:05.741245Z","iopub.status.idle":"2025-05-17T16:33:14.156160Z","shell.execute_reply.started":"2025-05-17T16:33:05.741225Z","shell.execute_reply":"2025-05-17T16:33:14.155514Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1bca8a2cf04c309472aef6e1a6975d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94071ce556114e2081f3975c0bae0e84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a04f0a82e44548bf360c0728c12a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87f46b96f7b4d75a456c156be0f8881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23c9ff760a74c4ca129c0ae53a4cd17"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Experiment 1","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased-distilled-squad\"\ntokenizer  = AutoTokenizer.from_pretrained(model_name)\nmodel_distilbert  = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:33:21.883655Z","iopub.execute_input":"2025-05-17T16:33:21.884003Z","iopub.status.idle":"2025-05-17T16:33:29.017683Z","shell.execute_reply.started":"2025-05-17T16:33:21.883974Z","shell.execute_reply":"2025-05-17T16:33:29.017159Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2d01f786724103a944c725a26e9623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd39e580be6438abeeda025c012e547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1c1d17f10540ce94af4a0632074732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d265022f23d64295a3a69bd93dcd4087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf482c453774447b8070d9a28efcd8a"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"tokenized = data.map(\n    lambda ex: preprocess(ex, tokenizer),\n    batched=True,\n    remove_columns=raw[\"train\"].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:33:35.865890Z","iopub.execute_input":"2025-05-17T16:33:35.866168Z","iopub.status.idle":"2025-05-17T16:33:50.045859Z","shell.execute_reply.started":"2025-05-17T16:33:35.866147Z","shell.execute_reply":"2025-05-17T16:33:50.045099Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd2b503581143959044924bdb6d96ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184158e2ec9a4ef8a5199f8871c0950f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6ab70a9dda405098494d82b74d1c62"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"args = TrainingArguments(\n  output_dir=\"distilbert-qa\",\n  per_device_train_batch_size=128,\n  per_device_eval_batch_size=128,\n  num_train_epochs=10,\n  learning_rate=3e-5,\n  weight_decay=0.01,\n  logging_steps=200,\n  log_level=\"info\",\n  report_to=[\"none\"],       \n  disable_tqdm=False,      \n  fp16=True if torch.cuda.is_available() else False,            \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:34:19.075451Z","iopub.execute_input":"2025-05-17T16:34:19.075983Z","iopub.status.idle":"2025-05-17T16:34:19.103487Z","shell.execute_reply.started":"2025-05-17T16:34:19.075959Z","shell.execute_reply":"2025-05-17T16:34:19.103000Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"trainer_full = Trainer(\n  model=model_distilbert,\n  args=args,\n  train_dataset=tokenized[\"train\"],\n  eval_dataset =tokenized[\"validation\"],\n  processing_class=tokenizer,\n  compute_metrics=compute_metrics\n)\ntrainer_full.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:10:48.548894Z","iopub.execute_input":"2025-05-17T15:10:48.549165Z","iopub.status.idle":"2025-05-17T15:10:53.056470Z","shell.execute_reply.started":"2025-05-17T15:10:48.549144Z","shell.execute_reply":"2025-05-17T15:10:53.055299Z"}},"outputs":[{"name":"stderr","text":"Using auto half precision backend\n***** Running training *****\n  Num examples = 17,751\n  Num Epochs = 10\n  Instantaneous batch size per device = 128\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1,390\n  Number of trainable parameters = 66,364,418\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1390 : < :, Epoch 0.01/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3679976342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2563\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m                     ):\n\u001b[1;32m   2567\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"metric = evaluate.load(\"squad\")\nqa = pipeline(\n    \"question-answering\",\n    model=\"/kaggle/working/distilbert-qa/checkpoint-1740\",\n    tokenizer=\"/kaggle/working/distilbert-qa/checkpoint-1740\",\n    device=0,\n)\n\nds = data[\"test\"]\n\nprint_metrics(ds, qa)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:42:10.151221Z","iopub.execute_input":"2025-05-17T15:42:10.151540Z","iopub.status.idle":"2025-05-17T15:43:37.698825Z","shell.execute_reply.started":"2025-05-17T15:42:10.151517Z","shell.execute_reply":"2025-05-17T15:43:37.698014Z"}},"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/working/distilbert-qa/checkpoint-1740/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForQuestionAnswering\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"vocab_size\": 30522\n}\n\nloading configuration file /kaggle/working/distilbert-qa/checkpoint-1740/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForQuestionAnswering\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"vocab_size\": 30522\n}\n\nloading weights file /kaggle/working/distilbert-qa/checkpoint-1740/model.safetensors\nAll model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n\nAll the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at /kaggle/working/distilbert-qa/checkpoint-1740.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\nloading file vocab.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"EM: 75.20, F1: 84.17\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Experiment 2","metadata":{}},{"cell_type":"code","source":"model_partial = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:45:49.637288Z","iopub.execute_input":"2025-05-17T16:45:49.637579Z","iopub.status.idle":"2025-05-17T16:45:50.270567Z","shell.execute_reply.started":"2025-05-17T16:45:49.637557Z","shell.execute_reply":"2025-05-17T16:45:50.270063Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"#### Partial fine tuning","metadata":{}},{"cell_type":"code","source":"for param in model_partial.distilbert.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:45:53.679003Z","iopub.execute_input":"2025-05-17T16:45:53.679533Z","iopub.status.idle":"2025-05-17T16:45:53.683317Z","shell.execute_reply.started":"2025-05-17T16:45:53.679512Z","shell.execute_reply":"2025-05-17T16:45:53.682475Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"args_partial = TrainingArguments(\n    output_dir=\"distilbert-qa-partial\",\n    per_device_train_batch_size=128,\n    per_device_eval_batch_size=128,\n    num_train_epochs=3,\n    learning_rate=5e-4,  \n    weight_decay=0.01,\n    logging_steps=200,\n    log_level=\"info\",\n    report_to=[\"none\"],\n    disable_tqdm=False,\n    fp16=True if torch.cuda.is_available() else False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:46:09.355558Z","iopub.execute_input":"2025-05-17T16:46:09.356257Z","iopub.status.idle":"2025-05-17T16:46:09.382549Z","shell.execute_reply.started":"2025-05-17T16:46:09.356232Z","shell.execute_reply":"2025-05-17T16:46:09.381861Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer_partial = Trainer(\n    model=model_partial,\n    args=args_partial,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:46:37.906374Z","iopub.execute_input":"2025-05-17T16:46:37.907180Z","iopub.status.idle":"2025-05-17T16:46:38.165089Z","shell.execute_reply.started":"2025-05-17T16:46:37.907145Z","shell.execute_reply":"2025-05-17T16:46:38.164551Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1290572945.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer_partial = Trainer(\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"trainer_partial.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:46:51.216086Z","iopub.execute_input":"2025-05-17T16:46:51.216761Z","iopub.status.idle":"2025-05-17T16:50:20.776240Z","shell.execute_reply.started":"2025-05-17T16:46:51.216740Z","shell.execute_reply":"2025-05-17T16:50:20.775676Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 17,751\n  Num Epochs = 3\n  Instantaneous batch size per device = 128\n  Total train batch size (w. parallel, distributed & accumulation) = 128\n  Gradient Accumulation steps = 1\n  Total optimization steps = 417\n  Number of trainable parameters = 1,538\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='417' max='417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [417/417 03:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.805900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.751800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to distilbert-qa-partial/checkpoint-417\nConfiguration saved in distilbert-qa-partial/checkpoint-417/config.json\nModel weights saved in distilbert-qa-partial/checkpoint-417/model.safetensors\ntokenizer config file saved in distilbert-qa-partial/checkpoint-417/tokenizer_config.json\nSpecial tokens file saved in distilbert-qa-partial/checkpoint-417/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=417, training_loss=0.7777368284815507, metrics={'train_runtime': 209.1538, 'train_samples_per_second': 254.612, 'train_steps_per_second': 1.994, 'total_flos': 3478834768002048.0, 'train_loss': 0.7777368284815507, 'epoch': 3.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"metric = evaluate.load(\"squad\")\nqa_partial = pipeline(\n    \"question-answering\",\n    model=\"distilbert-qa-partial/checkpoint-417\",  \n    tokenizer=\"distilbert-qa-partial/checkpoint-417\",\n    device=0 if torch.cuda.is_available() else -1,\n)\n\n\nprint_metrics(data[\"test\"], qa_partial)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:53:27.073133Z","iopub.execute_input":"2025-05-17T16:53:27.073406Z","iopub.status.idle":"2025-05-17T16:54:52.265074Z","shell.execute_reply.started":"2025-05-17T16:53:27.073386Z","shell.execute_reply":"2025-05-17T16:54:52.264355Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08297794de7d4fa0bb6acd6d897fbd36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40edeac0d76142b2821b2f52ea32e11a"}},"metadata":{}},{"name":"stderr","text":"loading configuration file distilbert-qa-partial/checkpoint-417/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForQuestionAnswering\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"vocab_size\": 30522\n}\n\nloading configuration file distilbert-qa-partial/checkpoint-417/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForQuestionAnswering\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"vocab_size\": 30522\n}\n\nloading weights file distilbert-qa-partial/checkpoint-417/model.safetensors\nAll model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n\nAll the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-qa-partial/checkpoint-417.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\nloading file vocab.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"EM: 78.85, F1: 86.57\n","output_type":"stream"}],"execution_count":17}]}