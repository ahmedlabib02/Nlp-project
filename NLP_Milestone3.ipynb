{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyMnw9EYMRvtCAVvsCBCKgcK","include_colab_link":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers\n!pip install evaluate","metadata":{"id":"dgLirjz0Dd0T","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu \n!pip install langchain\n!pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:16:02.907215Z","iopub.execute_input":"2025-05-17T20:16:02.907535Z","iopub.status.idle":"2025-05-17T20:16:18.507982Z","shell.execute_reply.started":"2025-05-17T20:16:02.907514Z","shell.execute_reply":"2025-05-17T20:16:18.507253Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain-core<1.0.0,>=0.3.59 (from langchain_community)\n  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain<1.0.0,>=0.3.25 (from langchain_community)\n  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.18)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain_community)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.50\n    Uninstalling langchain-core-0.3.50:\n      Successfully uninstalled langchain-core-0.3.50\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.7\n    Uninstalling langchain-text-splitters-0.3.7:\n      Successfully uninstalled langchain-text-splitters-0.3.7\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.22\n    Uninstalling langchain-0.3.22:\n      Successfully uninstalled langchain-0.3.22\nSuccessfully installed httpx-sse-0.4.0 langchain-0.3.25 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langchain_community-0.3.24 pydantic-settings-2.9.1 python-dotenv-1.1.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers import AutoTokenizer, DPRContextEncoder,DPRContextEncoderTokenizerFast, AutoModelForQuestionAnswering\nfrom transformers import TrainingArguments, Trainer, pipeline\nfrom transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, pipeline\n\nfrom datasets import DatasetDict\nimport evaluate\nimport torch\nimport faiss\nfrom datasets import Dataset\nfrom langchain.schema    import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings   import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.llms         import HuggingFacePipeline\nfrom langchain.chains       import RetrievalQA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:16:28.014804Z","iopub.execute_input":"2025-05-17T20:16:28.015137Z","iopub.status.idle":"2025-05-17T20:16:28.131934Z","shell.execute_reply.started":"2025-05-17T20:16:28.015097Z","shell.execute_reply":"2025-05-17T20:16:28.131193Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def preprocess(examples, tokenizer: PreTrainedTokenizerBase):\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=256,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized.pop(\"offset_mapping\")\n    start_positions = []\n    end_positions = []\n    for i, offsets in enumerate(offset_mapping):\n        sample_idx = sample_mapping[i]\n        answers = examples[\"answers\"][sample_idx]\n        seq_ids = tokenized.sequence_ids(i)\n        cls_index = 0\n        start_positions.append(cls_index)\n        end_positions.append(cls_index)\n        if len(answers[\"answer_start\"]) == 0:\n            continue\n        start_char = answers[\"answer_start\"][0]\n        end_char = start_char + len(answers[\"text\"][0])\n        context_tokens = [idx for idx, s_id in enumerate(seq_ids) if s_id == 1]\n        if not context_tokens:\n            continue\n        chunk_start_char = offsets[context_tokens[0]][0]\n        chunk_end_char = offsets[context_tokens[-1]][1]\n        if not (chunk_start_char <= start_char and end_char <= chunk_end_char):\n            continue\n        token_start_index = cls_index\n        token_end_index = cls_index\n        for idx in context_tokens:\n            off_start, off_end = offsets[idx]\n            if off_start <= start_char < off_end:\n                token_start_index = idx\n            if off_start < end_char <= off_end:\n                token_end_index = idx\n        start_positions[-1] = token_start_index\n        end_positions[-1] = token_end_index\n    tokenized[\"start_positions\"] = start_positions\n    tokenized[\"end_positions\"] = end_positions\n    return tokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:42:10.942410Z","iopub.execute_input":"2025-05-17T19:42:10.942991Z","iopub.status.idle":"2025-05-17T19:42:10.950103Z","shell.execute_reply.started":"2025-05-17T19:42:10.942967Z","shell.execute_reply":"2025-05-17T19:42:10.949460Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def print_metrics(ds, custom_pipeline):\n    outputs = custom_pipeline(question=ds[\"question\"], context=ds[\"context\"])\n    \n    preds = [\n        {\"id\": ex[\"id\"], \"prediction_text\": out[\"answer\"]}\n        for ex, out in zip(ds, outputs)\n    ]\n    refs = [\n        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n        for ex in ds\n    ]\n    \n    results = metric.compute(predictions=preds, references=refs)\n    print(f\"EM: {results['exact_match']:.2f}, F1: {results['f1']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:42:10.679578Z","iopub.execute_input":"2025-05-17T19:42:10.680153Z","iopub.status.idle":"2025-05-17T19:42:10.684628Z","shell.execute_reply.started":"2025-05-17T19:42:10.680105Z","shell.execute_reply":"2025-05-17T19:42:10.683872Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Dataset loading","metadata":{}},{"cell_type":"code","source":"raw = load_dataset(\"squad\")\nsub = raw[\"train\"].shuffle(seed=42).select(range(20000))\nsplit = sub.train_test_split(test_size=0.2, seed=42)\n\ndata = DatasetDict({\n    \"train\":      split[\"train\"],\n    \"validation\": split[\"test\"],\n    \"test\":       raw[\"validation\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T19:42:03.915611Z","iopub.execute_input":"2025-05-17T19:42:03.916279Z","iopub.status.idle":"2025-05-17T19:42:10.678361Z","shell.execute_reply.started":"2025-05-17T19:42:03.916254Z","shell.execute_reply":"2025-05-17T19:42:10.677758Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Experiment 1","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased-distilled-squad\"\ntokenizer  = AutoTokenizer.from_pretrained(model_name)\nmodel_distilbert  = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized = data.map(\n    lambda ex: preprocess(ex, tokenizer),\n    batched=True,\n    remove_columns=raw[\"train\"].column_names\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args = TrainingArguments(\n  output_dir=\"distilbert-qa\",\n  per_device_train_batch_size=128,\n  per_device_eval_batch_size=128,\n  num_train_epochs=10,\n  learning_rate=3e-5,\n  weight_decay=0.01,\n  logging_steps=200,\n  log_level=\"info\",\n  report_to=[\"none\"],       \n  disable_tqdm=False,      \n  fp16=True if torch.cuda.is_available() else False,            \n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_full = Trainer(\n  model=model_distilbert,\n  args=args,\n  train_dataset=tokenized[\"train\"],\n  eval_dataset =tokenized[\"validation\"],\n  processing_class=tokenizer,\n  compute_metrics=compute_metrics\n)\ntrainer_full.train()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric = evaluate.load(\"squad\")\nqa = pipeline(\n    \"question-answering\",\n    model=\"/kaggle/working/distilbert-qa/checkpoint-1740\",\n    tokenizer=\"/kaggle/working/distilbert-qa/checkpoint-1740\",\n    device=0,\n)\n\nds = data[\"test\"]\n\nprint_metrics(ds, qa)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Experiment 2","metadata":{}},{"cell_type":"code","source":"model_partial = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Partial fine tuning","metadata":{}},{"cell_type":"code","source":"for param in model_partial.distilbert.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args_partial = TrainingArguments(\n    output_dir=\"distilbert-qa-partial\",\n    per_device_train_batch_size=128,\n    per_device_eval_batch_size=128,\n    num_train_epochs=3,\n    learning_rate=5e-4,  \n    weight_decay=0.01,\n    logging_steps=200,\n    log_level=\"info\",\n    report_to=[\"none\"],\n    disable_tqdm=False,\n    fp16=True if torch.cuda.is_available() else False,\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_partial = Trainer(\n    model=model_partial,\n    args=args_partial,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_partial.train()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric = evaluate.load(\"squad\")\nqa_partial = pipeline(\n    \"question-answering\",\n    model=\"distilbert-qa-partial/checkpoint-417\",  \n    tokenizer=\"distilbert-qa-partial/checkpoint-417\",\n    device=0 if torch.cuda.is_available() else -1,\n)\n\n\nprint_metrics(data[\"test\"], qa_partial)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Experiment 3","metadata":{}},{"cell_type":"code","source":"contexts = raw[\"train\"][\"context\"] + raw[\"validation\"][\"context\"]\nunique   = list(dict.fromkeys(contexts))\ndocs     = [Document(page_content=c) for c in unique]\n\n\nsplitter    = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nsplit_docs  = splitter.split_documents(docs)\n\n\nembeddings = HuggingFaceEmbeddings()                    \ndb         = FAISS.from_documents(split_docs, embeddings)\nretriever  = db.as_retriever(search_kwargs={\"k\": 4})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:18:04.630249Z","iopub.execute_input":"2025-05-17T20:18:04.631106Z","iopub.status.idle":"2025-05-17T20:20:40.849529Z","shell.execute_reply.started":"2025-05-17T20:18:04.631079Z","shell.execute_reply":"2025-05-17T20:20:40.848879Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_124/3400411356.py:10: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n  embeddings = HuggingFaceEmbeddings()\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:20:58.614740Z","iopub.execute_input":"2025-05-17T20:20:58.615610Z","iopub.status.idle":"2025-05-17T20:20:59.442383Z","shell.execute_reply.started":"2025-05-17T20:20:58.615581Z","shell.execute_reply":"2025-05-17T20:20:59.441745Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizerFast'.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"qa_pipe = pipeline(\n    \"question-answering\",\n    model=\"distilbert-base-uncased-distilled-squad\",  \n    tokenizer=\"distilbert-base-uncased-distilled-squad\",\n)\n\ndef answer_with_distilbert(query: str, docs: list[Document]) -> str:\n    context = \" \".join(d.page_content for d in docs)\n    out = qa_pipe(question=query, context=context)\n    return out[\"answer\"]\n\n\nretrieved_docs = retriever.get_relevant_documents(\"Who is Thomas Hardy?\")\nprint(answer_with_distilbert(\"Who is Thomas Hardy?\", retrieved_docs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:22:15.211895Z","iopub.execute_input":"2025-05-17T20:22:15.212258Z","iopub.status.idle":"2025-05-17T20:22:15.660207Z","shell.execute_reply.started":"2025-05-17T20:22:15.212232Z","shell.execute_reply":"2025-05-17T20:22:15.659386Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"novelists and poets\n","output_type":"stream"}],"execution_count":30}]}