{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11489407,"sourceType":"datasetVersion","datasetId":7201839},{"sourceId":11500742,"sourceType":"datasetVersion","datasetId":7210129}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Milestone 2","metadata":{"id":"qNMvLH9BB7pR"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pandas as pd\nimport os, zipfile , json , random, requests\nimport re\nfrom pathlib import Path\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import  Sequential\nimport string","metadata":{"id":"kFomRI3xB9d8","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:32:54.749821Z","iopub.execute_input":"2025-04-22T20:32:54.750037Z","iopub.status.idle":"2025-04-22T20:33:08.777295Z","shell.execute_reply.started":"2025-04-22T20:32:54.750009Z","shell.execute_reply":"2025-04-22T20:33:08.776516Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 20:32:56.291554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745353976.473503      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745353976.524668      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Explorting dataset:","metadata":{"id":"1zzFTlcTCEc7"}},{"cell_type":"code","source":"\ndef is_kaggle():\n    # Kaggle kernels always set this env var\n    return 'KAGGLE_URL_BASE' in os.environ\n\ndef is_colab():\n    return (not is_kaggle()) and os.path.exists('/content')\n\ndef maybe_mount_drive():\n    if is_colab():\n        from google.colab import drive\n        if not os.path.isdir('/content/drive'):\n            drive.mount('/content/drive')\n\ndef get_data_path():\n    if is_kaggle():\n        return '/kaggle/input/squad-2-0/'\n    elif is_colab():\n        return '/content/drive/MyDrive/SQuAD'\n    else:\n        return './data/'\ndef get_model_dir():\n    if is_colab():\n        model_dir = '/content/drive/MyDrive/models'\n    elif is_kaggle():\n        model_dir = '/kaggle/working/models'\n    else:\n        model_dir = './models'\n    os.makedirs(model_dir, exist_ok=True)\n    return model_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:19.428789Z","iopub.execute_input":"2025-04-22T20:33:19.429438Z","iopub.status.idle":"2025-04-22T20:33:19.434723Z","shell.execute_reply.started":"2025-04-22T20:33:19.429413Z","shell.execute_reply":"2025-04-22T20:33:19.434011Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset_dir = get_data_path()\nmaybe_mount_drive()\nos.makedirs(dataset_dir, exist_ok=True)","metadata":{"id":"e0NuNGE435ad","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:22.521635Z","iopub.execute_input":"2025-04-22T20:33:22.522262Z","iopub.status.idle":"2025-04-22T20:33:22.526587Z","shell.execute_reply.started":"2025-04-22T20:33:22.522236Z","shell.execute_reply":"2025-04-22T20:33:22.525970Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"file_path = os.path.join(dataset_dir, 'train-v2.0.json')","metadata":{"id":"TSDZT50PX9sj","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:25.468953Z","iopub.execute_input":"2025-04-22T20:33:25.469246Z","iopub.status.idle":"2025-04-22T20:33:25.472906Z","shell.execute_reply.started":"2025-04-22T20:33:25.469225Z","shell.execute_reply":"2025-04-22T20:33:25.472071Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"with open(file_path, 'r', encoding='utf-8') as f:\n    squad = json.load(f)","metadata":{"id":"s_tEP-KyHMFT","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:27.535167Z","iopub.execute_input":"2025-04-22T20:33:27.535438Z","iopub.status.idle":"2025-04-22T20:33:29.081729Z","shell.execute_reply.started":"2025-04-22T20:33:27.535420Z","shell.execute_reply":"2025-04-22T20:33:29.081181Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"records = []\nfor article in squad['data']:\n    for para in article['paragraphs']:\n        ctx = para['context']\n        for qa in para['qas']:\n            answers = [a['text'] for a in qa.get('answers', [])]\n            starts  = [a['answer_start'] for a in qa.get('answers', [])]\n            ends    = [s + len(t) for s,t in zip(starts, answers)]\n            records.append({\n                'question': qa['question'],\n                'answers': answers,\n                'context': ctx,\n                'answer_start': starts,\n                'answer_end': ends\n            })\n\n","metadata":{"id":"JixYG4s6saGU","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:31.723690Z","iopub.execute_input":"2025-04-22T20:33:31.724298Z","iopub.status.idle":"2025-04-22T20:33:32.804167Z","shell.execute_reply.started":"2025-04-22T20:33:31.724274Z","shell.execute_reply":"2025-04-22T20:33:32.803586Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df = pd.DataFrame(records)\ndf.head()","metadata":{"id":"NhZlSX4qtmtj","outputId":"96567ffd-6526-4218-c322-84c26cd2e7e6","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:35.641512Z","iopub.execute_input":"2025-04-22T20:33:35.641803Z","iopub.status.idle":"2025-04-22T20:33:35.780102Z","shell.execute_reply.started":"2025-04-22T20:33:35.641781Z","shell.execute_reply":"2025-04-22T20:33:35.779382Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                            question                answers  \\\n0           When did Beyonce start becoming popular?    [in the late 1990s]   \n1  What areas did Beyonce compete in when she was...  [singing and dancing]   \n2  When did Beyonce leave Destiny's Child and bec...                 [2003]   \n3      In what city and state did Beyonce  grow up?        [Houston, Texas]   \n4         In which decade did Beyonce become famous?           [late 1990s]   \n\n                                             context answer_start answer_end  \n0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [269]      [286]  \n1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [207]      [226]  \n2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [526]      [530]  \n3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [166]      [180]  \n4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [276]      [286]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When did Beyonce start becoming popular?</td>\n      <td>[in the late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[269]</td>\n      <td>[286]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What areas did Beyonce compete in when she was...</td>\n      <td>[singing and dancing]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[207]</td>\n      <td>[226]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When did Beyonce leave Destiny's Child and bec...</td>\n      <td>[2003]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[526]</td>\n      <td>[530]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In what city and state did Beyonce  grow up?</td>\n      <td>[Houston, Texas]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[166]</td>\n      <td>[180]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In which decade did Beyonce become famous?</td>\n      <td>[late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[276]</td>\n      <td>[286]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(\"Total QA pairs:\", len(df))","metadata":{"id":"D72WTTHOupEF","outputId":"6686b957-2f37-44bc-c831-47864e7260dd","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:33:38.386950Z","iopub.execute_input":"2025-04-22T20:33:38.387474Z","iopub.status.idle":"2025-04-22T20:33:38.391419Z","shell.execute_reply.started":"2025-04-22T20:33:38.387450Z","shell.execute_reply":"2025-04-22T20:33:38.390661Z"}},"outputs":[{"name":"stdout","text":"Total QA pairs: 130319\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#shuffling\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# only working on subset of 15k row\ndf_subset = df.head(15000).copy().reset_index(drop=True)\n\nprint(\"Subset size:\", df_subset.shape)\ndf_subset.head()","metadata":{"id":"rSzBSJjsx1MU","outputId":"97a3a6d3-c217-4ab8-a1b7-edd234dca397","colab":{"base_uri":"https://localhost:8080/","height":310},"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:38:54.872612Z","iopub.execute_input":"2025-04-22T20:38:54.873147Z","iopub.status.idle":"2025-04-22T20:38:54.956528Z","shell.execute_reply.started":"2025-04-22T20:38:54.873111Z","shell.execute_reply":"2025-04-22T20:38:54.955794Z"}},"outputs":[{"name":"stdout","text":"Subset size: (15000, 5)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  What year did the global recession that follow...   \n1  what was a popular club in ibiza that started ...   \n2  In what century did Martin Luther honor Mary a...   \n3                          What is the climate like?   \n4        How many times has the Queen toured Canada?   \n\n                                   answers  \\\n0                                   [2012]   \n1                                [Amnesia]   \n2                                       []   \n3  [varies from hot and subhumid tropical]   \n4                                       []   \n\n                                             context answer_start answer_end  \n0  It threatened the collapse of large financial ...        [481]      [485]  \n1  But house was also being developed on Ibiza,[c...        [251]      [258]  \n2  Although Calvin and Huldrych Zwingli honored M...           []         []  \n3  Due to extreme variation in elevation, great v...        [115]      [152]  \n4  The Queen addressed the United Nations for a s...           []         []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What year did the global recession that follow...</td>\n      <td>[2012]</td>\n      <td>It threatened the collapse of large financial ...</td>\n      <td>[481]</td>\n      <td>[485]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what was a popular club in ibiza that started ...</td>\n      <td>[Amnesia]</td>\n      <td>But house was also being developed on Ibiza,[c...</td>\n      <td>[251]</td>\n      <td>[258]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In what century did Martin Luther honor Mary a...</td>\n      <td>[]</td>\n      <td>Although Calvin and Huldrych Zwingli honored M...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the climate like?</td>\n      <td>[varies from hot and subhumid tropical]</td>\n      <td>Due to extreme variation in elevation, great v...</td>\n      <td>[115]</td>\n      <td>[152]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How many times has the Queen toured Canada?</td>\n      <td>[]</td>\n      <td>The Queen addressed the United Nations for a s...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"id":"-EdBrPmqbBvA"}},{"cell_type":"markdown","source":"Dropping rows where answers are empty","metadata":{"id":"E_Cb25_x11aT"}},{"cell_type":"code","source":"df_subset = df_subset[df_subset['answers'].map(len) > 0].reset_index(drop=True)\nprint(\"Rows remaining after drop:\", len(df_subset))","metadata":{"id":"JCoQdBYJ19ON","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98bfb707-f5f5-4339-8879-dd7c8426a6d9","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:41.567874Z","iopub.execute_input":"2025-04-22T20:40:41.568567Z","iopub.status.idle":"2025-04-22T20:40:41.584082Z","shell.execute_reply.started":"2025-04-22T20:40:41.568540Z","shell.execute_reply":"2025-04-22T20:40:41.583410Z"}},"outputs":[{"name":"stdout","text":"Rows remaining after drop: 10020\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Removing Extra Whitespaces","metadata":{"id":"QBSLXMvASpe0"}},{"cell_type":"code","source":"def collapse_whitespace(s):\n    if isinstance(s, str):\n        return re.sub(r'\\s+', ' ', s.strip())\n    return s","metadata":{"id":"FZAnlrcyStOE","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:46.951304Z","iopub.execute_input":"2025-04-22T20:40:46.951590Z","iopub.status.idle":"2025-04-22T20:40:46.955306Z","shell.execute_reply.started":"2025-04-22T20:40:46.951568Z","shell.execute_reply":"2025-04-22T20:40:46.954599Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for col in ['question', 'context', 'answers']:\n    if col in df_subset.columns:\n        df_subset[col] = df_subset[col].apply(collapse_whitespace)","metadata":{"id":"fnVh34G5VBAm","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:48.181186Z","iopub.execute_input":"2025-04-22T20:40:48.181476Z","iopub.status.idle":"2025-04-22T20:40:48.652360Z","shell.execute_reply.started":"2025-04-22T20:40:48.181458Z","shell.execute_reply":"2025-04-22T20:40:48.651603Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Lets explore the length of the sequences which will determine some hyperparameters in training the models**","metadata":{"id":"JFeXiO2dzf8Q"}},{"cell_type":"code","source":"df_subset['question'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc530Af3zoTV","outputId":"f93ee1cc-11ca-4548-ea54-98ddafec6e05","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:51.086764Z","iopub.execute_input":"2025-04-22T20:40:51.087498Z","iopub.status.idle":"2025-04-22T20:40:51.096761Z","shell.execute_reply.started":"2025-04-22T20:40:51.087472Z","shell.execute_reply":"2025-04-22T20:40:51.096228Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"203"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df_subset['context'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDvLl2vx0a4o","outputId":"103fac31-6acf-45c6-cf69-bb1656c1fe4c","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:52.275017Z","iopub.execute_input":"2025-04-22T20:40:52.275304Z","iopub.status.idle":"2025-04-22T20:40:52.284687Z","shell.execute_reply.started":"2025-04-22T20:40:52.275286Z","shell.execute_reply":"2025-04-22T20:40:52.284147Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"3706"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"We just turn the array of the answers to a string since none have multiple answers","metadata":{"id":"6hWOoWFg1GZk"}},{"cell_type":"code","source":"df_subset['answer_text']  = df_subset['answers']  # you already collapsed to the first answer string\ndf_subset['answer_start'] = df_subset['answer_start'].apply(lambda lst: lst[0])\ndf_subset['answer_end']   = df_subset['answer_end'].apply(lambda lst: lst[0])","metadata":{"id":"rh00m5Jw0lWm","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:54.228865Z","iopub.execute_input":"2025-04-22T20:40:54.229330Z","iopub.status.idle":"2025-04-22T20:40:54.238863Z","shell.execute_reply.started":"2025-04-22T20:40:54.229307Z","shell.execute_reply":"2025-04-22T20:40:54.238316Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df_subset['answers'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoL1ZTTx0z8b","outputId":"e4736878-4cb3-4c68-df14-41efa24a60e8","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:40:55.771479Z","iopub.execute_input":"2025-04-22T20:40:55.771961Z","iopub.status.idle":"2025-04-22T20:40:55.781540Z","shell.execute_reply.started":"2025-04-22T20:40:55.771937Z","shell.execute_reply":"2025-04-22T20:40:55.780677Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"202"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Embeddings","metadata":{"id":"B_-pDASk5eOa"}},{"cell_type":"code","source":"df_subset['answer_text']  = df_subset['answers']  # you already collapsed to the first answer string\ndf_subset['answer_start'] = df_subset['answer_start'].apply(lambda lst: lst[0])\ndf_subset['answer_end']   = df_subset['answer_end'].apply(lambda lst: lst[0])","metadata":{"id":"gBEVZxrJ5gav","outputId":"e51f32d7-1a78-4015-9a21-d4a580224374","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:05:24.888560Z","iopub.execute_input":"2025-04-22T21:05:24.888844Z","iopub.status.idle":"2025-04-22T21:05:24.908902Z","shell.execute_reply.started":"2025-04-22T21:05:24.888824Z","shell.execute_reply":"2025-04-22T21:05:24.908289Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def whitespace_tokenize_with_spans(text):\n    tokens, spans = [], []\n    for m in re.finditer(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE):\n        tokens.append(m.group())\n        spans.append((m.start(), m.end()))\n    return tokens, spans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:05:44.703535Z","iopub.execute_input":"2025-04-22T21:05:44.703827Z","iopub.status.idle":"2025-04-22T21:05:44.708422Z","shell.execute_reply.started":"2025-04-22T21:05:44.703807Z","shell.execute_reply":"2025-04-22T21:05:44.707653Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"all_texts = df_subset['question'].tolist() + df_subset['context'].tolist()\ntokenizer = Tokenizer(oov_token='[UNK]')\ntokenizer.fit_on_texts(all_texts)\nvocab_size = len(tokenizer.word_index) + 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:06:15.871532Z","iopub.execute_input":"2025-04-22T21:06:15.871812Z","iopub.status.idle":"2025-04-22T21:06:16.942215Z","shell.execute_reply.started":"2025-04-22T21:06:15.871785Z","shell.execute_reply":"2025-04-22T21:06:16.941646Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"max_len = 384\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:06:58.379677Z","iopub.execute_input":"2025-04-22T21:06:58.380367Z","iopub.status.idle":"2025-04-22T21:06:58.383503Z","shell.execute_reply.started":"2025-04-22T21:06:58.380343Z","shell.execute_reply":"2025-04-22T21:06:58.382782Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"input_ids_list, attn_list = [], []\nstart_positions, end_positions = [], []\n\nfor _, row in df_subset.iterrows():\n    q, c = row['question'], row['context']\n    ans_s, ans_e = row['answer_start'], row['answer_end']\n    \n    # tokenize\n    q_tokens, _     = whitespace_tokenize_with_spans(q)\n    c_tokens, c_spans= whitespace_tokenize_with_spans(c)\n    \n    # full sequence: Q + [SEP] + C\n    seq_tokens = q_tokens + ['[SEP]'] + c_tokens\n    \n    # to IDs + pad/truncate\n    seq_ids = [tokenizer.word_index.get(t, tokenizer.word_index['[UNK]']) \n               for t in seq_tokens]\n    seq_ids = seq_ids[:max_len]\n    attn   = [1]*len(seq_ids)\n    pad_len = max_len - len(seq_ids)\n    seq_ids += [0]*pad_len\n    attn   += [0]*pad_len\n    \n    # find which context token covers the answer span\n    start_tok = next((i for i,(s,e) in enumerate(c_spans) if s <= ans_s < e), 0)\n    end_tok   = next((i for i,(s,e) in enumerate(c_spans) if s < ans_e <= e), start_tok)\n    \n    sep_off = len(q_tokens) + 1\n    start_pos = min(sep_off + start_tok, max_len-1)\n    end_pos   = min(sep_off + end_tok,   max_len-1)\n    \n    input_ids_list.append(seq_ids)\n    attn_list.append(attn)\n    start_positions.append(start_pos)\n    end_positions.append(end_pos)\n\n# 5) Convert to numpy arrays\ninput_ids_np = np.array(input_ids_list, dtype=np.int32)\nattn_np      = np.array(attn_list,      dtype=np.int32)\nstart_np     = np.array(start_positions, dtype=np.int32)\nend_np       = np.array(end_positions,   dtype=np.int32)\n\n# 6) Build a tf.data.Dataset and split 90/10\nds = tf.data.Dataset.from_tensor_slices(\n    ({\"input_ids\": input_ids_np, \"attention_mask\": attn_np},\n     (start_np, end_np))\n)\nds = ds.shuffle(len(df_subset), seed=42)\ntrain_size = int(0.9 * len(df_subset))\n\ntrain_ds = ds.take(train_size).batch(16)\nval_ds   = ds.skip(train_size).batch(16)\n\nprint(train_ds, val_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:07:02.291574Z","iopub.execute_input":"2025-04-22T21:07:02.291811Z","iopub.status.idle":"2025-04-22T21:07:05.676709Z","shell.execute_reply.started":"2025-04-22T21:07:02.291795Z","shell.execute_reply":"2025-04-22T21:07:05.676089Z"}},"outputs":[{"name":"stdout","text":"<_BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 384), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 384), dtype=tf.int32, name=None)}, (TensorSpec(shape=(None,), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None)))> <_BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 384), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 384), dtype=tf.int32, name=None)}, (TensorSpec(shape=(None,), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None)))>\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from tensorflow.keras.layers import Lambda, Reshape\n\ndef encoder_block(embed_dim, num_heads, ff_dim):\n    x_in = Input(shape=(None, embed_dim))\n    mask = Input(shape=(None,), dtype=\"int32\")\n\n    # expand mask to (batch, 1, 1, seq_len) and cast via a Lambda layer\n    mask_float = Lambda(\n        lambda m: tf.cast(tf.expand_dims(tf.expand_dims(m, axis=1), axis=1), \"float32\"),\n        name=\"mask_expand\"\n    )(mask)\n\n    attn = MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_dim\n    )(x_in, x_in, attention_mask=mask_float)\n\n    x = LayerNormalization(epsilon=1e-6)(x_in + attn)\n    proj = Dense(ff_dim, activation=\"relu\")(x)\n    proj = Dense(embed_dim)(proj)\n    out  = LayerNormalization(epsilon=1e-6)(x + proj)\n\n    return Model([x_in, mask], out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:08:20.432421Z","iopub.execute_input":"2025-04-22T21:08:20.432931Z","iopub.status.idle":"2025-04-22T21:08:20.438376Z","shell.execute_reply.started":"2025-04-22T21:08:20.432908Z","shell.execute_reply":"2025-04-22T21:08:20.437805Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# def truncate_context(context: str, ans_start: int, ans_end: int, max_len: int) -> str:\n#     \"\"\"\n#     Return a substring of `context` of length up to max_len characters,\n#     centered on the character span [ans_start:ans_end], adjusted to word boundaries.\n#     \"\"\"\n#     ans_len = ans_end - ans_start\n#     extra   = max_len - ans_len\n#     pre     = extra // 2\n#     post    = extra - pre\n\n#     # ideal window\n#     start = ans_start - pre\n#     end   = ans_end   + post\n\n#     # shift if off left edge\n#     if start < 0:\n#         start = 0\n#         end   = min(max_len, len(context))\n\n#     # shift if off right edge\n#     if end > len(context):\n#         end   = len(context)\n#         start = max(0, len(context) - max_len)\n\n#     # adjust start backward to nearest whitespace to avoid cutting a word\n#     if start > 0 and not context[start].isspace():\n#         m = re.search(r'\\s', context[:start][::-1])\n#         if m:\n#             # position of last whitespace before start\n#             start = start - m.start()\n\n#     # adjust end forward to nearest whitespace to avoid cutting a word\n#     if end < len(context) and not context[end].isspace():\n#         m = re.search(r'\\s', context[end:])\n#         if m:\n#             end = end + m.start()\n\n#     # final slice\n#     return context[start:end]\n\n# def build_truncated_context(df, max_len: int):\n#     \"\"\"\n#     Returns a list of truncated context strings for each row in df,\n#     preserving at least the answer span and cutting only at word boundaries.\n#     \"\"\"\n#     contexts = []\n#     for ctx, starts, ends in zip(df['context'], df['answer_start'], df['answer_end']):\n#         # pick the first span\n#         s = starts[0]\n#         e = ends[0]\n\n#         window = truncate_context(ctx, s, e, max_len)\n#         contexts.append(window)\n\n#     return contexts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:45:42.469909Z","iopub.execute_input":"2025-04-22T17:45:42.470122Z","iopub.status.idle":"2025-04-22T17:45:42.483690Z","shell.execute_reply.started":"2025-04-22T17:45:42.470099Z","shell.execute_reply":"2025-04-22T17:45:42.483060Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"**Add padding to the encoder and decoder inputs**","metadata":{}},{"cell_type":"markdown","source":"## Phase 2 using a transformer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, MultiHeadAttention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Lambda, Reshape\n\nembed_dim  = 128\nnum_heads  = 4\nff_dim     = 512\nnum_layers = 2\n\n# positional embeddings up to max_len\nclass PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, maxlen, dim):\n        super().__init__()\n        self.pos_emb = Embedding(input_dim=maxlen, output_dim=dim)\n    def call(self, x):\n        positions = tf.range(tf.shape(x)[1])\n        return self.pos_emb(positions)\n\n# one encoder block\ndef encoder_block(embed_dim, num_heads, ff_dim):\n    x_in = Input(shape=(None, embed_dim))\n    mask = Input(shape=(None,), dtype=\"int32\")\n\n    # expand mask to (batch, 1, 1, seq_len) and cast via a Lambda layer\n    mask_float = Lambda(\n        lambda m: tf.cast(tf.expand_dims(tf.expand_dims(m, axis=1), axis=1), \"float32\"),\n        name=\"mask_expand\"\n    )(mask)\n\n    attn = MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_dim\n    )(x_in, x_in, attention_mask=mask_float)\n\n    x = LayerNormalization(epsilon=1e-6)(x_in + attn)\n    proj = Dense(ff_dim, activation=\"relu\")(x)\n    proj = Dense(embed_dim)(proj)\n    out  = LayerNormalization(epsilon=1e-6)(x + proj)\n\n    return Model([x_in, mask], out)\n\n# inputs\nids = Input(shape=(max_len,), dtype=\"int32\", name=\"input_ids\")\nmask= Input(shape=(max_len,), dtype=\"int32\", name=\"attention_mask\")\n\n# embeddings + positional\ntok_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(ids)\npos_emb = PositionalEmbedding(max_len, embed_dim)(ids)\nx = tok_emb + pos_emb\n\n# stack encoders\nfor _ in range(num_layers):\n    block = encoder_block(embed_dim, num_heads, ff_dim)\n    x = block([x, mask])\n\n# span heads\nstart_logits = Dense(1)(x)                       # (batch, seq_len, 1)\nstart_logits = Lambda(lambda t: tf.squeeze(t, -1),\n                      name=\"start_squeeze\")(start_logits)\n\nend_logits = Dense(1)(x)\nend_logits = Lambda(lambda t: tf.squeeze(t, -1),\n                    name=\"end_squeeze\")(end_logits)\n\nstart_probs = tf.keras.layers.Activation(\"softmax\", name=\"start_probs\")(start_logits)\nend_probs   = tf.keras.layers.Activation(\"softmax\", name=\"end_probs\")(end_logits)\n\nmodel = Model([ids, mask], [start_probs, end_probs])\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:09:16.396462Z","iopub.execute_input":"2025-04-22T21:09:16.396740Z","iopub.status.idle":"2025-04-22T21:09:16.595455Z","shell.execute_reply.started":"2025-04-22T21:09:16.396721Z","shell.execute_reply":"2025-04-22T21:09:16.594898Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:09:19.794612Z","iopub.execute_input":"2025-04-22T21:09:19.795111Z","iopub.status.idle":"2025-04-22T21:10:04.013617Z","shell.execute_reply.started":"2025-04-22T21:09:19.795088Z","shell.execute_reply":"2025-04-22T21:10:04.012898Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25ms/step - end_probs_loss: 4.9682 - loss: 9.9421 - start_probs_loss: 4.9739 - val_end_probs_loss: 4.8659 - val_loss: 9.7311 - val_start_probs_loss: 4.8669\nEpoch 2/3\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - end_probs_loss: 4.9101 - loss: 9.8239 - start_probs_loss: 4.9138 - val_end_probs_loss: 4.9193 - val_loss: 9.8420 - val_start_probs_loss: 4.9220\nEpoch 3/3\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - end_probs_loss: 4.9118 - loss: 9.8246 - start_probs_loss: 4.9128 - val_end_probs_loss: 4.8855 - val_loss: 9.7774 - val_start_probs_loss: 4.8926\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# 1) Define the helper again (if you haven’t already in this notebook)\ndef predict_answer(question, context):\n    # tokenize exactly as in preprocess()\n    q_tokens, _      = whitespace_tokenize_with_spans(question)\n    c_tokens, c_spans= whitespace_tokenize_with_spans(context)\n    seq_tokens = q_tokens + [\"[SEP]\"] + c_tokens\n\n    # convert & pad\n    seq_ids = [tokenizer.word_index.get(t, tokenizer.word_index[\"[UNK]\"])\n               for t in seq_tokens]\n    seq_ids = seq_ids[:max_len] + [0]*(max_len - len(seq_ids))\n    attn    = [1]*min(len(seq_tokens),max_len) + [0]*(max_len - min(len(seq_tokens),max_len))\n\n    # run the model\n    start_p, end_p = model.predict({\n        \"input_ids\":      tf.constant([seq_ids]),\n        \"attention_mask\": tf.constant([attn])\n    })\n\n    # pick the highest‑scoring tokens\n    si = tf.argmax(start_p[0]).numpy()\n    ei = tf.argmax(end_p[0]).numpy()\n\n    # if the predicted span falls entirely before the sep token, we assume “no answer”\n    if ei < len(q_tokens) + 1:\n        return \"\"\n\n    # map back to characters\n    c_start = si - (len(q_tokens) + 1)\n    c_end   = ei - (len(q_tokens) + 1)\n    char_s  = c_spans[c_start][0]\n    char_e  = c_spans[c_end][1]\n    return context[char_s:char_e]\n\n# 2) Test it on a toy example\nquestion = \"Where does John live?\"\ncontext  = \"John is a software engineer who lives in Cairo and works at GUC.\"\nprint(\"Predicted answer:\", predict_answer(question, context))\n# → should print: \"Cairo\"\n\n# 3) Try another one\nquestion = \"What is Jane's job?\"\ncontext  = \"Jane is a biologist at the University of Alexandria. She specializes in marine ecology.\"\nprint(\"Predicted answer:\", predict_answer(question, context))\n# → should print: \"biologist\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:10:09.008738Z","iopub.execute_input":"2025-04-22T21:10:09.009486Z","iopub.status.idle":"2025-04-22T21:10:10.144446Z","shell.execute_reply.started":"2025-04-22T21:10:09.009463Z","shell.execute_reply":"2025-04-22T21:10:10.143819Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\nPredicted answer: works\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\nPredicted answer: Alexandria\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, max_len: int, embed_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        # self.supports_masking = True\n        self.max_len   = max_len\n        self.embed_dim = embed_dim\n\n        pos = np.arange(max_len)[:, np.newaxis]                 \n        dim = np.arange(embed_dim)[np.newaxis, :]                \n        angle_rates = 1.0 / np.power(10000.0, (2 * (dim//2)) / embed_dim)\n        angle_rads  = pos * angle_rates                          \n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])        \n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])        \n        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n        \n\n    def call(self, x):\n        seq_len = tf.shape(x)[1]\n        return x + self.pos_encoding[:, :seq_len, :]\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"max_len\": self.max_len,\n            \"embed_dim\": self.embed_dim,\n        })\n        return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:43.723567Z","iopub.execute_input":"2025-04-22T17:46:43.723795Z","iopub.status.idle":"2025-04-22T17:46:43.730085Z","shell.execute_reply.started":"2025-04-22T17:46:43.723779Z","shell.execute_reply":"2025-04-22T17:46:43.729460Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class Encoder(layers.Layer):\n    def __init__(self,embed_dim: int, num_heads: int, ff_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.layer1 = layers.LayerNormalization()\n        self.layer2 = layers.LayerNormalization()\n        self.ffn =  tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n\n    \n    def call(self,pos_matrix, padding_mask, **kwargs):\n        att_out= self.attention(query=pos_matrix, value=pos_matrix, key=pos_matrix,attention_mask=padding_mask)\n        norm1 = self.layer1(att_out+pos_matrix)\n        ff_out = self.ffn(norm1)\n        return self.layer2(ff_out)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n        })\n        return config\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:43.731555Z","iopub.execute_input":"2025-04-22T17:46:43.732048Z","iopub.status.idle":"2025-04-22T17:46:43.750719Z","shell.execute_reply.started":"2025-04-22T17:46:43.732024Z","shell.execute_reply":"2025-04-22T17:46:43.750131Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class Decoder(layers.Layer):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        # no supports_masking = True\n        self.self_mha  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.cross_mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn       = tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.norm1 = layers.LayerNormalization()\n        self.norm2 = layers.LayerNormalization()\n        self.norm3 = layers.LayerNormalization()\n\n    def call(self, x, enc_out,\n             look_ahead_mask=None,\n             padding_mask=None,\n             training=False):\n        # 1) Decoder self‑attention with your 3D look‑ahead+pad mask\n        att1 = self.self_mha(\n            query=x, value=x, key=x,\n            attention_mask=look_ahead_mask,\n            training=training\n        )\n        out1 = self.norm1(x + att1, training=training)\n\n        \n        att2 = self.cross_mha(\n            query=out1, value=enc_out, key=enc_out,\n            attention_mask=padding_mask,\n            training=training\n        )\n        out2 = self.norm2(out1 + att2, training=training)\n\n        \n        ffn_out = self.ffn(out2, training=training)\n        return self.norm3(out2 + ffn_out, training=training)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \"embedding_layer\": serialize_keras_object(self.token_emb)\n            \n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        \n        emb_conf = config.pop(\"embedding_layer\")\n        \n        embedding_layer = deserialize_keras_object(\n            emb_conf,\n            module_objects=globals(),       # globals() now contains Embedding\n            custom_objects=None\n        )\n        return cls(embedding_layer=embedding_layer, **config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:43.751337Z","iopub.execute_input":"2025-04-22T17:46:43.751510Z","iopub.status.idle":"2025-04-22T17:46:43.770555Z","shell.execute_reply.started":"2025-04-22T17:46:43.751496Z","shell.execute_reply":"2025-04-22T17:46:43.769855Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\nfrom tensorflow.keras.utils import serialize_keras_object, deserialize_keras_object\n\nclass Seq2SeqTransformer(tf.keras.Model):\n    def __init__(self,\n                 vocab_size,\n                 embed_dim,\n                 num_heads,\n                 ff_dim,\n                 max_enc_in_len,\n                 max_a_len,\n                 embedding_layer,\n                 pad_token_id=0,\n                 **kwargs):\n        super().__init__(**kwargs)\n\n        # store for serialization\n        self.vocab_size     = vocab_size\n        self.embed_dim      = embed_dim\n        self.num_heads      = num_heads\n        self.ff_dim         = ff_dim\n        self.max_enc_in_len = max_enc_in_len\n        self.max_a_len      = max_a_len\n        self.pad_token_id   = pad_token_id\n\n        if embedding_layer is None:\n            raise ValueError(\"`embedding_layer` must be provided\")\n        self.token_emb   = embedding_layer\n        self.pos_emb_enc = PositionalEmbedding(max_enc_in_len, embed_dim)\n        self.pos_emb_dec = PositionalEmbedding(max_a_len, embed_dim)\n        self.encoder     = Encoder(embed_dim, num_heads, ff_dim)\n        self.decoder     = Decoder(embed_dim, num_heads, ff_dim)\n        self.final_dense = layers.Dense(vocab_size, activation=\"softmax\")\n\n    def create_padding_mask(self, seq):\n        mask = tf.equal(seq, self.pad_token_id)\n        return mask[:, None, None, :]\n\n    def create_look_ahead_mask(self, size):\n        return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n\n    def call(self, inputs, training=False):\n        enc_seq, dec_seq = inputs\n        dec_len    = tf.shape(dec_seq)[1]\n\n        enc_padding_mask = self.create_padding_mask(enc_seq)\n        dec_pad_bool     = tf.equal(dec_seq, self.pad_token_id)\n\n        look2d      = tf.cast(self.create_look_ahead_mask(dec_len), tf.bool)\n        dec_pad_3d  = dec_pad_bool[:, None, :]\n        look3d      = look2d[None, :, :]\n        self_attn_mask = tf.logical_or(dec_pad_3d, look3d)\n\n        enc_x = self.pos_emb_enc(self.token_emb(enc_seq))\n        dec_x = self.pos_emb_dec(self.token_emb(dec_seq))\n\n        enc_out = self.encoder(\n            enc_x,\n            padding_mask=enc_padding_mask,\n            training=training\n        )\n        dec_out = self.decoder(\n            dec_x,\n            enc_out,\n            look_ahead_mask=self_attn_mask,\n            padding_mask=enc_padding_mask,\n            training=training\n        )\n        return self.final_dense(dec_out)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"vocab_size\":      self.vocab_size,\n            \"embed_dim\":       self.embed_dim,\n            \"num_heads\":       self.num_heads,\n            \"ff_dim\":          self.ff_dim,\n            \"max_enc_in_len\":  self.max_enc_in_len,\n            \"max_a_len\":       self.max_a_len,\n            \"pad_token_id\":    self.pad_token_id,\n            \"embedding_layer\": serialize_keras_object(self.token_emb),\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        emb_conf = config.pop(\"embedding_layer\")\n        embedding_layer = deserialize_keras_object(\n            emb_conf,\n            module_objects=globals(),\n            custom_objects={\"Embedding\": Embedding}\n        )\n        return cls(embedding_layer=embedding_layer, **config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:43.771298Z","iopub.execute_input":"2025-04-22T17:46:43.771513Z","iopub.status.idle":"2025-04-22T17:46:43.790259Z","shell.execute_reply.started":"2025-04-22T17:46:43.771489Z","shell.execute_reply":"2025-04-22T17:46:43.789567Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model = Seq2SeqTransformer(\n    vocab_size=tokenizer_phase_2.num_words,\n    embed_dim=100,\n    num_heads=8,\n    ff_dim=512,\n    max_enc_in_len=MAX_ENCODER_LEN,\n    max_a_len=MAX_A_LEN - 1, \n    embedding_layer=embedding_layer,\n    pad_token_id=0,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:43.791037Z","iopub.execute_input":"2025-04-22T17:46:43.791261Z","iopub.status.idle":"2025-04-22T17:46:43.823084Z","shell.execute_reply.started":"2025-04-22T17:46:43.791241Z","shell.execute_reply":"2025-04-22T17:46:43.822527Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"### Custom Evaluation metrics","metadata":{}},{"cell_type":"code","source":"def normalize_answer(s):\n    \"\"\"Lower, strip punctuation/extra whitespace, collapse to tokens.\"\"\"\n    s = s.lower()\n    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n    s = \" \".join(s.split())\n    return s\n\ndef exact_match_score(pred, truth):\n    return int(normalize_answer(pred) == normalize_answer(truth))\n\ndef f1_score(pred, truth):\n    pred_tokens = normalize_answer(pred).split()\n    truth_tokens = normalize_answer(truth).split()\n    if not pred_tokens or not truth_tokens:\n        return int(pred_tokens == truth_tokens)\n  \n    common = {}\n    for t in pred_tokens:\n        common[t] = common.get(t, 0) + 1\n    same = 0\n    for t in truth_tokens:\n        if common.get(t, 0) > 0:\n            same += 1\n            common[t] -= 1\n    if same == 0:\n        return 0.0\n    prec = same / len(pred_tokens)\n    rec  = same / len(truth_tokens)\n    return 2 * prec * rec / (prec + rec)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:46.808417Z","iopub.execute_input":"2025-04-22T17:46:46.808904Z","iopub.status.idle":"2025-04-22T17:46:46.815182Z","shell.execute_reply.started":"2025-04-22T17:46:46.808881Z","shell.execute_reply":"2025-04-22T17:46:46.814423Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"\ndef decode_batch(seqs, tokenizer, pad_id=0, sos_id=None, eos_id=None):\n    texts = tokenizer.sequences_to_texts(seqs)\n    clean_texts = []\n    for txt in texts:\n        tokens = txt.split()\n        tokens = [\n            t for t in tokens\n            if t not in {tokenizer.index_word.get(pad_id, \"\"), \n                         tokenizer.index_word.get(sos_id, \"\"), \n                         tokenizer.index_word.get(eos_id, \"\")}\n        ]\n        clean_texts.append(\" \".join(tokens))\n    return clean_texts\n\nclass QAEvalCallback(tf.keras.callbacks.Callback):\n    def __init__(self, val_ds, tokenizer, pad_id=0, sos_id=None, eos_id=None):\n        super().__init__()\n        self.val_ds    = val_ds\n        self.tokenizer = tokenizer\n        self.pad_id    = pad_id\n        self.sos_id    = sos_id\n        self.eos_id    = eos_id\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        total_em = 0.0\n        total_f1 = 0.0\n        count    = 0\n\n        for (enc_batch, dec_in_batch), dec_tar_batch, _ in self.val_ds:\n            logits = self.model((enc_batch, dec_in_batch), training=False)\n            pred_ids = tf.argmax(logits, axis=-1).numpy()\n\n            preds = decode_batch(pred_ids,\n                                 tokenizer=self.tokenizer,\n                                 pad_id=self.pad_id,\n                                 sos_id=self.sos_id,\n                                 eos_id=self.eos_id)\n            reals = decode_batch(dec_tar_batch.numpy(),\n                                 tokenizer=self.tokenizer,\n                                 pad_id=self.pad_id,\n                                 sos_id=self.sos_id,\n                                 eos_id=self.eos_id)\n\n            for p_str, r_str in zip(preds, reals):\n                total_em += exact_match_score(p_str, r_str)\n                total_f1 += f1_score(p_str, r_str)\n                count   += 1\n\n        val_em = 100.0 * total_em / count\n        val_f1 = 100.0 * total_f1 / count\n        logs[\"val_em\"] = val_em\n        logs[\"val_f1\"] = val_f1\n        print(f\" — val_EM: {val_em:.2f}%  — val_F1: {val_f1:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:51.346888Z","iopub.execute_input":"2025-04-22T17:46:51.347158Z","iopub.status.idle":"2025-04-22T17:46:51.355516Z","shell.execute_reply.started":"2025-04-22T17:46:51.347138Z","shell.execute_reply":"2025-04-22T17:46:51.354789Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"### Model training","metadata":{}},{"cell_type":"code","source":"dummy_enc = tf.zeros((1, MAX_ENCODER_LEN), dtype=tf.int32)\ndummy_dec = tf.zeros((1,MAX_A_LEN - 1 ), dtype=tf.int32)\n_ = model((dummy_enc, dummy_dec), training=False)\nmodel.summary()\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=[]    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:46:53.281599Z","iopub.execute_input":"2025-04-22T17:46:53.282258Z","iopub.status.idle":"2025-04-22T17:46:55.735798Z","shell.execute_reply.started":"2025-04-22T17:46:53.282234Z","shell.execute_reply":"2025-04-22T17:46:55.735290Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"seq2_seq_transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2_seq_transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                           │       \u001b[38;5;34m5,729,200\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_1               │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder (\u001b[38;5;33mEncoder\u001b[0m)                    │ ?                           │         \u001b[38;5;34m425,912\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder (\u001b[38;5;33mDecoder\u001b[0m)                    │ ?                           │         \u001b[38;5;34m748,612\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m50000\u001b[0m)              │       \u001b[38;5;34m5,050,000\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,200</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_1               │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                    │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">425,912</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                    │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">748,612</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,000</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,953,724\u001b[0m (45.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,953,724</span> (45.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,224,524\u001b[0m (23.74 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,224,524</span> (23.74 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,729,200\u001b[0m (21.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,200</span> (21.86 MB)\n</pre>\n"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"MODEL_DIR       = get_model_dir()\nCHECKPOINT_PATH = os.path.join(MODEL_DIR, 'best_transformer_1.keras')\n\ncheckpoint_cb = ModelCheckpoint(\n    filepath=CHECKPOINT_PATH,\n    monitor='val_f1',      \n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\n\nqa_eval_cb = QAEvalCallback(\n    val_ds,\n    tokenizer_phase_2,\n    pad_id=pad_id,\n    sos_id=sos_id,\n    eos_id=eos_id\n)\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10,\n    callbacks=[qa_eval_cb,checkpoint_cb]\n)\n\nprint(f\"Best model will be saved to: {CHECKPOINT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:52:09.819030Z","iopub.execute_input":"2025-04-22T17:52:09.819321Z","iopub.status.idle":"2025-04-22T17:54:18.290054Z","shell.execute_reply.started":"2025-04-22T17:52:09.819301Z","shell.execute_reply":"2025-04-22T17:54:18.289465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0716 — val_EM: 0.00%  — val_F1: 11.97%\n\nEpoch 1: val_f1 improved from -inf to 11.97097, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - loss: 0.0716 - val_loss: 0.4020 - val_em: 0.0000e+00 - val_f1: 11.9710\nEpoch 2/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0432 — val_EM: 0.00%  — val_F1: 12.38%\n\nEpoch 2: val_f1 improved from 11.97097 to 12.38409, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0432 - val_loss: 0.4000 - val_em: 0.0000e+00 - val_f1: 12.3841\nEpoch 3/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0259 — val_EM: 0.00%  — val_F1: 12.41%\n\nEpoch 3: val_f1 improved from 12.38409 to 12.41223, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0259 - val_loss: 0.4070 - val_em: 0.0000e+00 - val_f1: 12.4122\nEpoch 4/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0177 — val_EM: 0.00%  — val_F1: 12.58%\n\nEpoch 4: val_f1 improved from 12.41223 to 12.57900, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0177 - val_loss: 0.3984 - val_em: 0.0000e+00 - val_f1: 12.5790\nEpoch 5/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0135 — val_EM: 0.00%  — val_F1: 12.49%\n\nEpoch 5: val_f1 did not improve from 12.57900\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0135 - val_loss: 0.4068 - val_em: 0.0000e+00 - val_f1: 12.4895\nEpoch 6/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0138 — val_EM: 0.00%  — val_F1: 12.56%\n\nEpoch 6: val_f1 did not improve from 12.57900\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0138 - val_loss: 0.4068 - val_em: 0.0000e+00 - val_f1: 12.5629\nEpoch 7/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0115 — val_EM: 0.00%  — val_F1: 12.57%\n\nEpoch 7: val_f1 did not improve from 12.57900\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0115 - val_loss: 0.3997 - val_em: 0.0000e+00 - val_f1: 12.5745\nEpoch 8/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0098 — val_EM: 0.00%  — val_F1: 12.60%\n\nEpoch 8: val_f1 improved from 12.57900 to 12.59516, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - loss: 0.0098 - val_loss: 0.4100 - val_em: 0.0000e+00 - val_f1: 12.5952\nEpoch 9/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0091 — val_EM: 0.00%  — val_F1: 12.67%\n\nEpoch 9: val_f1 improved from 12.59516 to 12.66712, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0091 - val_loss: 0.3977 - val_em: 0.0000e+00 - val_f1: 12.6671\nEpoch 10/10\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0083 — val_EM: 0.00%  — val_F1: 12.63%\n\nEpoch 10: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0083 - val_loss: 0.4076 - val_em: 0.0000e+00 - val_f1: 12.6311\nBest model will be saved to: /kaggle/working/models/best_transformer_1.keras\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"best_model = load_model(\n    '/kaggle/working/models/best_transformer_1.keras',\n    custom_objects={\n        \"Seq2SeqTransformer\": Seq2SeqTransformer,\n        \"PositionalEmbedding\": PositionalEmbedding,\n        \"Encoder\": Encoder,\n        \"Decoder\": Decoder,\n        \"Embedding\": Embedding,   \n    }\n)\n\nbest_model.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[]\n)\n\n\nhistory = best_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[qa_eval_cb, checkpoint_cb]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:54:49.292250Z","iopub.execute_input":"2025-04-22T17:54:49.292662Z","iopub.status.idle":"2025-04-22T17:59:24.480027Z","shell.execute_reply.started":"2025-04-22T17:54:49.292640Z","shell.execute_reply":"2025-04-22T17:59:24.479268Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745344500.606035     100 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m140/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0128","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745344514.557092     101 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0129","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745344519.146207      99 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nW0000 00:00:1745344520.341257      98 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":" — val_EM: 0.00%  — val_F1: 12.54%\n\nEpoch 1: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 135ms/step - loss: 0.0129 - val_loss: 0.3610 - val_em: 0.0000e+00 - val_f1: 12.5432\nEpoch 2/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0095 — val_EM: 0.00%  — val_F1: 12.51%\n\nEpoch 2: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0095 - val_loss: 0.3694 - val_em: 0.0000e+00 - val_f1: 12.5099\nEpoch 3/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0087 — val_EM: 0.00%  — val_F1: 12.52%\n\nEpoch 3: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0087 - val_loss: 0.3768 - val_em: 0.0000e+00 - val_f1: 12.5230\nEpoch 4/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0081 — val_EM: 0.00%  — val_F1: 12.49%\n\nEpoch 4: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0081 - val_loss: 0.3859 - val_em: 0.0000e+00 - val_f1: 12.4909\nEpoch 5/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0078 — val_EM: 0.00%  — val_F1: 12.66%\n\nEpoch 5: val_f1 did not improve from 12.66712\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0078 - val_loss: 0.3821 - val_em: 0.0000e+00 - val_f1: 12.6608\nEpoch 6/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0069 — val_EM: 0.00%  — val_F1: 12.69%\n\nEpoch 6: val_f1 improved from 12.66712 to 12.68931, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0069 - val_loss: 0.3844 - val_em: 0.0000e+00 - val_f1: 12.6893\nEpoch 7/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0072 — val_EM: 0.00%  — val_F1: 12.66%\n\nEpoch 7: val_f1 did not improve from 12.68931\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0072 - val_loss: 0.3838 - val_em: 0.0000e+00 - val_f1: 12.6557\nEpoch 8/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0069 — val_EM: 0.00%  — val_F1: 12.70%\n\nEpoch 8: val_f1 improved from 12.68931 to 12.69602, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0069 - val_loss: 0.4023 - val_em: 0.0000e+00 - val_f1: 12.6960\nEpoch 9/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0061 — val_EM: 0.00%  — val_F1: 12.74%\n\nEpoch 9: val_f1 improved from 12.69602 to 12.74444, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0061 - val_loss: 0.4048 - val_em: 0.0000e+00 - val_f1: 12.7444\nEpoch 10/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0058 — val_EM: 0.00%  — val_F1: 12.73%\n\nEpoch 10: val_f1 did not improve from 12.74444\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0058 - val_loss: 0.4046 - val_em: 0.0000e+00 - val_f1: 12.7325\nEpoch 11/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0064 — val_EM: 0.00%  — val_F1: 12.69%\n\nEpoch 11: val_f1 did not improve from 12.74444\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0064 - val_loss: 0.4172 - val_em: 0.0000e+00 - val_f1: 12.6935\nEpoch 12/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0066 — val_EM: 0.00%  — val_F1: 12.71%\n\nEpoch 12: val_f1 did not improve from 12.74444\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0066 - val_loss: 0.3997 - val_em: 0.0000e+00 - val_f1: 12.7119\nEpoch 13/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0062 — val_EM: 0.00%  — val_F1: 12.80%\n\nEpoch 13: val_f1 improved from 12.74444 to 12.79941, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 92ms/step - loss: 0.0062 - val_loss: 0.4170 - val_em: 0.0000e+00 - val_f1: 12.7994\nEpoch 14/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0059 — val_EM: 0.00%  — val_F1: 12.75%\n\nEpoch 14: val_f1 did not improve from 12.79941\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0059 - val_loss: 0.4203 - val_em: 0.0000e+00 - val_f1: 12.7543\nEpoch 15/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0056 — val_EM: 0.00%  — val_F1: 12.80%\n\nEpoch 15: val_f1 did not improve from 12.79941\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0056 - val_loss: 0.4077 - val_em: 0.0000e+00 - val_f1: 12.7955\nEpoch 16/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0062 — val_EM: 0.00%  — val_F1: 12.66%\n\nEpoch 16: val_f1 did not improve from 12.79941\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0062 - val_loss: 0.4224 - val_em: 0.0000e+00 - val_f1: 12.6629\nEpoch 17/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0058 — val_EM: 0.00%  — val_F1: 12.74%\n\nEpoch 17: val_f1 did not improve from 12.79941\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0058 - val_loss: 0.4217 - val_em: 0.0000e+00 - val_f1: 12.7384\nEpoch 18/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0060 — val_EM: 0.00%  — val_F1: 12.63%\n\nEpoch 18: val_f1 did not improve from 12.79941\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 89ms/step - loss: 0.0060 - val_loss: 0.4313 - val_em: 0.0000e+00 - val_f1: 12.6344\nEpoch 19/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0070 — val_EM: 0.00%  — val_F1: 12.82%\n\nEpoch 19: val_f1 improved from 12.79941 to 12.82336, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0070 - val_loss: 0.4413 - val_em: 0.0000e+00 - val_f1: 12.8234\nEpoch 20/20\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0058 — val_EM: 0.00%  — val_F1: 12.85%\n\nEpoch 20: val_f1 improved from 12.82336 to 12.84990, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 93ms/step - loss: 0.0058 - val_loss: 0.4453 - val_em: 0.0000e+00 - val_f1: 12.8499\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"\n\nprint(\"  • # train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"  • # val   batches:\", tf.data.experimental.cardinality(val_ds).numpy())\n\n\nfor (enc_batch, decin_batch), dec_tar_batch, weight_batch in train_ds.take(1):\n    print(\"Encoder shape:\", enc_batch.shape)       # (B, enc_len)\n    print(\"Decoder‑in shape:\", decin_batch.shape)  # (B, ans_len)\n    print(\"Decoder‑tar shape:\", dec_tar_batch.shape)\n    print(\"Weights shape:\", weight_batch.shape)\n    \n    enc_ids = enc_batch[0].numpy().tolist()\n    decin_ids = decin_batch[0].numpy().tolist()\n    detar_ids = dec_tar_batch[0].numpy().tolist()\n    \n    \n    enc_ids = [i for i in enc_ids if i != pad_id]\n    decin_ids = [i for i in decin_ids if i not in (pad_id, sos_id, eos_id)]\n    detar_ids = [i for i in detar_ids if i not in (pad_id, sos_id, eos_id)]\n    \n    q_c_text = tokenizer_phase_2.sequences_to_texts([enc_ids])[0]\n    inp_ans  = tokenizer_phase_2.sequences_to_texts([decin_ids])[0]\n    tar_ans  = tokenizer_phase_2.sequences_to_texts([detar_ids])[0]\n    \n    print(\"\\nSample #1\")\n    print(\"  Q+SEP+C →\", q_c_text[:200] + \"…\")\n    print(\"  Decoder‑in →\", inp_ans)\n    print(\"  Decoder‑tar→\", tar_ans)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:47:02.810064Z","iopub.execute_input":"2025-04-22T18:47:02.810633Z","iopub.status.idle":"2025-04-22T18:47:02.852414Z","shell.execute_reply.started":"2025-04-22T18:47:02.810606Z","shell.execute_reply":"2025-04-22T18:47:02.851877Z"}},"outputs":[{"name":"stdout","text":"  • # train batches: 141\n  • # val   batches: 16\nEncoder shape: (64, 408)\nDecoder‑in shape: (64, 31)\nDecoder‑tar shape: (64, 31)\nWeights shape: (64, 31)\n\nSample #1\n  Q+SEP+C → what are two of wright's designs [SEP] architects such as frank lloyd wright developed organic architecture in which the form was defined by its environment and purpose with an aim to promote harmony …\n  Decoder‑in → robie house and fallingwater\n  Decoder‑tar→ robie house and fallingwater\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"\n\ndef show_errors_greedy(model, dataset, tokenizer, max_encoder_len, max_answer_len, sep_id, pad_id, sos_id, eos_id, n=5):\n    it = dataset.as_numpy_iterator()\n    samples = random.sample(list(it), n)\n    for (enc, _), dec_tar, _ in samples:\n        enc_row = [i for i in enc[0].tolist() if i != pad_id]\n        sep_index = enc_row.index(sep_id)\n        q_ids = enc_row[:sep_index]\n        c_ids = enc_row[sep_index+1:]\n        q_txt = tokenizer.sequences_to_texts([q_ids])[0]\n        c_txt = tokenizer.sequences_to_texts([c_ids])[0]\n        dec_input = [sos_id]\n        for _ in range(max_answer_len):\n            dec_pad = pad_sequences([dec_input], maxlen=max_answer_len, padding=\"post\", value=pad_id)\n            logits = model((enc, dec_pad), training=False)\n            next_id = int(tf.argmax(logits[0, len(dec_input)-1]).numpy())\n            if next_id == eos_id:\n                break\n            dec_input.append(next_id)\n        pred_ids = [i for i in dec_input[1:] if i not in (pad_id, sos_id, eos_id)]\n        true_ids = [i for i in dec_tar[0].tolist() if i not in (pad_id, sos_id, eos_id)]\n        pred_txt = tokenizer.sequences_to_texts([pred_ids])[0]\n        true_txt = tokenizer.sequences_to_texts([true_ids])[0]\n        print(f\"Q:  {q_txt!r}\")\n        print(f\"C:  {c_txt[:200]!r}…\")\n        print(f\"GT → {true_txt!r}\")\n        print(f\"PR → {pred_txt!r}\")\n        print(\"-\"*80)\n\nshow_errors_greedy(best_model, val_ds, tokenizer_phase_2, MAX_ENCODER_LEN, MAX_A_LEN-1, sep_id, pad_id, sos_id, eos_id, n=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:32:59.033355Z","iopub.execute_input":"2025-04-22T18:32:59.033711Z","iopub.status.idle":"2025-04-22T18:32:59.671215Z","shell.execute_reply.started":"2025-04-22T18:32:59.033689Z","shell.execute_reply":"2025-04-22T18:32:59.670673Z"}},"outputs":[{"name":"stdout","text":"Q:  'what was the less expensive version of the macintosh se that was offered until 2001'\nC:  'in response apple introduced a range of relatively inexpensive macs in october 1990 the macintosh classic essentially a less expensive version of the macintosh se was the least expensive mac offered u'…\nGT → 'the macintosh classic'\nPR → '60–80'\n--------------------------------------------------------------------------------\nQ:  'what does federalism mean in europe'\nC:  'in contrast europe has a greater history of unitary states than north america thus european federalism argues for a weaker central government relative to a unitary state the modern american usage of t'…\nGT → 'weaker central government relative to a unitary state'\nPR → '60–80'\n--------------------------------------------------------------------------------\nQ:  'which group of people were now considered to be politically equal with the plebeians'\nC:  'the plebeians had finally achieved political equality with the patricians however the plight of the average plebeian had not changed a small number of plebeian families achieved the same standing that'…\nGT → 'patricians'\nPR → '60–80'\n--------------------------------------------------------------------------------\nQ:  'on what date did the austrian government secretly decide on going to war with france'\nC:  'after four years on the sidelines austria sought another war with france to [UNK] its recent defeats austria could not count on russian support because the latter was at war with britain sweden and th'…\nGT → '8 february 1809'\nPR → '60–80'\n--------------------------------------------------------------------------------\nQ:  'from what language did espanyol translate their name'\nC:  'in 1918 espanyol started a counter petition against autonomy which at that time had become a pertinent issue later on an espanyol supporter group would join the [UNK] in the spanish civil war siding w'…\nGT → 'spanish'\nPR → '60–80'\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\n\n# Suppose decoder_targets is your unbatched numpy array of shape (N, seq_len)\n# You padded to MAX_A_LEN, so decoder_targets exists as detar before train_test_split.\nall_answers = []\nfor seq in decoder_targets[:500]:\n    clean_ids = [i for i in seq if i not in (pad_id, sos_id, eos_id)]\n    text = tokenizer_phase_2.sequences_to_texts([clean_ids])[0]\n    all_answers.append(text)\n\nanswer_counts = Counter(all_answers)\ndf_top = pd.DataFrame(answer_counts.items(), columns=['answer', 'count']) \\\n    .sort_values('count', ascending=False) \\\n    .reset_index(drop=True)\n\nprint(df_top.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:34:16.195295Z","iopub.execute_input":"2025-04-22T18:34:16.195884Z","iopub.status.idle":"2025-04-22T18:34:16.246502Z","shell.execute_reply.started":"2025-04-22T18:34:16.195860Z","shell.execute_reply":"2025-04-22T18:34:16.245922Z"}},"outputs":[{"name":"stdout","text":"                                               answer  count\n0                                                  45      2\n1                                                1988      2\n2                                               1 000      2\n3                                               india      2\n4                                           the bible      2\n5                                                2010      2\n6                                               eight      2\n7                                            couplets      1\n8                digitize and offer nara video online      1\n9                          banking financial services      1\n10            pollen either fails to reach the stigma      1\n11            the communist party of the soviet union      1\n12                                               rich      1\n13                                      louis agassiz      1\n14                                                 56      1\n15                                    research center      1\n16  used it as part of the modern gay rights movem...      1\n17                                         10 550 350      1\n18                                            halifax      1\n19                          bolivia romania and italy      1\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## Post Processing","metadata":{}},{"cell_type":"code","source":"def greedy_decode(model,\n                  question: str,\n                  context: str,\n                  tokenizer,\n                  max_encoder_len: int,\n                  max_answer_len: int,\n                  sep_token_id: int,\n                  pad_token_id: int,\n                  sos_token_id: int,\n                  eos_token_id: int):\n    q_ids = tokenizer.texts_to_sequences([question])[0]\n    c_ids = tokenizer.texts_to_sequences([context])[0]\n    enc_seq = q_ids + [sep_token_id] + c_ids\n    enc_input = pad_sequences(\n        [enc_seq],\n        maxlen=max_encoder_len,\n        padding='post',\n        truncating='post',\n        value=pad_token_id\n    )\n\n   \n    dec_input = [sos_token_id]\n\n    for _ in range(max_answer_len):\n        dec_pad = pad_sequences(\n            [dec_input],\n            maxlen=max_answer_len,\n            padding='post',\n            truncating='post',\n            value=pad_token_id\n        )\n\n        preds = model((enc_input, dec_pad), training=False)\n        next_id = int(tf.argmax(preds[0, len(dec_input)-1]).numpy())\n        if next_id == eos_token_id:\n            break\n\n        dec_input.append(next_id)\n\n    decoded_tokens = [tokenizer.index_word.get(i, \"\") for i in dec_input[1:]]\n    return \" \".join(decoded_tokens)\n\n\nsos_id = tokenizer_phase_2.word_index[\"[SOS]\"]\neos_id = tokenizer_phase_2.word_index[\"[EOS]\"]\nsep_id = tokenizer_phase_2.word_index[\"[SEP]\"]\npad_id = 0\n\nquestion = \"What is the capital of France?\"\ncontext  = (\"France is a country in Western Europe. \"\n            \"Its capital city is Paris, known for the Eiffel Tower.\")\n\nanswer = greedy_decode(\n    best_model,\n    question,\n    context,\n    tokenizer_phase_2,\n    max_encoder_len=MAX_ENCODER_LEN,\n    max_answer_len=MAX_A_LEN-1,\n    sep_token_id=sep_id,\n    pad_token_id=pad_id,\n    sos_token_id=sos_id,\n    eos_token_id=eos_id\n)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:45:39.845051Z","iopub.execute_input":"2025-04-22T18:45:39.845710Z","iopub.status.idle":"2025-04-22T18:45:39.956872Z","shell.execute_reply.started":"2025-04-22T18:45:39.845688Z","shell.execute_reply":"2025-04-22T18:45:39.956149Z"}},"outputs":[{"name":"stdout","text":"Answer: 60–80\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"question = \"What is my name?\"\ncontext = \"My name is ahmed and please my name is ahmed\"\n\nanswer2 = greedy_decode(\n    best_model,\n    question,\n    context,\n    tokenizer_phase_2,\n    max_encoder_len=MAX_ENCODER_LEN,\n    max_answer_len=MAX_A_LEN-1,\n    sep_token_id=sep_id,\n    pad_token_id=pad_id,\n    sos_token_id=sos_id,\n    eos_token_id=eos_id\n)\nprint(\"Answer: \", answer2 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:45:58.402743Z","iopub.execute_input":"2025-04-22T18:45:58.403024Z","iopub.status.idle":"2025-04-22T18:45:58.508531Z","shell.execute_reply.started":"2025-04-22T18:45:58.403005Z","shell.execute_reply":"2025-04-22T18:45:58.507978Z"}},"outputs":[{"name":"stdout","text":"Answer:  60–80\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"## Inference using the best model","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nsquad_dev = load_dataset(\"squad_v2\", split=\"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:44:45.313131Z","iopub.execute_input":"2025-04-22T11:44:45.313630Z","iopub.status.idle":"2025-04-22T11:44:50.812112Z","shell.execute_reply.started":"2025-04-22T11:44:45.313605Z","shell.execute_reply":"2025-04-22T11:44:50.811536Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a6fa9223ed4c76b1bcfb93c17780e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba91a2de954428385b92fff7dc28029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da1e285b1de24a08bc524770c00f3570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2932f6ba9c47c7833b23afef560b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8126b8b7792d46f5b7b54a57e8c08cfa"}},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"import pandas as pd\n\n# 1) Pull out the raw columns\ncontexts = ans_dev[\"context\"]\nanswer_dicts = ans_dev[\"answers\"]  # this is a list of {\"text\": [...], \"answer_start\": [...]}\n\n# 2) Split into two parallel lists: starts and texts\nanswer_starts = [d[\"answer_start\"] for d in answer_dicts]\nanswer_texts  = [d[\"text\"]         for d in answer_dicts]\n\n# 3) Compute end positions for each text span\nanswer_ends = [\n    [s + len(t) for s, t in zip(starts, texts)]\n    for starts, texts in zip(answer_starts, answer_texts)\n]\n\n# 4) Build your DataFrame\ndf_dev = pd.DataFrame({\n    \"context\":      contexts,\n    \"answer_start\": answer_starts,\n    \"answer_end\":   answer_ends,\n})\n\n# Now your previous code will work:\ncontexts_trunc = build_truncated_context(df_dev, max_len=2000)\nquestions     = ans_dev[\"question\"]\nanswers       = [texts[0] for texts in answer_texts]  # take first text per example\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:48:17.527820Z","iopub.execute_input":"2025-04-22T11:48:17.528107Z","iopub.status.idle":"2025-04-22T11:48:20.056266Z","shell.execute_reply.started":"2025-04-22T11:48:17.528088Z","shell.execute_reply":"2025-04-22T11:48:20.055374Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b764621d97434887d74d12be7e2ba7"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1103065552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m df_dev = pd.DataFrame({\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mans_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m\"answer_start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mans_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer_start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \"answer_end\":   [[s + len(t) \n\u001b[1;32m     10\u001b[0m                       for s,t in zip(starts, texts)] \n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"],"ename":"TypeError","evalue":"list indices must be integers or slices, not str","output_type":"error"}],"execution_count":86},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}