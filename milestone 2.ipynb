{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMvLH9BB7pR"
      },
      "source": [
        "# Milestone 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import os, zipfile , json , random\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "kFomRI3xB9d8"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explorting dataset:"
      ],
      "metadata": {
        "id": "1zzFTlcTCEc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq41oiX90fQq",
        "outputId": "fe5a3fca-6b13-4311-e832-665e0b15a13e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/drive/MyDrive/TriviaQA_RC.zip \"https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0NuNGE435ad",
        "outputId": "0a22bcda-bc15-4325-80d4-dce763c70e0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-13 11:20:19--  https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.117, 2607:4000:200:12:3eec:efff:fe5e:6f68\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2665779500 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘/content/drive/MyDrive/TriviaQA_RC.zip’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   2.48G  49.1MB/s    in 89s     \n",
            "\n",
            "2025-04-13 11:21:48 (28.6 MB/s) - ‘/content/drive/MyDrive/TriviaQA_RC.zip’ saved [2665779500/2665779500]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/TriviaQA_RC.zip\" /content/TriviaQA_RC.zip"
      ],
      "metadata": {
        "id": "TSDZT50PX9sj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p /content/TriviaQA_RC"
      ],
      "metadata": {
        "id": "s_tEP-KyHMFT"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /content/TriviaQA_RC.zip -C /content/TriviaQA_RC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xRqXefbcF_-",
        "outputId": "681f81f2-79f2-443e-9fd3-2aff9416ade8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/TriviaQA_RC -maxdepth 2 | sed -e '1,5!d'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp9G6t5ndklu",
        "outputId": "6fc2f95f-ec2b-4882-bff7-549b05d6e540"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TriviaQA_RC\n",
            "/content/TriviaQA_RC/README\n",
            "/content/TriviaQA_RC/qa\n",
            "/content/TriviaQA_RC/qa/wikipedia-train.json\n",
            "/content/TriviaQA_RC/qa/web-train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,50p' /content/TriviaQA_RC/README"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZjxAxE3n-2x",
        "outputId": "3b33d9c2-337f-4c51-cc1a-587b49f9e8ba"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------\n",
            "The University of Washington TriviaQA Dataset (version 1.0)\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "\n",
            "TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. The details can be found in our paper\n",
            "\n",
            "@InProceedings{JoshiTriviaQA2017,\n",
            "  author    = {Joshi, Mandar  and  Choi, Eunsol  and  Weld, Daniel S. and Zettlemoyer, Luke},\n",
            "  title     = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},\n",
            "  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n",
            "  month     = {July},\n",
            "  year      = {2017},\n",
            "  address   = {Vancouver, Canada},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "}\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "LIST OF DATA FILES\n",
            "* qa/wikipedia-train.json, qa/web-train.json\n",
            "* qa/[verified-]wikipedia-dev.json, qa/[verified-]web-dev.json\n",
            "These files contain [verified] questions, answers, and document names in the train/dev set for the wikipedia/web domain. The details of the verified evaluation set are described in section 4 (evidence analysis) of the paper. The wikipedia/web documents are listed in the json array \"EntityPages\"/\"SearchResults\" for each question. The \"Filename\" field in each element of the array indicates the relative path of the file inside the wikipedia/web directory in evidence.\n",
            "\n",
            "* qa/wikipedia-test-without-answers.json, qa/web-test-without-answers.json\n",
            "These files contain questions and document names (no answers) in the train/dev set for the wikipedia/web domain. We are withholding the test answers for a later release. Please check the website -- http://nlp.cs.washington.edu/triviaqa/ -- for details of the test evaluation. \n",
            "\n",
            "* evidence/web, evidence/wikipedia\n",
            "These directories contain documents in the train/dev/test set for the wikipedia/web domain. The documents are referenced in the json array \"EntityPages\"/\"SearchResults\" for each question in the QA pair files. The \"Filename\" field in each element of the array indicates the relative path of the file inside the wikipedia/web directory.\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "FORMAT\n",
            "Each QA example is in the following format.\n",
            "- Answer: \n",
            "  - Value: The answer string obtained from the orginal trivia website\n",
            "  - Aliases: A list of aliases (from Wikipedia and other sources). \n",
            "  - NormalizedAliases: Normalized variant of the above. A predicted answer is considered an exact match if it belongs to this list.\n",
            "  - Type: Numeric, WikipediaEntity or FreeForm\n",
            "  - MatchedWikiEntityName: Present only if Type is WikipediaEntity\n",
            "  - NormalizedMatchedWikiEntityName: Present only if Type is WikipediaEntity\n",
            "- EntityPages: A list of Wikipedia documents for the question. Each document of the format:\n",
            "  - DocSource: Search or TagMe\n",
            "  - Filename: Filename in \"evidence/wikipedia\" directory.\n",
            "  - Title: Wikipedia page title\n",
            "- Question: The question string\n",
            "- QuestionId: A unique alphanumeric ID for the question \n",
            "- QuestionSource: The website from which the question was crawled.\n",
            "- SearchResults: A list of retrieved web pages which contains the answer entities. Each webpage of format \n",
            "  - Description: Search snippet.\n",
            "  - Filename: Filename in \"evidence/web\" directory.\n",
            "  - Rank: Search output rank\n",
            "  - Title: Page Title\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = Path('/content/TriviaQA_RC')\n",
        "qa_file = base_dir / 'qa' / 'wikipedia-train.json'\n",
        "with open(qa_file, 'r', encoding='utf-8') as f:\n",
        "    examples = json.load(f)\n",
        "examples.keys()\n",
        "\n"
      ],
      "metadata": {
        "id": "JixYG4s6saGU",
        "outputId": "17a436af-54dd-4728-f2b3-cb8367020447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Data', 'Domain', 'Split', 'VerifiedEval', 'Version'])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(base_dir / 'qa' / 'wikipedia-train.json', 'r', encoding='utf-8') as f:\n",
        "    wrapper = json.load(f)\n",
        "\n",
        "# 2. Extract the list of examples\n",
        "examples = wrapper['Data']\n",
        "print(f\"Total examples available: {len(examples)}\\n\")\n",
        "\n",
        "# 3. Inspect the first 3 entries\n",
        "for idx, sample in enumerate(examples[:3]):\n",
        "    print(f\"=== Example {idx+1} ===\")\n",
        "    print(\"QuestionId :\", sample['QuestionId'])\n",
        "    print(\"Question   :\", sample['Question'])\n",
        "    print(\"Answer     :\", sample['Answer']['Value'])\n",
        "    print(\"Answer Aliases:\", sample['Answer']['Aliases'][:5], \"…\\n\")\n",
        "\n",
        "    page = sample['EntityPages'][0]\n",
        "    ctx_path = base_dir / 'evidence' / 'wikipedia' / page['Filename']\n",
        "    if ctx_path.exists():\n",
        "        text = ctx_path.read_text(encoding='utf-8', errors='ignore')\n",
        "        print(f\"Context file ({page['Filename']}) snippet:\")\n",
        "        print(text[:300].replace('\\n', ' '), \"…\\n\")\n",
        "    else:\n",
        "        print(f\"Context file not found: {ctx_path}\\n\")\n"
      ],
      "metadata": {
        "id": "NhZlSX4qtmtj",
        "outputId": "23fc3993-8af6-42b4-f16e-0af9b1922283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total examples available: 61888\n",
            "\n",
            "=== Example 1 ===\n",
            "QuestionId : tc_3\n",
            "Question   : Where in England was Dame Judi Dench born?\n",
            "Answer     : York\n",
            "Answer Aliases: ['Park Grove (1895)', 'York UA', 'Yorkish', 'UN/LOCODE:GBYRK', 'York, UK'] …\n",
            "\n",
            "Context file (England.txt) snippet:\n",
            "England is a country that is part of the United Kingdom.   It shares land borders with Scotland to the north and Wales to the west. The Irish Sea lies northwest of England and the Celtic Sea lies to the southwest. England is separated from continental Europe by the North Sea to the east and the Engl …\n",
            "\n",
            "=== Example 2 ===\n",
            "QuestionId : tc_8\n",
            "Question   : From which country did Angola achieve independence in 1975?\n",
            "Answer     : Portugal\n",
            "Answer Aliases: ['Portogało', 'Republic of Portugal', 'PORTUGAL', 'Portekiz', 'Portugallu'] …\n",
            "\n",
            "Context file (Nation_state.txt) snippet:\n",
            "A nation state is a type of state that conjoins the political entity of a state to the cultural entity of a nation, from which it aims to derive its political legitimacy to rule and potentially its status as a sovereign state if one accepts the declarative theory of statehood as opposed to the const …\n",
            "\n",
            "=== Example 3 ===\n",
            "QuestionId : tc_9\n",
            "Question   : Which city does David Soul come from?\n",
            "Answer     : Chicago\n",
            "Answer Aliases: ['Chi-Beria', 'Sayre language academy', 'Chicago', 'Chicago, Illinois', 'Hog Butcher for the World'] …\n",
            "\n",
            "Context file (David_Soul.txt) snippet:\n",
            "David Soul (born August 28, 1943) is an American-British actor and singer. He is known for his role as Detective Kenneth \"Hutch\" Hutchinson in the ABC television series Starsky & Hutch from 1975 to 1979. He became a British citizen in 2004.   Early life  Soul was born David Richard Solberg in Chicag …\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(examples)\n",
        "subset = examples[:15000]\n",
        "len(subset)"
      ],
      "metadata": {
        "id": "D72WTTHOupEF",
        "outputId": "7d7a83f3-92b0-4552-ab31-290168cdfed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset[4]"
      ],
      "metadata": {
        "id": "rSzBSJjsx1MU",
        "outputId": "a7f4b952-64b0-43c2-a847-7a87629fbd18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Answer': {'Aliases': [\"I'll Stand by You (Pretenders song)\",\n",
              "   \"I'll Stand by You\",\n",
              "   \"I'll Stand By You (Carrie Underwood song)\",\n",
              "   'I’ll Stand By You',\n",
              "   \"I'll Stand By You (Girls Aloud song)\",\n",
              "   \"I'll Stand By You (song)\",\n",
              "   \"I'll Stand By You\",\n",
              "   'Ill stand by you'],\n",
              "  'MatchedWikiEntityName': \"I'll Stand by You\",\n",
              "  'NormalizedAliases': ['i ll stand by you carrie underwood song',\n",
              "   'i ll stand by you song',\n",
              "   'ill stand by you',\n",
              "   'i ll stand by you',\n",
              "   'i ll stand by you pretenders song',\n",
              "   'i ll stand by you girls aloud song'],\n",
              "  'NormalizedMatchedWikiEntityName': 'i ll stand by you',\n",
              "  'NormalizedValue': 'i ll stand by you',\n",
              "  'Type': 'WikipediaEntity',\n",
              "  'Value': 'I’ll Stand By You'},\n",
              " 'EntityPages': [{'DocSource': 'TagMe',\n",
              "   'Filename': 'The_Pretenders.txt',\n",
              "   'Title': 'The Pretenders'},\n",
              "  {'DocSource': 'TagMe',\n",
              "   'Filename': 'Girls_Aloud.txt',\n",
              "   'Title': 'Girls Aloud'},\n",
              "  {'DocSource': 'Search',\n",
              "   'Filename': \"I'll_Stand_by_You.txt\",\n",
              "   'Title': \"I'll Stand by You\",\n",
              "   'originalUrl': 'https://sv.wikipedia.org/wiki/I%27ll_Stand_by_You'}],\n",
              " 'Question': 'Which song was a top 40 hit for both The Pretenders in 1994 and Girls Aloud in 2004?',\n",
              " 'QuestionId': 'odql_6260',\n",
              " 'QuestionSource': 'http://www.odquiz.org.uk/'}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning"
      ],
      "metadata": {
        "id": "-EdBrPmqbBvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context needed**: so we add the context from each file to the subset list"
      ],
      "metadata": {
        "id": "E_Cb25_x11aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method that adds context to each entry\n",
        "def get_context(sample, base_dir):\n",
        "    contexts = []\n",
        "    for page in sample[\"EntityPages\"]:\n",
        "        fname = page[\"Filename\"]\n",
        "        ctx_path = base_dir / 'evidence' / 'wikipedia' / fname\n",
        "        if ctx_path.exists():\n",
        "            contexts.append(\n",
        "                ctx_path.read_text(encoding='utf-8', errors='ignore')\n",
        "            )\n",
        "    sample[\"Context\"] = contexts\n",
        "    return sample\n",
        "\n",
        "\n",
        "for sample in subset:\n",
        "    get_context(sample, base_dir)"
      ],
      "metadata": {
        "id": "JCoQdBYJ19ON"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset[0].keys()"
      ],
      "metadata": {
        "id": "1cKHh0Kr4Kob",
        "outputId": "7559d128-6304-449b-e91d-2c4efc77f6ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Answer', 'EntityPages', 'Question', 'QuestionId', 'QuestionSource', 'Context'])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keeping only important features**"
      ],
      "metadata": {
        "id": "NftXigsA4evJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_sample(sample):\n",
        "    \"\"\"\n",
        "    Given a full TriviaQA sample (with Question, EntityPages, Context, Answer, etc.),\n",
        "    return a dict with only the fields we need for modeling:\n",
        "      - question: the question string\n",
        "      - context : the first context passage (string)\n",
        "      - answer  : the ground-truth answer string\n",
        "    \"\"\"\n",
        "    # 1) Grab the question\n",
        "    q = sample['Question']\n",
        "\n",
        "\n",
        "    ctx = sample.get('Context', [])\n",
        "    c = ctx[0] if ctx else \"\"\n",
        "\n",
        "\n",
        "    a = sample['Answer']['Value']\n",
        "\n",
        "    return {\n",
        "        'question': q,\n",
        "        'context' : c,\n",
        "        'answer'  : a\n",
        "    }\n",
        "\n",
        "\n",
        "cleaned = [ simplify_sample(s) for s in subset ]\n",
        "\n"
      ],
      "metadata": {
        "id": "s0ziPitA4dU9"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "B_-pDASk5eOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gensim"
      ],
      "metadata": {
        "id": "gBEVZxrJ5gav",
        "outputId": "ddb8414a-0853-4a0e-c543-556307ea58c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=20000,\n",
        "    oov_token='[UNK]'\n",
        ")\n",
        "\n",
        "questions = [s['question'] for s in cleaned]\n",
        "contexts  = [s['context']  for s in cleaned]\n",
        "answers   = [s['answer']   for s in cleaned]\n",
        "\n",
        "tokenizer.fit_on_texts(questions + contexts + answers)\n",
        "\n",
        "q_seqs = tokenizer.texts_to_sequences(questions)\n",
        "c_seqs = tokenizer.texts_to_sequences(contexts)\n",
        "a_seqs = tokenizer.texts_to_sequences(answers)"
      ],
      "metadata": {
        "id": "l2U-kh4D5mdK"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_seqs[0]"
      ],
      "metadata": {
        "id": "_PX28kds7kK2",
        "outputId": "d29a4db7-61ab-40a0-de88-537c4e329f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 182, 136, 254, 448, 1550, 9927, 8860, 8538, 681, 8987, 6064, 4, 6250]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load gloVe dictionary**"
      ],
      "metadata": {
        "id": "bEwBdMqoGBO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_zip = tf.keras.utils.get_file(\n",
        "    fname=\"glove.6B.zip\",\n",
        "    origin=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
        "    extract=False\n",
        ")\n",
        "glove_dir = os.path.dirname(glove_zip)\n",
        "\n",
        "with zipfile.ZipFile(glove_zip, 'r') as z:\n",
        "    files = z.namelist()\n",
        "    target = \"glove.6B.100d.txt\"\n",
        "    if target in files and not os.path.exists(os.path.join(glove_dir, target)):\n",
        "        z.extract(target, path=glove_dir)\n",
        "\n",
        "glove_path = os.path.join(glove_dir, \"glove.6B.100d.txt\")\n"
      ],
      "metadata": {
        "id": "sHHCMrjhGG4V",
        "outputId": "f9e92aed-fbfa-4dc5-ca6c-29af2070f0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
            "\u001b[1m862182613/862182613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'zipfile' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-4226d61c2541>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating embeddings index (mapping words to vectors)**"
      ],
      "metadata": {
        "id": "moKmgC0MGTU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.rstrip().split(\" \")\n",
        "        word = parts[0]\n",
        "        vec  = np.asarray(parts[1:], dtype='float32')\n",
        "        embeddings_index[word] = vec"
      ],
      "metadata": {
        "id": "3cSrFo3eGdH8"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JaZiYhBIjw7",
        "outputId": "e90a662a-c389-417d-8313-ed443d15048e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-abd7d839b66b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"the\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating our look-up table (embedding matrix)**"
      ],
      "metadata": {
        "id": "DWBBEfnDHXhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "emb_dim = 100   #features of each vector embeddings\n",
        "# each word is a column vector\n",
        "embedding_matrix = np.random.normal(size=(vocab_size, emb_dim)) * 0.01"
      ],
      "metadata": {
        "id": "KNj69dr19Io_"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, idx in tokenizer.word_index.items():\n",
        "    if idx >= vocab_size:\n",
        "        continue\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[idx] = embeddings_index[word]"
      ],
      "metadata": {
        "id": "5OFnRaWp_DsR"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = tokenizer.index_word[2]\n",
        "print(word)\n",
        "print(embedding_matrix[2])"
      ],
      "metadata": {
        "id": "IkAwXryuJNZU",
        "outputId": "9b66d5e3-c45f-40d9-d6de-d8d73163b7ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
            " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
            "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
            " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
            "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
            " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
            "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
            "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
            " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
            " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
            " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
            " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
            " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
            " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
            " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
            "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
            " -0.52028    -0.1459      0.82779998  0.27061999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create embedding layer**"
      ],
      "metadata": {
        "id": "fXmVw1ItJ-uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=emb_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    mask_zero=True,\n",
        "    trainable=False,\n",
        "    name='glove_embedding'\n",
        ")"
      ],
      "metadata": {
        "id": "FVUoVwpZJPMt"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XSOJYRpOKaqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}