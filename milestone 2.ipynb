{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11489407,"sourceType":"datasetVersion","datasetId":7201839},{"sourceId":11500742,"sourceType":"datasetVersion","datasetId":7210129}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Milestone 2","metadata":{"id":"qNMvLH9BB7pR"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pandas as pd\nimport os, zipfile , json , random, requests\nimport re\nfrom pathlib import Path\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import  Sequential","metadata":{"id":"kFomRI3xB9d8","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:55.799060Z","iopub.execute_input":"2025-04-21T17:07:55.799282Z","iopub.status.idle":"2025-04-21T17:08:10.110146Z","shell.execute_reply.started":"2025-04-21T17:07:55.799238Z","shell.execute_reply":"2025-04-21T17:08:10.109583Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 17:07:57.443469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745255277.645923      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745255277.704311      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Explorting dataset:","metadata":{"id":"1zzFTlcTCEc7"}},{"cell_type":"code","source":"\ndef is_kaggle():\n    # Kaggle kernels always set this env var\n    return 'KAGGLE_URL_BASE' in os.environ\n\ndef is_colab():\n    return (not is_kaggle()) and os.path.exists('/content')\n\ndef maybe_mount_drive():\n    if is_colab():\n        from google.colab import drive\n        if not os.path.isdir('/content/drive'):\n            drive.mount('/content/drive')\n\ndef get_data_path():\n    if is_kaggle():\n        return '/kaggle/input/squad-2-0/'\n    elif is_colab():\n        return '/content/drive/MyDrive/SQuAD'\n    else:\n        return './data/'\ndef get_model_dir():\n    if is_colab():\n        model_dir = '/content/drive/MyDrive/models'\n    elif is_kaggle():\n        model_dir = '/kaggle/working/models'\n    else:\n        model_dir = './models'\n    os.makedirs(model_dir, exist_ok=True)\n    return model_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:10.114021Z","iopub.execute_input":"2025-04-21T17:08:10.114264Z","iopub.status.idle":"2025-04-21T17:08:10.120891Z","shell.execute_reply.started":"2025-04-21T17:08:10.114229Z","shell.execute_reply":"2025-04-21T17:08:10.120185Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset_dir = get_data_path()\nmaybe_mount_drive()\nos.makedirs(dataset_dir, exist_ok=True)","metadata":{"id":"e0NuNGE435ad","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:10.121484Z","iopub.execute_input":"2025-04-21T17:08:10.121684Z","iopub.status.idle":"2025-04-21T17:08:10.326557Z","shell.execute_reply.started":"2025-04-21T17:08:10.121666Z","shell.execute_reply":"2025-04-21T17:08:10.325913Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"file_path = os.path.join(dataset_dir, 'train-v2.0.json')","metadata":{"id":"TSDZT50PX9sj","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:19.374730Z","iopub.execute_input":"2025-04-21T17:08:19.374994Z","iopub.status.idle":"2025-04-21T17:08:19.378615Z","shell.execute_reply.started":"2025-04-21T17:08:19.374975Z","shell.execute_reply":"2025-04-21T17:08:19.378051Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"with open(file_path, 'r', encoding='utf-8') as f:\n    squad = json.load(f)","metadata":{"id":"s_tEP-KyHMFT","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:21.054800Z","iopub.execute_input":"2025-04-21T17:08:21.055375Z","iopub.status.idle":"2025-04-21T17:08:22.689198Z","shell.execute_reply.started":"2025-04-21T17:08:21.055355Z","shell.execute_reply":"2025-04-21T17:08:22.688594Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"records = []\nfor article in squad['data']:\n    for para in article['paragraphs']:\n        ctx = para['context']\n        for qa in para['qas']:\n            answers = [a['text'] for a in qa.get('answers', [])]\n            starts  = [a['answer_start'] for a in qa.get('answers', [])]\n            ends    = [s + len(t) for s,t in zip(starts, answers)]\n            records.append({\n                'question': qa['question'],\n                'answers': answers,\n                'context': ctx,\n                'answer_start': starts,\n                'answer_end': ends\n            })\n\n","metadata":{"id":"JixYG4s6saGU","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:22.690234Z","iopub.execute_input":"2025-04-21T17:08:22.690444Z","iopub.status.idle":"2025-04-21T17:08:23.736082Z","shell.execute_reply.started":"2025-04-21T17:08:22.690429Z","shell.execute_reply":"2025-04-21T17:08:23.735288Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.DataFrame(records)\ndf.head()","metadata":{"id":"NhZlSX4qtmtj","outputId":"96567ffd-6526-4218-c322-84c26cd2e7e6","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:23.737191Z","iopub.execute_input":"2025-04-21T17:08:23.737483Z","iopub.status.idle":"2025-04-21T17:08:23.850590Z","shell.execute_reply.started":"2025-04-21T17:08:23.737464Z","shell.execute_reply":"2025-04-21T17:08:23.850005Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                            question                answers  \\\n0           When did Beyonce start becoming popular?    [in the late 1990s]   \n1  What areas did Beyonce compete in when she was...  [singing and dancing]   \n2  When did Beyonce leave Destiny's Child and bec...                 [2003]   \n3      In what city and state did Beyonce  grow up?        [Houston, Texas]   \n4         In which decade did Beyonce become famous?           [late 1990s]   \n\n                                             context answer_start answer_end  \n0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [269]      [286]  \n1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [207]      [226]  \n2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [526]      [530]  \n3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [166]      [180]  \n4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [276]      [286]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When did Beyonce start becoming popular?</td>\n      <td>[in the late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[269]</td>\n      <td>[286]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What areas did Beyonce compete in when she was...</td>\n      <td>[singing and dancing]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[207]</td>\n      <td>[226]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When did Beyonce leave Destiny's Child and bec...</td>\n      <td>[2003]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[526]</td>\n      <td>[530]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In what city and state did Beyonce  grow up?</td>\n      <td>[Houston, Texas]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[166]</td>\n      <td>[180]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In which decade did Beyonce become famous?</td>\n      <td>[late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[276]</td>\n      <td>[286]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(\"Total QA pairs:\", len(df))","metadata":{"id":"D72WTTHOupEF","outputId":"6686b957-2f37-44bc-c831-47864e7260dd","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:24.164784Z","iopub.execute_input":"2025-04-21T17:08:24.165509Z","iopub.status.idle":"2025-04-21T17:08:24.169224Z","shell.execute_reply.started":"2025-04-21T17:08:24.165482Z","shell.execute_reply":"2025-04-21T17:08:24.168665Z"}},"outputs":[{"name":"stdout","text":"Total QA pairs: 130319\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#shuffling\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# only working on subset of 15k row\ndf_subset = df.head(15000).copy().reset_index(drop=True)\n\nprint(\"Subset size:\", df_subset.shape)\ndf_subset.head()","metadata":{"id":"rSzBSJjsx1MU","outputId":"97a3a6d3-c217-4ab8-a1b7-edd234dca397","colab":{"base_uri":"https://localhost:8080/","height":310},"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:25.142816Z","iopub.execute_input":"2025-04-21T17:08:25.143053Z","iopub.status.idle":"2025-04-21T17:08:25.223450Z","shell.execute_reply.started":"2025-04-21T17:08:25.143038Z","shell.execute_reply":"2025-04-21T17:08:25.222705Z"}},"outputs":[{"name":"stdout","text":"Subset size: (15000, 5)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  What year did the global recession that follow...   \n1  what was a popular club in ibiza that started ...   \n2  In what century did Martin Luther honor Mary a...   \n3                          What is the climate like?   \n4        How many times has the Queen toured Canada?   \n\n                                   answers  \\\n0                                   [2012]   \n1                                [Amnesia]   \n2                                       []   \n3  [varies from hot and subhumid tropical]   \n4                                       []   \n\n                                             context answer_start answer_end  \n0  It threatened the collapse of large financial ...        [481]      [485]  \n1  But house was also being developed on Ibiza,[c...        [251]      [258]  \n2  Although Calvin and Huldrych Zwingli honored M...           []         []  \n3  Due to extreme variation in elevation, great v...        [115]      [152]  \n4  The Queen addressed the United Nations for a s...           []         []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What year did the global recession that follow...</td>\n      <td>[2012]</td>\n      <td>It threatened the collapse of large financial ...</td>\n      <td>[481]</td>\n      <td>[485]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what was a popular club in ibiza that started ...</td>\n      <td>[Amnesia]</td>\n      <td>But house was also being developed on Ibiza,[c...</td>\n      <td>[251]</td>\n      <td>[258]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In what century did Martin Luther honor Mary a...</td>\n      <td>[]</td>\n      <td>Although Calvin and Huldrych Zwingli honored M...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the climate like?</td>\n      <td>[varies from hot and subhumid tropical]</td>\n      <td>Due to extreme variation in elevation, great v...</td>\n      <td>[115]</td>\n      <td>[152]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How many times has the Queen toured Canada?</td>\n      <td>[]</td>\n      <td>The Queen addressed the United Nations for a s...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"id":"-EdBrPmqbBvA"}},{"cell_type":"markdown","source":"Dropping rows where answers are empty","metadata":{"id":"E_Cb25_x11aT"}},{"cell_type":"code","source":"df_subset = df_subset[df_subset['answers'].map(len) > 0].reset_index(drop=True)\nprint(\"Rows remaining after drop:\", len(df_subset))","metadata":{"id":"JCoQdBYJ19ON","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98bfb707-f5f5-4339-8879-dd7c8426a6d9","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:28.081496Z","iopub.execute_input":"2025-04-21T17:08:28.082172Z","iopub.status.idle":"2025-04-21T17:08:28.097435Z","shell.execute_reply.started":"2025-04-21T17:08:28.082148Z","shell.execute_reply":"2025-04-21T17:08:28.096756Z"}},"outputs":[{"name":"stdout","text":"Rows remaining after drop: 10020\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Removing Extra Whitespaces","metadata":{"id":"QBSLXMvASpe0"}},{"cell_type":"code","source":"def collapse_whitespace(s):\n    if isinstance(s, str):\n        return re.sub(r'\\s+', ' ', s.strip())\n    return s","metadata":{"id":"FZAnlrcyStOE","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:29.955573Z","iopub.execute_input":"2025-04-21T17:08:29.955819Z","iopub.status.idle":"2025-04-21T17:08:29.959824Z","shell.execute_reply.started":"2025-04-21T17:08:29.955803Z","shell.execute_reply":"2025-04-21T17:08:29.959070Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for col in ['question', 'context', 'answers']:\n    if col in df_subset.columns:\n        df_subset[col] = df_subset[col].apply(collapse_whitespace)","metadata":{"id":"fnVh34G5VBAm","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:30.987872Z","iopub.execute_input":"2025-04-21T17:08:30.988371Z","iopub.status.idle":"2025-04-21T17:08:31.457139Z","shell.execute_reply.started":"2025-04-21T17:08:30.988347Z","shell.execute_reply":"2025-04-21T17:08:31.456588Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"**Lets explore the length of the sequences which will determine some hyperparameters in training the models**","metadata":{"id":"JFeXiO2dzf8Q"}},{"cell_type":"code","source":"df_subset['question'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc530Af3zoTV","outputId":"f93ee1cc-11ca-4548-ea54-98ddafec6e05","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:32.460302Z","iopub.execute_input":"2025-04-21T17:08:32.461057Z","iopub.status.idle":"2025-04-21T17:08:32.470480Z","shell.execute_reply.started":"2025-04-21T17:08:32.461033Z","shell.execute_reply":"2025-04-21T17:08:32.469833Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"203"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"df_subset['context'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDvLl2vx0a4o","outputId":"103fac31-6acf-45c6-cf69-bb1656c1fe4c","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:33.436481Z","iopub.execute_input":"2025-04-21T17:08:33.436744Z","iopub.status.idle":"2025-04-21T17:08:33.445928Z","shell.execute_reply.started":"2025-04-21T17:08:33.436726Z","shell.execute_reply":"2025-04-21T17:08:33.445382Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"3706"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"We just turn the array of the answers to a string since none have multiple answers","metadata":{"id":"6hWOoWFg1GZk"}},{"cell_type":"code","source":"df_subset['answers']= df_subset['answers'].apply(lambda x: x[0])","metadata":{"id":"rh00m5Jw0lWm","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:34.883530Z","iopub.execute_input":"2025-04-21T17:08:34.883789Z","iopub.status.idle":"2025-04-21T17:08:34.893839Z","shell.execute_reply.started":"2025-04-21T17:08:34.883771Z","shell.execute_reply":"2025-04-21T17:08:34.893144Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_subset['answers'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoL1ZTTx0z8b","outputId":"e4736878-4cb3-4c68-df14-41efa24a60e8","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:35.669439Z","iopub.execute_input":"2025-04-21T17:08:35.670073Z","iopub.status.idle":"2025-04-21T17:08:35.679191Z","shell.execute_reply.started":"2025-04-21T17:08:35.670049Z","shell.execute_reply":"2025-04-21T17:08:35.678679Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"202"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Embeddings","metadata":{"id":"B_-pDASk5eOa"}},{"cell_type":"code","source":"!pip install --quiet gensim","metadata":{"id":"gBEVZxrJ5gav","outputId":"e51f32d7-1a78-4015-9a21-d4a580224374","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:37.515775Z","iopub.execute_input":"2025-04-21T17:08:37.516027Z","iopub.status.idle":"2025-04-21T17:08:47.203271Z","shell.execute_reply.started":"2025-04-21T17:08:37.516009Z","shell.execute_reply":"2025-04-21T17:08:47.202302Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Tokenizer for phase 1 only**","metadata":{}},{"cell_type":"code","source":"tokenizer_phase_1 = Tokenizer(\n    num_words=20000,\n    oov_token='[UNK]',\n    filters='''!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'''\n)\ntokenizer_phase_1.fit_on_texts(df_subset[\"question\"].tolist()+df_subset[\"answers\"].tolist())\n\nq_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['question'])\na_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['answers'])","metadata":{"id":"l2U-kh4D5mdK","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:47.204936Z","iopub.execute_input":"2025-04-21T17:08:47.205434Z","iopub.status.idle":"2025-04-21T17:08:47.579307Z","shell.execute_reply.started":"2025-04-21T17:08:47.205409Z","shell.execute_reply":"2025-04-21T17:08:47.578337Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"vocab_size = len(tokenizer_phase_1.word_index)\nprint(\"Total unique tokens:\", vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:47.580308Z","iopub.execute_input":"2025-04-21T17:08:47.580615Z","iopub.status.idle":"2025-04-21T17:08:47.584934Z","shell.execute_reply.started":"2025-04-21T17:08:47.580590Z","shell.execute_reply":"2025-04-21T17:08:47.584102Z"}},"outputs":[{"name":"stdout","text":"Total unique tokens: 18733\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Tokenizer for phase 2**","metadata":{}},{"cell_type":"code","source":"def truncate_context(context: str, ans_start: int, ans_end: int, max_len: int) -> str:\n    \"\"\"\n    Return a substring of `context` of length up to max_len characters,\n    centered on the character span [ans_start:ans_end], adjusted to word boundaries.\n    \"\"\"\n    ans_len = ans_end - ans_start\n    extra   = max_len - ans_len\n    pre     = extra // 2\n    post    = extra - pre\n\n    # ideal window\n    start = ans_start - pre\n    end   = ans_end   + post\n\n    # shift if off left edge\n    if start < 0:\n        start = 0\n        end   = min(max_len, len(context))\n\n    # shift if off right edge\n    if end > len(context):\n        end   = len(context)\n        start = max(0, len(context) - max_len)\n\n    # adjust start backward to nearest whitespace to avoid cutting a word\n    if start > 0 and not context[start].isspace():\n        m = re.search(r'\\s', context[:start][::-1])\n        if m:\n            # position of last whitespace before start\n            start = start - m.start()\n\n    # adjust end forward to nearest whitespace to avoid cutting a word\n    if end < len(context) and not context[end].isspace():\n        m = re.search(r'\\s', context[end:])\n        if m:\n            end = end + m.start()\n\n    # final slice\n    return context[start:end]\n\ndef build_truncated_context(df, max_len: int):\n    \"\"\"\n    Returns a list of truncated context strings for each row in df,\n    preserving at least the answer span and cutting only at word boundaries.\n    \"\"\"\n    contexts = []\n    for ctx, starts, ends in zip(df['context'], df['answer_start'], df['answer_end']):\n        # pick the first span\n        s = starts[0]\n        e = ends[0]\n\n        window = truncate_context(ctx, s, e, max_len)\n        contexts.append(window)\n\n    return contexts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:55.180846Z","iopub.execute_input":"2025-04-21T17:08:55.181118Z","iopub.status.idle":"2025-04-21T17:08:55.188181Z","shell.execute_reply.started":"2025-04-21T17:08:55.181098Z","shell.execute_reply":"2025-04-21T17:08:55.187583Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#2000 is the number of allowed characters\ncontexts =  build_truncated_context(df_subset, 2000)\nlen(contexts[10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:58.290734Z","iopub.execute_input":"2025-04-21T17:08:58.290973Z","iopub.status.idle":"2025-04-21T17:08:58.327976Z","shell.execute_reply.started":"2025-04-21T17:08:58.290956Z","shell.execute_reply":"2025-04-21T17:08:58.327302Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"1012"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"SEP_TOKEN = \"[SEP]\"\n\nquestions    = df_subset['question'].astype(str).tolist()\nanswers = df_subset['answers'].astype(str).tolist()\n\ntokenizer_phase_2 = Tokenizer(\n    num_words=20000,\n    oov_token='[UNK]',\n    filters='''!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'''\n)\n\nall_texts = questions + contexts + answers + [SEP_TOKEN]\ntokenizer_phase_2.fit_on_texts(all_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:59.873555Z","iopub.execute_input":"2025-04-21T17:08:59.873809Z","iopub.status.idle":"2025-04-21T17:09:00.972424Z","shell.execute_reply.started":"2025-04-21T17:08:59.873793Z","shell.execute_reply":"2025-04-21T17:09:00.971863Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"trunc_c_seqs = tokenizer_phase_2.texts_to_sequences(contexts)\nmax(len(x) for x in trunc_c_seqs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:01.822791Z","iopub.execute_input":"2025-04-21T17:09:01.823323Z","iopub.status.idle":"2025-04-21T17:09:02.435153Z","shell.execute_reply.started":"2025-04-21T17:09:01.823301Z","shell.execute_reply":"2025-04-21T17:09:02.434407Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"374"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"sep_id = tokenizer_phase_2.word_index.get(SEP_TOKEN)\nif sep_id is None:\n    sep_id = max(tokenizer_phase_2.word_index.values()) + 1\n    tokenizer_phase_2.word_index[SEP_TOKEN]   = sep_id\n    tokenizer_phase_2.index_word[sep_id]      = SEP_TOKEN\n\nprint(\"✔️ SEP token id is\", sep_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:03.408790Z","iopub.execute_input":"2025-04-21T17:09:03.409360Z","iopub.status.idle":"2025-04-21T17:09:03.414681Z","shell.execute_reply.started":"2025-04-21T17:09:03.409336Z","shell.execute_reply":"2025-04-21T17:09:03.413974Z"}},"outputs":[{"name":"stdout","text":"✔️ SEP token id is 57292\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# assume VOCAB_SIZE = 20000\nMAX_ALLOWED = tokenizer_phase_2.num_words - 1   # reserve 0 for padding\n\n# get its current (too large) id\nold_sep_id = tokenizer_phase_2.word_index[SEP_TOKEN]\n\n# pick a new “legal” id\nnew_sep_id = MAX_ALLOWED\n\n# find which token currently holds new_sep_id (if any)\nswap_token = tokenizer_phase_2.index_word.get(new_sep_id)\n\n\nif swap_token:\n    tokenizer_phase_2.word_index[swap_token] = old_sep_id\n    tokenizer_phase_2.index_word[old_sep_id] = swap_token\n\n\ntokenizer_phase_2.word_index[SEP_TOKEN]   = new_sep_id\ntokenizer_phase_2.index_word[new_sep_id]  = SEP_TOKEN\n\nprint(f\"SEP_TOKEN remapped from {old_sep_id} → {new_sep_id}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:04.996359Z","iopub.execute_input":"2025-04-21T17:09:04.996619Z","iopub.status.idle":"2025-04-21T17:09:05.001663Z","shell.execute_reply.started":"2025-04-21T17:09:04.996602Z","shell.execute_reply":"2025-04-21T17:09:05.001097Z"}},"outputs":[{"name":"stdout","text":"SEP_TOKEN remapped from 57292 → 19999\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"MAX_C_TRUNC   = max(len(seq) for seq in trunc_c_seqs)\nMAX_ENCODER_LEN = max(len(s) for s in q_seqs) + 1 + MAX_C_TRUNC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:07.269493Z","iopub.execute_input":"2025-04-21T17:09:07.270133Z","iopub.status.idle":"2025-04-21T17:09:07.275090Z","shell.execute_reply.started":"2025-04-21T17:09:07.270111Z","shell.execute_reply":"2025-04-21T17:09:07.274312Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"**Load gloVe dictionary**","metadata":{"id":"bEwBdMqoGBO_"}},{"cell_type":"code","source":"def prepare_glove(target_dim=100, work_subdir='glove',\n                  input_dataset_slug='glove6b',\n                  download_url='http://nlp.stanford.edu/data/glove.6B.zip'):\n    maybe_mount_drive()\n\n    if is_kaggle():\n        work_dir = f'/kaggle/working/{work_subdir}'\n        uploaded_zip = f'/kaggle/input/{input_dataset_slug}/glove.6B.zip'\n    elif is_colab():\n        work_dir = f'/content/drive/MyDrive/{work_subdir}'\n        uploaded_zip = None\n    else:\n        work_dir = f'./data/{work_subdir}'\n        uploaded_zip = None\n\n    os.makedirs(work_dir, exist_ok=True)\n\n    target_file = f'glove.6B.{target_dim}d.txt'\n    txt_path = os.path.join(work_dir, target_file)\n    zip_path = os.path.join(work_dir, os.path.basename(download_url))\n\n    if os.path.exists(txt_path):\n        return txt_path\n\n    if is_kaggle() and uploaded_zip and os.path.exists(uploaded_zip):\n        zip_path = uploaded_zip\n    else:\n        if requests is None:\n            raise RuntimeError(\"`requests` not available; offline mode\")\n        with requests.get(download_url, stream=True) as r, open(zip_path, 'wb') as f:\n            r.raise_for_status()\n            for chunk in r.iter_content(8192):\n                f.write(chunk)\n\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        z.extract(target_file, path=work_dir)\n\n    if not os.path.exists(txt_path):\n        raise RuntimeError(f\"Failed to extract {target_file}\")\n\n    return txt_path\n\n# Usage\nglove_path = prepare_glove()\nprint(\"GloVe file:\", glove_path)","metadata":{"id":"sHHCMrjhGG4V","outputId":"cc629348-918c-4ab8-85a2-0297f924a578","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:10.840165Z","iopub.execute_input":"2025-04-21T17:09:10.840450Z","iopub.status.idle":"2025-04-21T17:09:10.848430Z","shell.execute_reply.started":"2025-04-21T17:09:10.840429Z","shell.execute_reply":"2025-04-21T17:09:10.847886Z"}},"outputs":[{"name":"stdout","text":"GloVe file: /kaggle/working/glove/glove.6B.100d.txt\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def create_embedding_layer(\n    tokenizer,\n    glove_path: str,\n    embedding_dim: int,\n    mask_zero: bool = True,\n    trainable: bool = False,\n    oov_token: str = '[UNK]'\n) -> Embedding:\n    \"\"\"\n    Build a Keras Embedding layer from a fitted tokenizer and a GloVe file.\n\n    Args:\n        tokenizer: a fitted keras.preprocessing.text.Tokenizer\n        glove_path: path to a GloVe‑style file (word + embedding_dim floats)\n        max_num_words: max vocabulary size (typically tokenizer.num_words)\n        embedding_dim: dimensionality of the GloVe vectors\n        mask_zero: if True, reserve index 0 for padding (and mask it)\n        trainable: if False, freeze the embedding weights\n        oov_token: the out‑of‑vocab token (must match tokenizer.oov_token)\n\n    Returns:\n        A tf.keras.layers.Embedding instance with pretrained weights.\n    \"\"\"\n   \n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.rstrip().split(\" \")\n            word = parts[0]\n            coefs = np.asarray(parts[1:], dtype='float32')\n            if coefs.shape[0] != embedding_dim:\n                continue  # skip any lines that don't match expected dim\n            embeddings_index[word] = coefs\n\n    \n    vocab_size =  len(tokenizer.word_index) + 1\n    embedding_matrix = np.random.normal(\n        scale=0.01,\n        size=(vocab_size, embedding_dim)\n    ).astype('float32')\n\n    \n    for word, idx in tokenizer.word_index.items():\n        if idx == 0 or idx >= vocab_size:\n            continue\n        vec = embeddings_index.get(word)\n        if vec is not None:\n            embedding_matrix[idx] = vec\n\n    # 4) Build the Keras layer\n    embedding_layer = Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_dim,\n        weights=[embedding_matrix],\n        mask_zero=mask_zero,\n        trainable=trainable,\n        name='pretrained_embedding'\n    )\n    return embedding_layer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:14.742018Z","iopub.execute_input":"2025-04-21T17:09:14.742373Z","iopub.status.idle":"2025-04-21T17:09:14.751066Z","shell.execute_reply.started":"2025-04-21T17:09:14.742351Z","shell.execute_reply":"2025-04-21T17:09:14.749769Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"MAX_Q_LEN = max(len(s) for s in q_seqs)\nMAX_A_LEN = max(len(s) for s in a_seqs)\nMAX_C_LEN   = max(len(s) for s in trunc_c_seqs)\nMAX_ENCODER_LEN = max(len(s) for s in q_seqs) + 1 + MAX_C_TRUNC\nVOCAB_SIZE  = 20000\nEMB_DIM     = 100\nUNITS       = 128\nBATCH_SIZE  = 64","metadata":{"id":"XSOJYRpOKaqa","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:18.786630Z","iopub.execute_input":"2025-04-21T17:09:18.786892Z","iopub.status.idle":"2025-04-21T17:09:18.794227Z","shell.execute_reply.started":"2025-04-21T17:09:18.786871Z","shell.execute_reply":"2025-04-21T17:09:18.793504Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"q_padded = pad_sequences(q_seqs, maxlen=MAX_Q_LEN, padding='post', truncating='post')\na_padded = pad_sequences(a_seqs, maxlen=MAX_A_LEN, padding='post', truncating='post')","metadata":{"id":"JERoPRt-RHlC","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:21.296615Z","iopub.execute_input":"2025-04-21T17:09:21.296927Z","iopub.status.idle":"2025-04-21T17:09:21.337152Z","shell.execute_reply.started":"2025-04-21T17:09:21.296908Z","shell.execute_reply":"2025-04-21T17:09:21.336437Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Phase One","metadata":{"id":"Ci0NXaY-UBOi"}},{"cell_type":"code","source":"EMB_DIM      = 100\nGLOVE_PATH   = prepare_glove()\n\nembedding_layer = create_embedding_layer(\n    tokenizer=tokenizer_phase_1,\n    glove_path=GLOVE_PATH,\n    embedding_dim=EMB_DIM,\n    mask_zero=True,      \n    trainable=False      \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:52.224242Z","iopub.execute_input":"2025-04-21T17:09:52.224758Z","iopub.status.idle":"2025-04-21T17:10:02.512518Z","shell.execute_reply.started":"2025-04-21T17:09:52.224738Z","shell.execute_reply":"2025-04-21T17:10:02.511711Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745255401.226533      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"decoder_input  = a_padded[:, :-1]\ndecoder_target = a_padded[:, 1:]\n\nXq_tr, Xq_val, Din_tr, Din_val, Dt_tr, Dt_val = train_test_split(\n    q_padded, decoder_input, decoder_target,\n    test_size=0.1, random_state=42\n)\n\ndef make_ds(q, d_in, d_tar, batch_size=64):\n    mask = tf.cast(tf.not_equal(d_tar, 0), tf.float32)\n    ds = tf.data.Dataset.from_tensor_slices(\n        ((q, d_in), d_tar, mask)\n    )\n    return ds.shuffle(2000).batch(batch_size).prefetch(1)\n\ntrain_ds = make_ds(Xq_tr, Din_tr, Dt_tr, batch_size=BATCH_SIZE)\nval_ds   = make_ds(Xq_val, Din_val, Dt_val, batch_size=BATCH_SIZE)","metadata":{"id":"zaezBfWvSmaO","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:02.514501Z","iopub.execute_input":"2025-04-21T17:10:02.514758Z","iopub.status.idle":"2025-04-21T17:10:02.551404Z","shell.execute_reply.started":"2025-04-21T17:10:02.514741Z","shell.execute_reply":"2025-04-21T17:10:02.550873Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(\"Train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"Val batches:\", tf.data.experimental.cardinality(val_ds).numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:02.552065Z","iopub.execute_input":"2025-04-21T17:10:02.552339Z","iopub.status.idle":"2025-04-21T17:10:02.557008Z","shell.execute_reply.started":"2025-04-21T17:10:02.552322Z","shell.execute_reply":"2025-04-21T17:10:02.556496Z"}},"outputs":[{"name":"stdout","text":"Train batches: 141\nVal batches: 16\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**Building the model**","metadata":{"id":"s-DjaB2H1448"}},{"cell_type":"code","source":"class Seq2SeqLSTM(tf.keras.Model):\n    def __init__(self,vocab_size,emb_dim,units,max_q_len,max_a_len,embedding_matrix=None,pad_token_id=0,**kwargs):\n        super().__init__(**kwargs)\n        self.pad_token_id = pad_token_id\n\n\n        if embedding_matrix is not None:\n            self.embedding = Embedding(vocab_size, emb_dim,weights=[embedding_matrix],trainable=False,mask_zero=True)\n        else:\n            self.embedding = Embedding(vocab_size, emb_dim,mask_zero=True)\n\n        #units is the vector size of the hidden state\n        #return_state if true returns the final h and c\n        self.encoder_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n\n        #return sequence returns all the hidden states from h_1 to h_n\n        #return sequence is for evaluation\n        #return state is for inference because after each token generated we need to feed the model the states again\n        self.decoder_lstm = LSTM(units,return_sequences=True,return_state=True, name='decoder_lstm')\n\n        #the layer needed to predict the next word\n        self.dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n\n    def call(self, inputs, training=False):\n        encoder_inputs, decoder_inputs = inputs\n\n        x_enc = self.embedding(encoder_inputs)\n        _, state_h, state_c = self.encoder_lstm(x_enc, training=training)\n        encoder_states = [state_h, state_c]\n\n        x_dec = self.embedding(decoder_inputs)\n        dec_outputs, _, _ = self.decoder_lstm(x_dec, initial_state=encoder_states, training=training)\n        return self.dense(dec_outputs)\n","metadata":{"id":"XONLiGcUxUgA","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:02.558223Z","iopub.execute_input":"2025-04-21T17:10:02.558547Z","iopub.status.idle":"2025-04-21T17:10:02.572395Z","shell.execute_reply.started":"2025-04-21T17:10:02.558524Z","shell.execute_reply":"2025-04-21T17:10:02.571795Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"model = Seq2SeqLSTM( vocab_size=vocab_size,\n                    emb_dim=100,units=128,\n                     max_q_len=MAX_Q_LEN,max_a_len=MAX_A_LEN,embedding_matrix=embedding_matrix,pad_token_id=0)\n\ndummy_q = tf.zeros((1, MAX_Q_LEN), dtype=tf.int32)\ndummy_a = tf.zeros((1, MAX_A_LEN-1), dtype=tf.int32)\n_ = model((dummy_q, dummy_a))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T17:07:22.030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n)\nmodel.summary()","metadata":{"id":"WY8ot7KK74I-","outputId":"b3a84da1-e1f8-44e5-8cdf-ba203079b9be","colab":{"base_uri":"https://localhost:8080/","height":289},"trusted":true,"execution":{"execution_failed":"2025-04-21T17:07:22.030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /content/drive/MyDrive/models","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36vdUH2M4TGY","outputId":"28b9f184-420b-4feb-f9fb-ce4118b6f98c","trusted":true,"execution":{"execution_failed":"2025-04-21T17:07:22.030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MODEL_DIR = get_model_dir()\n# CHECKPOINT_PATH = os.path.join(MODEL_DIR, 'seq2seq_lstm_best.keras')\n\n# checkpoint_cb = ModelCheckpoint(\n#     filepath=CHECKPOINT_PATH,\n#     monitor='val_sparse_categorical_accuracy',\n#     save_best_only=True,\n#     mode='max',\n#     verbose=1\n# )\n# history = model.fit(\n#     train_ds,\n#     validation_data=val_ds,\n#     epochs=EPOCHS,\n#     callbacks=[checkpoint_cb]\n# )\n# print(f\"Best model will be saved to: {CHECKPOINT_PATH}\")","metadata":{"id":"C9i5PQ0q8AhI","outputId":"32a5b44e-efa3-4617-cb3a-fac519494bfc","colab":{"base_uri":"https://localhost:8080/","height":211},"trusted":true,"execution":{"execution_failed":"2025-04-21T17:07:22.030Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 2 using a transformer","metadata":{}},{"cell_type":"code","source":"q_seqs = tokenizer_phase_2.texts_to_sequences(df_subset['question'])\na_seqs = tokenizer_phase_2.texts_to_sequences(df_subset['answers'])\ntrunc_c_seqs = tokenizer_phase_2.texts_to_sequences(contexts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:15.109567Z","iopub.execute_input":"2025-04-21T17:10:15.110080Z","iopub.status.idle":"2025-04-21T17:10:15.874627Z","shell.execute_reply.started":"2025-04-21T17:10:15.110059Z","shell.execute_reply":"2025-04-21T17:10:15.873809Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"embedding_layer = create_embedding_layer(\n    tokenizer=tokenizer_phase_2,\n    glove_path=GLOVE_PATH,\n    embedding_dim=EMB_DIM,\n    mask_zero=True,      \n    trainable=False      \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:19.507496Z","iopub.execute_input":"2025-04-21T17:10:19.508152Z","iopub.status.idle":"2025-04-21T17:10:27.592867Z","shell.execute_reply.started":"2025-04-21T17:10:19.508128Z","shell.execute_reply":"2025-04-21T17:10:27.592290Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"sep_id = tokenizer_phase_2.word_index.get('[SEP]')\nencoder_seqs = [\n    q + [sep_id] + c\n    for q, c in zip(q_seqs, trunc_c_seqs)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:27.594743Z","iopub.execute_input":"2025-04-21T17:10:27.595021Z","iopub.status.idle":"2025-04-21T17:10:27.642484Z","shell.execute_reply.started":"2025-04-21T17:10:27.594995Z","shell.execute_reply":"2025-04-21T17:10:27.641727Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"a_padded = pad_sequences(a_seqs,maxlen=MAX_A_LEN,padding='post',truncating='post')\nencoder_inputs = pad_sequences(encoder_seqs,maxlen=MAX_ENCODER_LEN,padding='post',truncating='post')\ndecoder_inputs = a_padded[:, :-1] \ndecoder_targets= a_padded[:,  1:]\n\n(enc_tr, enc_val,\n decin_tr, decin_val,\n dectar_tr, dectar_val) = train_test_split(\n    encoder_inputs,\n    decoder_inputs,\n    decoder_targets,\n    test_size=0.1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:29.156555Z","iopub.execute_input":"2025-04-21T17:10:29.157102Z","iopub.status.idle":"2025-04-21T17:10:29.267875Z","shell.execute_reply.started":"2025-04-21T17:10:29.157081Z","shell.execute_reply":"2025-04-21T17:10:29.267093Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def make_ds(enc, decin, dectar, batch_size=BATCH_SIZE):\n    ds = tf.data.Dataset.from_tensor_slices(((enc, decin), dectar))\n    return ds.shuffle(2000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\ntrain_ds = make_ds(enc_tr, decin_tr, dectar_tr)\nval_ds   = make_ds(enc_val, decin_val, dectar_val)\nprint(\"   • train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"   • val batches:  \", tf.data.experimental.cardinality(val_ds).numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:30.149329Z","iopub.execute_input":"2025-04-21T17:10:30.149603Z","iopub.status.idle":"2025-04-21T17:10:30.175163Z","shell.execute_reply.started":"2025-04-21T17:10:30.149584Z","shell.execute_reply":"2025-04-21T17:10:30.174592Z"}},"outputs":[{"name":"stdout","text":"   • train batches: 141\n   • val batches:   16\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, max_len: int, embed_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.max_len   = max_len\n        self.embed_dim = embed_dim\n\n        # build the full pos‑encoding once\n        pos = np.arange(max_len)[:, np.newaxis]                 \n        dim = np.arange(embed_dim)[np.newaxis, :]                \n        angle_rates = 1.0 / np.power(10000.0, (2 * (dim//2)) / embed_dim)\n        angle_rads  = pos * angle_rates                          \n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])        \n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])        \n\n        # store as a constant so it's on the TF graph\n        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n        # shape = (1, max_len, embed_dim)\n\n    def call(self, x):\n        # x.shape = (batch, seq_len, embed_dim)\n        seq_len = tf.shape(x)[1]\n        return x + self.pos_encoding[:, :seq_len, :]\n\n    def compute_mask(self, inputs, mask=None):\n        \n        return mask\n\n    def get_config(self):\n        cfg = super().get_config()\n        cfg.update({\"max_len\": self.max_len, \"embed_dim\": self.embed_dim})\n        return cfg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:31.580352Z","iopub.execute_input":"2025-04-21T17:10:31.581032Z","iopub.status.idle":"2025-04-21T17:10:31.588029Z","shell.execute_reply.started":"2025-04-21T17:10:31.581007Z","shell.execute_reply":"2025-04-21T17:10:31.587133Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class Encoder(layers.Layer):\n    def __init__(self,embed_dim: int, num_heads: int, ff_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True \n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.layer1 = layers.LayerNormalization()\n        self.layer2 = layers.LayerNormalization()\n        self.ffn =  tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n    \n    def call(self,pos_matrix, padding_mask, **kwargs):\n        att_out= self.attention(key=pos_matrix, value= pos_matrix, query=pos_matrix,attention_mask=padding_mask)\n        norm1 = self.layer1(att_out+pos_matrix)\n        ff_out = self.ffn(norm1)\n        return self.layer2(ff_out)\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:32.637077Z","iopub.execute_input":"2025-04-21T17:10:32.637764Z","iopub.status.idle":"2025-04-21T17:10:32.642973Z","shell.execute_reply.started":"2025-04-21T17:10:32.637739Z","shell.execute_reply":"2025-04-21T17:10:32.642316Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class Decoder(layers.Layer):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True \n        self.self_mha  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.cross_mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn       = Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.norm1 = layers.LayerNormalization()\n        self.norm2 = layers.LayerNormalization()\n        self.norm3 = layers.LayerNormalization()\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def call(self, x, enc_out,\n             look_ahead_mask=None,\n             padding_mask=None,\n             training=False):\n        att1 = self.self_mha(x, x, x,\n                             attention_mask=look_ahead_mask,\n                             training=training)\n        out1 = self.norm1(x + att1)\n\n        att2 = self.cross_mha(out1, enc_out, enc_out,\n                              attention_mask=padding_mask,\n                              training=training)\n        out2 = self.norm2(out1 + att2)\n\n        ffn_out = self.ffn(out2, training=training)\n        return self.norm3(out2 + ffn_out)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:33.956753Z","iopub.execute_input":"2025-04-21T17:10:33.957022Z","iopub.status.idle":"2025-04-21T17:10:33.963327Z","shell.execute_reply.started":"2025-04-21T17:10:33.957003Z","shell.execute_reply":"2025-04-21T17:10:33.962579Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class Seq2SeqTransformer(Model):\n    def __init__(self,\n                 vocab_size,\n                 embed_dim,\n                 num_heads,\n                 ff_dim,\n                 max_q_len,\n                 max_c_len,\n                 max_a_len,\n                 embedding_layer,\n                 pad_token_id=0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        if embedding_layer is None:\n            raise ValueError(\"`embedding_layer` must be provided\")\n        self.token_emb   = embedding_layer\n        self.pad_id      = pad_token_id\n        self.pos_emb_enc = PositionalEmbedding(max_q_len + 1 + max_c_len, embed_dim)\n        self.pos_emb_dec = PositionalEmbedding(max_a_len, embed_dim)\n        self.encoder     = Encoder(embed_dim, num_heads, ff_dim)\n        self.decoder     = Decoder(embed_dim, num_heads, ff_dim)\n        self.final_dense = layers.Dense(vocab_size, activation=\"softmax\")\n\n    def create_padding_mask(self, seq):\n        mask = tf.cast(tf.equal(seq, self.pad_id), tf.float32)\n        return mask[:, None, None, :]\n\n    def create_look_ahead_mask(self, size):\n        return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n\n    def call(self, inputs, training=False):\n        enc_seq, dec_input = inputs\n    \n        enc_pad_mask4d = self.create_padding_mask(enc_seq)    \n        dec_pad_bool = tf.equal(dec_input, self.pad_id)\n    \n        \n        dec_pad_mask2d = dec_pad_bool[:, tf.newaxis, :]\n    \n        Ldec = tf.shape(dec_input)[1]\n        look2d = self.create_look_ahead_mask(Ldec)         \n        look2d = tf.cast(look2d, tf.bool)\n    \n       \n        look3d = tf.broadcast_to(look2d[tf.newaxis, ...],\n                                 [tf.shape(dec_input)[0], Ldec, Ldec])\n    \n        \n        self_attn_mask = tf.logical_or(dec_pad_mask2d, look3d)\n    \n        x = self.token_emb(enc_seq)\n        x = self.pos_emb_enc(x)\n        enc_out = self.encoder(\n            x,\n            padding_mask=enc_pad_mask4d,  # still 4‑D, Keras will reduce it\n            training=training\n        )\n    \n       \n        y = self.token_emb(dec_input)\n        y = self.pos_emb_dec(y)\n        dec_out = self.decoder(\n            y,\n            enc_out,\n            look_ahead_mask=self_attn_mask,   # 3‑D mask for self‑attention\n            padding_mask=enc_pad_mask4d,      # 4‑D mask for cross‑attention\n            training=training\n        )\n    \n        return self.final_dense(dec_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:35.398667Z","iopub.execute_input":"2025-04-21T17:10:35.399365Z","iopub.status.idle":"2025-04-21T17:10:35.407704Z","shell.execute_reply.started":"2025-04-21T17:10:35.399329Z","shell.execute_reply":"2025-04-21T17:10:35.406988Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model = Seq2SeqTransformer(\n    vocab_size=tokenizer_phase_2.num_words,\n    embed_dim=100,\n    num_heads=8,\n    ff_dim=512,\n    max_q_len=MAX_Q_LEN,\n    max_c_len=MAX_C_TRUNC,\n    max_a_len=MAX_A_LEN - 1,   # because we shift target by 1\n    embedding_layer=embedding_layer,     # or your glove matrix\n    pad_token_id=0,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:38.108403Z","iopub.execute_input":"2025-04-21T17:10:38.108872Z","iopub.status.idle":"2025-04-21T17:10:38.131732Z","shell.execute_reply.started":"2025-04-21T17:10:38.108851Z","shell.execute_reply":"2025-04-21T17:10:38.131034Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"model.summary()\nmodel.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:39.713657Z","iopub.execute_input":"2025-04-21T17:10:39.713915Z","iopub.status.idle":"2025-04-21T17:10:39.746243Z","shell.execute_reply.started":"2025-04-21T17:10:39.713896Z","shell.execute_reply":"2025-04-21T17:10:39.745552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"seq2_seq_transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2_seq_transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                           │       \u001b[38;5;34m5,729,300\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_1               │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder (\u001b[38;5;33mEncoder\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder (\u001b[38;5;33mDecoder\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,300</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_1               │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,729,300\u001b[0m (21.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,300</span> (21.86 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,729,300\u001b[0m (21.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,300</span> (21.86 MB)\n</pre>\n"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"MODEL_DIR = get_model_dir()\nCHECKPOINT_PATH = os.path.join(MODEL_DIR, 'seq2seq_lstm_best.keras')\n\ncheckpoint_cb = ModelCheckpoint(\n    filepath=CHECKPOINT_PATH,\n    monitor='val_sparse_categorical_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=50,\n    callbacks=[checkpoint_cb]\n)\nprint(f\"Best model will be saved to: {CHECKPOINT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:41.563025Z","iopub.execute_input":"2025-04-21T17:10:41.563589Z","iopub.status.idle":"2025-04-21T17:10:52.894300Z","shell.execute_reply.started":"2025-04-21T17:10:41.563568Z","shell.execute_reply":"2025-04-21T17:10:52.893186Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745255452.382317      95 service.cc:148] XLA service 0x7ad58000ae00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745255452.383033      95 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2051485659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/seq2_seq_transformer_1/encoder_1/multi_head_attention_1/softmax_1/add/BroadcastGradientArgs defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [64,8,408,408] vs. [64,64,408,408]\n\nStack trace for op definition: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\nFile \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\nFile \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"/tmp/ipykernel_31/2051485659.py\", line 11, in <cell line: 0>\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n\n\t [[{{node gradient_tape/seq2_seq_transformer_1/encoder_1/multi_head_attention_1/softmax_1/add/BroadcastGradientArgs}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_10659[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_10946]"],"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node gradient_tape/seq2_seq_transformer_1/encoder_1/multi_head_attention_1/softmax_1/add/BroadcastGradientArgs defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [64,8,408,408] vs. [64,64,408,408]\n\nStack trace for op definition: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\nFile \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\nFile \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"/tmp/ipykernel_31/2051485659.py\", line 11, in <cell line: 0>\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n\n\t [[{{node gradient_tape/seq2_seq_transformer_1/encoder_1/multi_head_attention_1/softmax_1/add/BroadcastGradientArgs}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_10659[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_10946]","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}