{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11489407,"sourceType":"datasetVersion","datasetId":7201839},{"sourceId":11500742,"sourceType":"datasetVersion","datasetId":7210129}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Milestone 2","metadata":{"id":"qNMvLH9BB7pR"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pandas as pd\nimport os, zipfile , json , random, requests\nimport re\nfrom pathlib import Path\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import  Sequential\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import serialize_keras_object, deserialize_keras_object\nfrom tensorflow.keras.layers import Embedding\nimport pandas as pd\nfrom collections import Counter","metadata":{"id":"kFomRI3xB9d8","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:56:52.509887Z","iopub.execute_input":"2025-04-23T15:56:52.510141Z","iopub.status.idle":"2025-04-23T15:57:15.104598Z","shell.execute_reply.started":"2025-04-23T15:56:52.510117Z","shell.execute_reply":"2025-04-23T15:57:15.103873Z"}},"outputs":[{"name":"stderr","text":"2025-04-23 15:56:56.211604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745423816.724714      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745423816.842171      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"markdown","source":"### Custom evaluation metrics","metadata":{}},{"cell_type":"code","source":"def exact_match_score(pred, truth):\n    return int(normalize_answer(pred) == normalize_answer(truth))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:57:14.489167Z","iopub.execute_input":"2025-04-23T16:57:14.489428Z","iopub.status.idle":"2025-04-23T16:57:14.492891Z","shell.execute_reply.started":"2025-04-23T16:57:14.489412Z","shell.execute_reply":"2025-04-23T16:57:14.492304Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"\ndef f1_score(pred, truth):\n    pred_tokens = normalize_answer(pred).split()\n    truth_tokens = normalize_answer(truth).split()\n    if not pred_tokens or not truth_tokens:\n        return int(pred_tokens == truth_tokens)\n  \n    common = {}\n    for t in pred_tokens:\n        common[t] = common.get(t, 0) + 1\n    same = 0\n    for t in truth_tokens:\n        if common.get(t, 0) > 0:\n            same += 1\n            common[t] -= 1\n    if same == 0:\n        return 0.0\n    prec = same / len(pred_tokens)\n    rec  = same / len(truth_tokens)\n    return 2 * prec * rec / (prec + rec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:57:15.377949Z","iopub.execute_input":"2025-04-23T16:57:15.378517Z","iopub.status.idle":"2025-04-23T16:57:15.383315Z","shell.execute_reply.started":"2025-04-23T16:57:15.378495Z","shell.execute_reply":"2025-04-23T16:57:15.382700Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"### For directory work","metadata":{}},{"cell_type":"code","source":"\ndef is_kaggle():\n    return 'KAGGLE_URL_BASE' in os.environ\n\ndef is_colab():\n    return (not is_kaggle()) and os.path.exists('/content')\n\ndef maybe_mount_drive():\n    if is_colab():\n        from google.colab import drive\n        if not os.path.isdir('/content/drive'):\n            drive.mount('/content/drive')\n\n\ndef get_data_path():\n    if is_kaggle():\n        return '/kaggle/input/squad-2-0/'\n    elif is_colab():\n        return '/content/drive/MyDrive/SQuAD'\n    else:\n        return './data/'\ndef get_model_dir():\n    if is_colab():\n        model_dir = '/content/drive/MyDrive/models'\n    elif is_kaggle():\n        model_dir = '/kaggle/working/models'\n    else:\n        model_dir = './models'\n    os.makedirs(model_dir, exist_ok=True)\n    return model_dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Other","metadata":{}},{"cell_type":"code","source":"def normalize_answer(s):\n    s = s.lower()\n    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n    s = \" \".join(s.split())\n    return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:58:37.409817Z","iopub.execute_input":"2025-04-23T16:58:37.410288Z","iopub.status.idle":"2025-04-23T16:58:37.414663Z","shell.execute_reply.started":"2025-04-23T16:58:37.410263Z","shell.execute_reply":"2025-04-23T16:58:37.413809Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"## Explorting dataset:","metadata":{"id":"1zzFTlcTCEc7"}},{"cell_type":"code","source":"dataset_dir = get_data_path()\nmaybe_mount_drive()\nos.makedirs(dataset_dir, exist_ok=True)","metadata":{"id":"e0NuNGE435ad","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:41.044236Z","iopub.execute_input":"2025-04-23T15:57:41.044761Z","iopub.status.idle":"2025-04-23T15:57:41.048614Z","shell.execute_reply.started":"2025-04-23T15:57:41.044739Z","shell.execute_reply":"2025-04-23T15:57:41.047904Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"file_path = os.path.join(dataset_dir, 'train-v2.0.json')","metadata":{"id":"TSDZT50PX9sj","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:41.989217Z","iopub.execute_input":"2025-04-23T15:57:41.989705Z","iopub.status.idle":"2025-04-23T15:57:41.993043Z","shell.execute_reply.started":"2025-04-23T15:57:41.989685Z","shell.execute_reply":"2025-04-23T15:57:41.992334Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"with open(file_path, 'r', encoding='utf-8') as f:\n    squad = json.load(f)","metadata":{"id":"s_tEP-KyHMFT","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:42.624408Z","iopub.execute_input":"2025-04-23T15:57:42.625005Z","iopub.status.idle":"2025-04-23T15:57:43.569699Z","shell.execute_reply.started":"2025-04-23T15:57:42.624987Z","shell.execute_reply":"2025-04-23T15:57:43.568776Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"records = []\nfor article in squad['data']:\n    for para in article['paragraphs']:\n        ctx = para['context']\n        for qa in para['qas']:\n            answers = [a['text'] for a in qa.get('answers', [])]\n            starts  = [a['answer_start'] for a in qa.get('answers', [])]\n            ends    = [s + len(t) for s,t in zip(starts, answers)]\n            records.append({\n                'question': qa['question'],\n                'answers': answers,\n                'context': ctx,\n                'answer_start': starts,\n                'answer_end': ends\n            })\n\n","metadata":{"id":"JixYG4s6saGU","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:43.570999Z","iopub.execute_input":"2025-04-23T15:57:43.571334Z","iopub.status.idle":"2025-04-23T15:57:44.279894Z","shell.execute_reply.started":"2025-04-23T15:57:43.571306Z","shell.execute_reply":"2025-04-23T15:57:44.279273Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = pd.DataFrame(records)\ndf.head()","metadata":{"id":"NhZlSX4qtmtj","outputId":"96567ffd-6526-4218-c322-84c26cd2e7e6","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:44.942139Z","iopub.execute_input":"2025-04-23T15:57:44.942673Z","iopub.status.idle":"2025-04-23T15:57:45.071703Z","shell.execute_reply.started":"2025-04-23T15:57:44.942654Z","shell.execute_reply":"2025-04-23T15:57:45.070947Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                            question                answers  \\\n0           When did Beyonce start becoming popular?    [in the late 1990s]   \n1  What areas did Beyonce compete in when she was...  [singing and dancing]   \n2  When did Beyonce leave Destiny's Child and bec...                 [2003]   \n3      In what city and state did Beyonce  grow up?        [Houston, Texas]   \n4         In which decade did Beyonce become famous?           [late 1990s]   \n\n                                             context answer_start answer_end  \n0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [269]      [286]  \n1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [207]      [226]  \n2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [526]      [530]  \n3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [166]      [180]  \n4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [276]      [286]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When did Beyonce start becoming popular?</td>\n      <td>[in the late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[269]</td>\n      <td>[286]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What areas did Beyonce compete in when she was...</td>\n      <td>[singing and dancing]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[207]</td>\n      <td>[226]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When did Beyonce leave Destiny's Child and bec...</td>\n      <td>[2003]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[526]</td>\n      <td>[530]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In what city and state did Beyonce  grow up?</td>\n      <td>[Houston, Texas]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[166]</td>\n      <td>[180]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In which decade did Beyonce become famous?</td>\n      <td>[late 1990s]</td>\n      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n      <td>[276]</td>\n      <td>[286]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(\"Total QA pairs:\", len(df))","metadata":{"id":"D72WTTHOupEF","outputId":"6686b957-2f37-44bc-c831-47864e7260dd","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:57:48.950443Z","iopub.execute_input":"2025-04-23T15:57:48.951262Z","iopub.status.idle":"2025-04-23T15:57:48.955726Z","shell.execute_reply.started":"2025-04-23T15:57:48.951229Z","shell.execute_reply":"2025-04-23T15:57:48.954890Z"}},"outputs":[{"name":"stdout","text":"Total QA pairs: 130319\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"id":"-EdBrPmqbBvA"}},{"cell_type":"markdown","source":"Dropping rows where answers are empty","metadata":{"id":"E_Cb25_x11aT"}},{"cell_type":"code","source":"df = df[df['answers'].map(len) > 0].reset_index(drop=True)\nprint(\"Rows remaining after drop:\", len(df))","metadata":{"id":"JCoQdBYJ19ON","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98bfb707-f5f5-4339-8879-dd7c8426a6d9","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:06.390610Z","iopub.execute_input":"2025-04-23T15:59:06.390893Z","iopub.status.idle":"2025-04-23T15:59:06.466237Z","shell.execute_reply.started":"2025-04-23T15:59:06.390873Z","shell.execute_reply":"2025-04-23T15:59:06.465453Z"}},"outputs":[{"name":"stdout","text":"Rows remaining after drop: 86821\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndf_subset = df.head(15000).copy().reset_index(drop=True)\n\nprint(\"Subset size:\", df_subset.shape)\ndf_subset.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:11.712383Z","iopub.execute_input":"2025-04-23T15:59:11.713066Z","iopub.status.idle":"2025-04-23T15:59:11.771459Z","shell.execute_reply.started":"2025-04-23T15:59:11.713041Z","shell.execute_reply":"2025-04-23T15:59:11.770848Z"}},"outputs":[{"name":"stdout","text":"Subset size: (15000, 5)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                            question                  answers  \\\n0  After cellulose, what component is most plenti...          [hemicellulose]   \n1  How much of the Bronx's vote in 1916 did Hughe...                  [42.6%]   \n2  When did Homo sapiens begin using subsistence ...  [0.2 million years ago]   \n3            When did she say the she is a feminist?             [April 2013]   \n4  Along with Somalo-Islamic architecture, what i...        [Western designs]   \n\n                                             context answer_start answer_end  \n0  Aside from water, wood has three main componen...        [152]      [165]  \n1  Since then, the Bronx has always supported the...        [602]      [607]  \n2  Hunting and gathering was presumably the subsi...        [174]      [195]  \n3  In an interview published by Vogue in April 20...         [38]       [48]  \n4  Somali architecture is a rich and diverse trad...        [500]      [515]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answers</th>\n      <th>context</th>\n      <th>answer_start</th>\n      <th>answer_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>After cellulose, what component is most plenti...</td>\n      <td>[hemicellulose]</td>\n      <td>Aside from water, wood has three main componen...</td>\n      <td>[152]</td>\n      <td>[165]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How much of the Bronx's vote in 1916 did Hughe...</td>\n      <td>[42.6%]</td>\n      <td>Since then, the Bronx has always supported the...</td>\n      <td>[602]</td>\n      <td>[607]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When did Homo sapiens begin using subsistence ...</td>\n      <td>[0.2 million years ago]</td>\n      <td>Hunting and gathering was presumably the subsi...</td>\n      <td>[174]</td>\n      <td>[195]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>When did she say the she is a feminist?</td>\n      <td>[April 2013]</td>\n      <td>In an interview published by Vogue in April 20...</td>\n      <td>[38]</td>\n      <td>[48]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Along with Somalo-Islamic architecture, what i...</td>\n      <td>[Western designs]</td>\n      <td>Somali architecture is a rich and diverse trad...</td>\n      <td>[500]</td>\n      <td>[515]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Removing Extra Whitespaces","metadata":{"id":"QBSLXMvASpe0"}},{"cell_type":"code","source":"def collapse_whitespace(s):\n    if isinstance(s, str):\n        return re.sub(r'\\s+', ' ', s.strip())\n    return s","metadata":{"id":"FZAnlrcyStOE","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:17.321614Z","iopub.execute_input":"2025-04-23T15:59:17.322225Z","iopub.status.idle":"2025-04-23T15:59:17.325743Z","shell.execute_reply.started":"2025-04-23T15:59:17.322201Z","shell.execute_reply":"2025-04-23T15:59:17.324931Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"for col in ['question', 'context', 'answers']:\n    if col in df_subset.columns:\n        df_subset[col] = df_subset[col].apply(collapse_whitespace)","metadata":{"id":"fnVh34G5VBAm","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:19.136035Z","iopub.execute_input":"2025-04-23T15:59:19.136314Z","iopub.status.idle":"2025-04-23T15:59:19.851704Z","shell.execute_reply.started":"2025-04-23T15:59:19.136297Z","shell.execute_reply":"2025-04-23T15:59:19.851056Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Lets explore the length of the sequences which will determine some hyperparameters in training the models**","metadata":{"id":"JFeXiO2dzf8Q"}},{"cell_type":"code","source":"df_subset['question'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc530Af3zoTV","outputId":"f93ee1cc-11ca-4548-ea54-98ddafec6e05","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:24.643321Z","iopub.execute_input":"2025-04-23T15:59:24.643884Z","iopub.status.idle":"2025-04-23T15:59:24.656720Z","shell.execute_reply.started":"2025-04-23T15:59:24.643864Z","shell.execute_reply":"2025-04-23T15:59:24.655847Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"270"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"df_subset['context'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDvLl2vx0a4o","outputId":"103fac31-6acf-45c6-cf69-bb1656c1fe4c","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:26.544614Z","iopub.execute_input":"2025-04-23T15:59:26.545180Z","iopub.status.idle":"2025-04-23T15:59:26.555913Z","shell.execute_reply.started":"2025-04-23T15:59:26.545156Z","shell.execute_reply":"2025-04-23T15:59:26.555212Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"3385"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"We just turn the array of the answers to a string since none have multiple answers","metadata":{"id":"6hWOoWFg1GZk"}},{"cell_type":"code","source":"df_subset['answers']= df_subset['answers'].apply(lambda x: x[0])","metadata":{"id":"rh00m5Jw0lWm","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:30.156132Z","iopub.execute_input":"2025-04-23T15:59:30.156753Z","iopub.status.idle":"2025-04-23T15:59:30.168464Z","shell.execute_reply.started":"2025-04-23T15:59:30.156733Z","shell.execute_reply":"2025-04-23T15:59:30.167957Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df_subset['answers'].str.len().max()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoL1ZTTx0z8b","outputId":"e4736878-4cb3-4c68-df14-41efa24a60e8","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:32.333201Z","iopub.execute_input":"2025-04-23T15:59:32.333794Z","iopub.status.idle":"2025-04-23T15:59:32.344734Z","shell.execute_reply.started":"2025-04-23T15:59:32.333774Z","shell.execute_reply":"2025-04-23T15:59:32.344215Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"175"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Embeddings","metadata":{"id":"B_-pDASk5eOa"}},{"cell_type":"code","source":"!pip install --quiet gensim","metadata":{"id":"gBEVZxrJ5gav","outputId":"e51f32d7-1a78-4015-9a21-d4a580224374","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:35.453262Z","iopub.execute_input":"2025-04-23T15:59:35.453696Z","iopub.status.idle":"2025-04-23T15:59:48.599789Z","shell.execute_reply.started":"2025-04-23T15:59:35.453675Z","shell.execute_reply":"2025-04-23T15:59:48.599046Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"**Tokenizer for phase 1 only**","metadata":{}},{"cell_type":"code","source":"# tokenizer_phase_1 = Tokenizer(\n#     num_words=20000,\n#     oov_token='[UNK]',\n#     filters='''!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'''\n# )\n# tokenizer_phase_1.fit_on_texts(df_subset[\"question\"].tolist()+df_subset[\"answers\"].tolist())\n\n# q_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['question'])\n# a_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['answers'])\n\n","metadata":{"id":"l2U-kh4D5mdK","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T15:59:48.601442Z","iopub.execute_input":"2025-04-23T15:59:48.601686Z","iopub.status.idle":"2025-04-23T15:59:49.127536Z","shell.execute_reply.started":"2025-04-23T15:59:48.601666Z","shell.execute_reply":"2025-04-23T15:59:49.126950Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"vocab_size = len(tokenizer_phase_1.word_index)\n# print(\"Total unique tokens:\", vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:50:52.364787Z","iopub.execute_input":"2025-04-23T00:50:52.365028Z","iopub.status.idle":"2025-04-23T00:50:52.369251Z","shell.execute_reply.started":"2025-04-23T00:50:52.365011Z","shell.execute_reply":"2025-04-23T00:50:52.368448Z"}},"outputs":[{"name":"stdout","text":"Total unique tokens: 18733\n","output_type":"stream"}],"execution_count":300},{"cell_type":"markdown","source":"**Tokenizer for phase 2**","metadata":{}},{"cell_type":"code","source":"def truncate_context(context: str, ans_start: int, ans_end: int, max_len: int) -> str:\n    \"\"\"\n    Return a substring of `context` of length up to max_len characters,\n    centered on the character span [ans_start:ans_end], adjusted to word boundaries.\n    \"\"\"\n    ans_len = ans_end - ans_start\n    extra   = max_len - ans_len\n    pre     = extra // 2\n    post    = extra - pre\n\n    start = ans_start - pre\n    end   = ans_end   + post\n\n    if start < 0:\n        start = 0\n        end   = min(max_len, len(context))\n\n    if end > len(context):\n        end   = len(context)\n        start = max(0, len(context) - max_len)\n\n    if start > 0 and not context[start].isspace():\n        m = re.search(r'\\s', context[:start][::-1])\n        if m:\n            # position of last whitespace before start\n            start = start - m.start()\n\n    if end < len(context) and not context[end].isspace():\n        m = re.search(r'\\s', context[end:])\n        if m:\n            end = end + m.start()\n\n    return context[start:end]\n\ndef build_truncated_context(df, max_len: int):\n    \"\"\"\n    Returns a list of truncated context strings for each row in df,\n    preserving at least the answer span and cutting only at word boundaries.\n    \"\"\"\n    contexts = []\n    for ctx, starts, ends in zip(df['context'], df['answer_start'], df['answer_end']):\n        # pick the first span\n        s = starts[0]\n        e = ends[0]\n\n        window = truncate_context(ctx, s, e, max_len)\n        contexts.append(window)\n\n    return contexts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:12.014748Z","iopub.execute_input":"2025-04-23T16:00:12.015235Z","iopub.status.idle":"2025-04-23T16:00:12.021790Z","shell.execute_reply.started":"2025-04-23T16:00:12.015214Z","shell.execute_reply":"2025-04-23T16:00:12.020928Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\nquestions = df_subset['question'].astype(str).tolist()\nanswers   = df_subset['answers'].astype(str).tolist()\ncontexts  = build_truncated_context(df_subset, 2000)\ntokenizer_phase_2 = Tokenizer(\n    num_words=50000,\n    oov_token='[UNK]',\n    filters='''!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'''\n)\ntokenizer_phase_2.fit_on_texts(questions + answers + contexts + ['[SEP]'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:16.607253Z","iopub.execute_input":"2025-04-23T16:00:16.607515Z","iopub.status.idle":"2025-04-23T16:00:18.266622Z","shell.execute_reply.started":"2025-04-23T16:00:16.607495Z","shell.execute_reply":"2025-04-23T16:00:18.266046Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"**Adding sep sos and eos tag**","metadata":{}},{"cell_type":"code","source":"sep_token = \"[SEP]\"\nsep_id = tokenizer_phase_2.word_index.get(sep_token)\nif sep_id is None:\n    sep_id = tokenizer_phase_2.num_words - 1\n    occupant = tokenizer_phase_2.index_word.get(sep_id)\n    if occupant:\n        del tokenizer_phase_2.word_index[occupant] \n    tokenizer_phase_2.word_index[sep_token] = sep_id\n    tokenizer_phase_2.index_word[sep_id]  = sep_token\nprint(\"✅ SEP_TOKEN id =\", sep_id)\n\n\nfor token, offset in {\"[SOS]\":1, \"[EOS]\":2}.items():\n    tok_id = tokenizer_phase_2.word_index.get(token)\n    if tok_id is None:\n        new_id = tokenizer_phase_2.num_words - offset - 1\n        occupant = tokenizer_phase_2.index_word.get(new_id)\n        if occupant:\n            del tokenizer_phase_2.word_index[occupant]\n        tokenizer_phase_2.word_index[token]      = new_id\n        tokenizer_phase_2.index_word[new_id]     = token\n        print(f\"✅ {token} injected at id {new_id}\")\n    else:\n        print(f\"✅ {token} already at id {tok_id}\")\n\nsep_id = tokenizer_phase_2.word_index['[SEP]']\nsos_id = tokenizer_phase_2.word_index['[SOS]']\neos_id = tokenizer_phase_2.word_index['[EOS]']\npad_id = 0\n\nprint(\"Special IDs:\", {'SEP':sep_id, 'SOS':sos_id, 'EOS':eos_id, 'PAD':pad_id})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:18.649183Z","iopub.execute_input":"2025-04-23T16:00:18.649435Z","iopub.status.idle":"2025-04-23T16:00:18.656190Z","shell.execute_reply.started":"2025-04-23T16:00:18.649417Z","shell.execute_reply":"2025-04-23T16:00:18.655385Z"}},"outputs":[{"name":"stdout","text":"✅ SEP_TOKEN id = 49999\n✅ [SOS] injected at id 49998\n✅ [EOS] injected at id 49997\nSpecial IDs: {'SEP': 49999, 'SOS': 49998, 'EOS': 49997, 'PAD': 0}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"q_seqs = tokenizer_phase_2.texts_to_sequences(questions)\nc_seqs = tokenizer_phase_2.texts_to_sequences(contexts)\na_raw  = tokenizer_phase_2.texts_to_sequences(answers)\na_seqs = [[sos_id] + seq + [eos_id] for seq in a_raw]\n\nMAX_Q_LEN  = max(len(s) for s in q_seqs)\nMAX_C_LEN  = max(len(s) for s in c_seqs)\nMAX_A_LEN  = max(len(s) for s in a_seqs)\nMAX_ENCODER_LEN = MAX_Q_LEN + 1 + MAX_C_LEN\n\nprint(\"Lengths:\", {'Q':MAX_Q_LEN, 'C':MAX_C_LEN, 'A':MAX_A_LEN, 'Enc':MAX_ENCODER_LEN})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:32.659359Z","iopub.execute_input":"2025-04-23T16:00:32.659619Z","iopub.status.idle":"2025-04-23T16:00:33.784125Z","shell.execute_reply.started":"2025-04-23T16:00:32.659599Z","shell.execute_reply":"2025-04-23T16:00:33.783382Z"}},"outputs":[{"name":"stdout","text":"Lengths: {'Q': 40, 'C': 362, 'A': 32, 'Enc': 403}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"**Add padding to the encoder and decoder inputs**","metadata":{}},{"cell_type":"code","source":"enc_seqs = [q + [sep_id] + c for q, c in zip(q_seqs, c_seqs)]\nencoder_inputs = pad_sequences(\n    enc_seqs,\n    maxlen=MAX_ENCODER_LEN,\n    padding='post',\n    truncating='post',\n    value=pad_id\n)\n\na_padded = pad_sequences(\n    a_seqs,\n    maxlen=MAX_A_LEN,\n    padding='post',\n    truncating='post',\n    value=pad_id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:36.553037Z","iopub.execute_input":"2025-04-23T16:00:36.553702Z","iopub.status.idle":"2025-04-23T16:00:36.748898Z","shell.execute_reply.started":"2025-04-23T16:00:36.553680Z","shell.execute_reply":"2025-04-23T16:00:36.748324Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"**Split the data**","metadata":{}},{"cell_type":"code","source":"decoder_inputs  = a_padded[:, :-1] \ndecoder_targets = a_padded[:,  1:] \n\nprint(\"encoder_inputs:\", encoder_inputs.shape)\nprint(\"decoder_inputs:\", decoder_inputs.shape)\nprint(\"decoder_targets:\", decoder_targets.shape)\nassert decoder_inputs.shape == decoder_targets.shape\n\n(enc_tr, enc_val,\n decin_tr, decin_val,\n dectar_tr, dectar_val) = train_test_split(\n    encoder_inputs,\n    decoder_inputs,\n    decoder_targets,\n    test_size=0.1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:45.331498Z","iopub.execute_input":"2025-04-23T16:00:45.332181Z","iopub.status.idle":"2025-04-23T16:00:45.354782Z","shell.execute_reply.started":"2025-04-23T16:00:45.332157Z","shell.execute_reply":"2025-04-23T16:00:45.354138Z"}},"outputs":[{"name":"stdout","text":"encoder_inputs: (15000, 403)\ndecoder_inputs: (15000, 31)\ndecoder_targets: (15000, 31)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"**Load gloVe dictionary**","metadata":{"id":"bEwBdMqoGBO_"}},{"cell_type":"code","source":"def prepare_glove(target_dim=100, work_subdir='glove',\n                  input_dataset_slug='glove6b',\n                  download_url='http://nlp.stanford.edu/data/glove.6B.zip'):\n    maybe_mount_drive()\n\n    if is_kaggle():\n        work_dir = f'/kaggle/working/{work_subdir}'\n        uploaded_zip = f'/kaggle/input/{input_dataset_slug}/glove.6B.zip'\n    elif is_colab():\n        work_dir = f'/content/drive/MyDrive/{work_subdir}'\n        uploaded_zip = None\n    else:\n        work_dir = f'./data/{work_subdir}'\n        uploaded_zip = None\n\n    os.makedirs(work_dir, exist_ok=True)\n\n    target_file = f'glove.6B.{target_dim}d.txt'\n    txt_path = os.path.join(work_dir, target_file)\n    zip_path = os.path.join(work_dir, os.path.basename(download_url))\n\n    if os.path.exists(txt_path):\n        return txt_path\n\n    if is_kaggle() and uploaded_zip and os.path.exists(uploaded_zip):\n        zip_path = uploaded_zip\n    else:\n        if requests is None:\n            raise RuntimeError(\"`requests` not available; offline mode\")\n        with requests.get(download_url, stream=True) as r, open(zip_path, 'wb') as f:\n            r.raise_for_status()\n            for chunk in r.iter_content(8192):\n                f.write(chunk)\n\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        z.extract(target_file, path=work_dir)\n\n    if not os.path.exists(txt_path):\n        raise RuntimeError(f\"Failed to extract {target_file}\")\n\n    return txt_path\n\nglove_path = prepare_glove()\nprint(\"GloVe file:\", glove_path)","metadata":{"id":"sHHCMrjhGG4V","outputId":"cc629348-918c-4ab8-85a2-0297f924a578","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:00:58.026636Z","iopub.execute_input":"2025-04-23T16:00:58.027188Z","iopub.status.idle":"2025-04-23T16:00:58.035220Z","shell.execute_reply.started":"2025-04-23T16:00:58.027164Z","shell.execute_reply":"2025-04-23T16:00:58.034504Z"}},"outputs":[{"name":"stdout","text":"GloVe file: /kaggle/working/glove/glove.6B.100d.txt\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"def create_embedding_layer(\n    tokenizer,\n    glove_path: str,\n    embedding_dim: int,\n    mask_zero: bool = True,\n    trainable: bool = False,\n    oov_token: str = '[UNK]'\n) -> Embedding:\n    \"\"\"\n    Build a Keras Embedding layer from a fitted tokenizer and a GloVe file.\n\n    Args:\n        tokenizer: a fitted keras.preprocessing.text.Tokenizer\n        glove_path: path to a GloVe‑style file (word + embedding_dim floats)\n        max_num_words: max vocabulary size (typically tokenizer.num_words)\n        embedding_dim: dimensionality of the GloVe vectors\n        mask_zero: if True, reserve index 0 for padding (and mask it)\n        trainable: if False, freeze the embedding weights\n        oov_token: the out‑of‑vocab token (must match tokenizer.oov_token)\n\n    Returns:\n        A tf.keras.layers.Embedding instance with pretrained weights.\n    \"\"\"\n   \n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.rstrip().split(\" \")\n            word = parts[0]\n            coefs = np.asarray(parts[1:], dtype='float32')\n            if coefs.shape[0] != embedding_dim:\n                continue  \n            embeddings_index[word] = coefs\n\n    \n    vocab_size =  len(tokenizer.word_index) + 1\n    embedding_matrix = np.random.normal(\n        scale=0.01,\n        size=(vocab_size, embedding_dim)\n    ).astype('float32')\n\n    \n    for word, idx in tokenizer.word_index.items():\n        if idx == 0 or idx >= vocab_size:\n            continue\n        vec = embeddings_index.get(word)\n        if vec is not None:\n            embedding_matrix[idx] = vec\n\n    embedding_layer = Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_dim,\n        weights=[embedding_matrix],\n        mask_zero=mask_zero,\n        trainable=trainable,\n        name='pretrained_embedding'\n    )\n    return embedding_layer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:01:24.362435Z","iopub.execute_input":"2025-04-23T16:01:24.362724Z","iopub.status.idle":"2025-04-23T16:01:24.369416Z","shell.execute_reply.started":"2025-04-23T16:01:24.362704Z","shell.execute_reply":"2025-04-23T16:01:24.368828Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Phase One","metadata":{"id":"Ci0NXaY-UBOi","jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"MAX_Q_LEN = max(len(s) for s in q_seqs)\nMAX_A_LEN = max(len(s) for s in a_seqs)\nMAX_C_LEN   = max(len(s) for s in trunc_c_seqs)\nMAX_ENCODER_LEN = max(len(s) for s in q_seqs) + 1 + MAX_C_TRUNC\nVOCAB_SIZE  = 20000\nEMB_DIM     = 100\nUNITS       = 128\nBATCH_SIZE  = 64","metadata":{"id":"XSOJYRpOKaqa","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.857583Z","iopub.status.idle":"2025-04-23T00:30:00.857890Z","shell.execute_reply.started":"2025-04-23T00:30:00.857742Z","shell.execute_reply":"2025-04-23T00:30:00.857754Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q_padded = pad_sequences(q_seqs, maxlen=MAX_Q_LEN, padding='post', truncating='post')\na_padded = pad_sequences(a_seqs, maxlen=MAX_A_LEN, padding='post', truncating='post')","metadata":{"id":"JERoPRt-RHlC","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.858973Z","iopub.status.idle":"2025-04-23T00:30:00.859261Z","shell.execute_reply.started":"2025-04-23T00:30:00.859115Z","shell.execute_reply":"2025-04-23T00:30:00.859127Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EMB_DIM      = 100\nGLOVE_PATH   = prepare_glove()\n\nembedding_layer = create_embedding_layer(\n    tokenizer=tokenizer_phase_1,\n    glove_path=GLOVE_PATH,\n    embedding_dim=EMB_DIM,\n    mask_zero=True,      \n    trainable=False      \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.860887Z","iopub.status.idle":"2025-04-23T00:30:00.861195Z","shell.execute_reply.started":"2025-04-23T00:30:00.861048Z","shell.execute_reply":"2025-04-23T00:30:00.861061Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder_input  = a_padded[:, :-1]\ndecoder_target = a_padded[:, 1:]\n\nXq_tr, Xq_val, Din_tr, Din_val, Dt_tr, Dt_val = train_test_split(\n    q_padded, decoder_input, decoder_target,\n    test_size=0.1, random_state=42\n)\n\ndef make_ds(q, d_in, d_tar, batch_size=64):\n    mask = tf.cast(tf.not_equal(d_tar, 0), tf.float32)\n    ds = tf.data.Dataset.from_tensor_slices(\n        ((q, d_in), d_tar, mask)\n    )\n    return ds.shuffle(2000).batch(batch_size).prefetch(1)\n\ntrain_ds = make_ds(Xq_tr, Din_tr, Dt_tr, batch_size=BATCH_SIZE)\nval_ds   = make_ds(Xq_val, Din_val, Dt_val, batch_size=BATCH_SIZE)","metadata":{"id":"zaezBfWvSmaO","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.861823Z","iopub.status.idle":"2025-04-23T00:30:00.862111Z","shell.execute_reply.started":"2025-04-23T00:30:00.861962Z","shell.execute_reply":"2025-04-23T00:30:00.861975Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"Val batches:\", tf.data.experimental.cardinality(val_ds).numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.863124Z","iopub.status.idle":"2025-04-23T00:30:00.863348Z","shell.execute_reply.started":"2025-04-23T00:30:00.863252Z","shell.execute_reply":"2025-04-23T00:30:00.863262Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Building the model**","metadata":{"id":"s-DjaB2H1448"}},{"cell_type":"code","source":"class Seq2SeqLSTM(tf.keras.Model):\n    def __init__(self,vocab_size,emb_dim,units,max_q_len,max_a_len,embedding_matrix=None,pad_token_id=0,**kwargs):\n        super().__init__(**kwargs)\n        self.pad_token_id = pad_token_id\n\n\n        if embedding_matrix is not None:\n            self.embedding = Embedding(vocab_size, emb_dim,weights=[embedding_matrix],trainable=False,mask_zero=True)\n        else:\n            self.embedding = Embedding(vocab_size, emb_dim,mask_zero=True)\n\n        #units is the vector size of the hidden state\n        #return_state if true returns the final h and c\n        self.encoder_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n\n        #return sequence returns all the hidden states from h_1 to h_n\n        #return sequence is for evaluation\n        #return state is for inference because after each token generated we need to feed the model the states again\n        self.decoder_lstm = LSTM(units,return_sequences=True,return_state=True, name='decoder_lstm')\n\n        #the layer needed to predict the next word\n        self.dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n\n    def call(self, inputs, training=False):\n        encoder_inputs, decoder_inputs = inputs\n\n        x_enc = self.embedding(encoder_inputs)\n        _, state_h, state_c = self.encoder_lstm(x_enc, training=training)\n        encoder_states = [state_h, state_c]\n\n        x_dec = self.embedding(decoder_inputs)\n        dec_outputs, _, _ = self.decoder_lstm(x_dec, initial_state=encoder_states, training=training)\n        return self.dense(dec_outputs)\n    \n","metadata":{"id":"XONLiGcUxUgA","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.864321Z","iopub.status.idle":"2025-04-23T00:30:00.864533Z","shell.execute_reply.started":"2025-04-23T00:30:00.864432Z","shell.execute_reply":"2025-04-23T00:30:00.864441Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Seq2SeqLSTM( vocab_size=vocab_size,\n                    emb_dim=100,units=128,\n                     max_q_len=MAX_Q_LEN,max_a_len=MAX_A_LEN,embedding_matrix=embedding_matrix,pad_token_id=0)\n\ndummy_q = tf.zeros((1, MAX_Q_LEN), dtype=tf.int32)\ndummy_a = tf.zeros((1, MAX_A_LEN-1), dtype=tf.int32)\n_ = model((dummy_q, dummy_a))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.865198Z","iopub.status.idle":"2025-04-23T00:30:00.865474Z","shell.execute_reply.started":"2025-04-23T00:30:00.865357Z","shell.execute_reply":"2025-04-23T00:30:00.865370Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n)\nmodel.summary()","metadata":{"id":"WY8ot7KK74I-","outputId":"b3a84da1-e1f8-44e5-8cdf-ba203079b9be","colab":{"base_uri":"https://localhost:8080/","height":289},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.866234Z","iopub.status.idle":"2025-04-23T00:30:00.866541Z","shell.execute_reply.started":"2025-04-23T00:30:00.866374Z","shell.execute_reply":"2025-04-23T00:30:00.866388Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !mkdir /content/drive/MyDrive/models","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36vdUH2M4TGY","outputId":"28b9f184-420b-4feb-f9fb-ce4118b6f98c","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.867851Z","iopub.status.idle":"2025-04-23T00:30:00.868047Z","shell.execute_reply.started":"2025-04-23T00:30:00.867955Z","shell.execute_reply":"2025-04-23T00:30:00.867963Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MODEL_DIR = get_model_dir()\n# CHECKPOINT_PATH = os.path.join(MODEL_DIR, 'seq2seq_lstm_best.keras')\n\n# checkpoint_cb = ModelCheckpoint(\n#     filepath=CHECKPOINT_PATH,\n#     monitor='val_sparse_categorical_accuracy',\n#     save_best_only=True,\n#     mode='max',\n#     verbose=1\n# )\n# history = model.fit(\n#     train_ds,\n#     validation_data=val_ds,\n#     epochs=EPOCHS,\n#     callbacks=[checkpoint_cb]\n# )\n# print(f\"Best model will be saved to: {CHECKPOINT_PATH}\")","metadata":{"id":"C9i5PQ0q8AhI","outputId":"32a5b44e-efa3-4617-cb3a-fac519494bfc","colab":{"base_uri":"https://localhost:8080/","height":211},"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:30:00.868692Z","iopub.status.idle":"2025-04-23T00:30:00.868921Z","shell.execute_reply.started":"2025-04-23T00:30:00.868826Z","shell.execute_reply":"2025-04-23T00:30:00.868834Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 2 using a transformer","metadata":{}},{"cell_type":"code","source":"EMB_DIM      = 100\nGLOVE_PATH   = prepare_glove()\n\nembedding_layer = create_embedding_layer(\n    tokenizer=tokenizer_phase_2,\n    glove_path=GLOVE_PATH,\n    embedding_dim=EMB_DIM,\n    mask_zero=False,      \n    trainable=False      \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:00:01.121970Z","iopub.execute_input":"2025-04-23T17:00:01.122675Z","iopub.status.idle":"2025-04-23T17:00:09.485040Z","shell.execute_reply.started":"2025-04-23T17:00:01.122650Z","shell.execute_reply":"2025-04-23T17:00:09.484310Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"PAD_ID = 0  \nBATCH_SIZE= 64\ndef make_ds(enc, decin, dectar, batch_size=BATCH_SIZE):\n    \n    ds = tf.data.Dataset.from_tensor_slices(((enc, decin), dectar))\n    \n    def add_sample_weight(inputs, target):\n        weights = tf.cast(tf.not_equal(target, PAD_ID), tf.float32)\n        return inputs, target, weights\n    \n    return (\n        ds\n        .shuffle(2000)\n        .map(add_sample_weight, num_parallel_calls=tf.data.AUTOTUNE)\n        .batch(batch_size)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n\ntrain_ds = make_ds(enc_tr, decin_tr, dectar_tr)\nval_ds   = make_ds(enc_val, decin_val, dectar_val)\n\nprint(\"   • train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"   • val batches:  \", tf.data.experimental.cardinality(val_ds).numpy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:00:25.148293Z","iopub.execute_input":"2025-04-23T17:00:25.148552Z","iopub.status.idle":"2025-04-23T17:00:25.245975Z","shell.execute_reply.started":"2025-04-23T17:00:25.148534Z","shell.execute_reply":"2025-04-23T17:00:25.245425Z"}},"outputs":[{"name":"stdout","text":"   • train batches: 211\n   • val batches:   24\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, max_len: int, embed_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        # self.supports_masking = True\n        self.max_len   = max_len\n        self.embed_dim = embed_dim\n        \n        pos = np.arange(max_len)[:, np.newaxis]                 \n        dim = np.arange(embed_dim)[np.newaxis, :]                \n        angle_rates = 1.0 / np.power(10000.0, (2 * (dim//2)) / embed_dim)\n        angle_rads  = pos * angle_rates                          \n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])        \n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])        \n        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n        \n\n    def call(self, x):\n        seq_len = tf.shape(x)[1]\n        return x + self.pos_encoding[:, :seq_len, :]\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"max_len\": self.max_len,\n            \"embed_dim\": self.embed_dim,\n        })\n        return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:01:24.544808Z","iopub.execute_input":"2025-04-23T17:01:24.545054Z","iopub.status.idle":"2025-04-23T17:01:24.552122Z","shell.execute_reply.started":"2025-04-23T17:01:24.545038Z","shell.execute_reply":"2025-04-23T17:01:24.551382Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"class Encoder(layers.Layer):\n    def __init__(self,embed_dim: int, num_heads: int, ff_dim: int, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim    = ff_dim\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.layer1 = layers.LayerNormalization()\n        self.layer2 = layers.LayerNormalization()\n        self.ffn =  tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n\n    \n    def call(self,pos_matrix, padding_mask, training=False, **kwargs):\n        att_out= self.attention(query=pos_matrix, value=pos_matrix, key=pos_matrix,attention_mask=padding_mask)\n        norm1 = self.layer1(att_out+pos_matrix)\n        ff_out = self.ffn(norm1, training=training)\n        return self.layer2(norm1 + ff_out, training=training)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n        })\n        return config\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:01:25.726693Z","iopub.execute_input":"2025-04-23T17:01:25.727265Z","iopub.status.idle":"2025-04-23T17:01:25.733102Z","shell.execute_reply.started":"2025-04-23T17:01:25.727242Z","shell.execute_reply":"2025-04-23T17:01:25.732553Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"class Decoder(layers.Layer):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, **kwargs):\n        super().__init__(**kwargs)\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim    = ff_dim\n        self.self_mha  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.cross_mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn       = tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.norm1 = layers.LayerNormalization()\n        self.norm2 = layers.LayerNormalization()\n        self.norm3 = layers.LayerNormalization()\n\n    def call(self, x, enc_out,\n             look_ahead_mask=None,\n             padding_mask=None,\n             training=False):\n        att1 = self.self_mha(\n            query=x, value=x, key=x,\n            attention_mask=look_ahead_mask,\n            training=training\n        )\n        out1 = self.norm1(x + att1, training=training)\n\n        \n        att2 = self.cross_mha(\n            query=out1, value=enc_out, key=enc_out,\n            attention_mask=padding_mask,\n            training=training\n        )\n        out2 = self.norm2(out1 + att2, training=training)\n\n        \n        ffn_out = self.ffn(out2, training=training)\n        return self.norm3(out2 + ffn_out, training=training)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \n            \n        })\n        return config\n\n    def from_config(cls, config):\n        return cls(**config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:01:27.275739Z","iopub.execute_input":"2025-04-23T17:01:27.276006Z","iopub.status.idle":"2025-04-23T17:01:27.283213Z","shell.execute_reply.started":"2025-04-23T17:01:27.275986Z","shell.execute_reply":"2025-04-23T17:01:27.282607Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"\n\nclass Seq2SeqTransformer(tf.keras.Model):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim,\n        num_heads,\n        ff_dim,\n        max_enc_in_len,\n        max_a_len,\n        embedding_layer,\n        pad_token_id=0,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.max_enc_in_len = max_enc_in_len\n        self.max_a_len = max_a_len\n        self.pad_token_id = pad_token_id\n\n        if embedding_layer is None:\n            raise ValueError(\"`embedding_layer` must be provided\")\n        self.token_emb = embedding_layer\n        self.pos_emb_enc = PositionalEmbedding(max_enc_in_len, embed_dim)\n        self.pos_emb_dec = PositionalEmbedding(max_a_len, embed_dim)\n        self.encoder1 = Encoder(embed_dim, num_heads, ff_dim)\n        self.encoder2 = Encoder(embed_dim, num_heads, ff_dim)\n        self.decoder1 = Decoder(embed_dim, num_heads, ff_dim)\n        self.decoder2 = Decoder(embed_dim, num_heads, ff_dim)\n        self.final_dense = layers.Dense(vocab_size, activation=\"softmax\")\n\n    def create_padding_mask(self, seq):\n        mask = tf.equal(seq, self.pad_token_id)\n        return mask[:, None, None, :]\n\n    def create_look_ahead_mask(self, size):\n        return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n\n    def call(self, inputs, training=False):\n        enc_seq, dec_seq = inputs\n        dec_len = tf.shape(dec_seq)[1]\n        enc_pad = self.create_padding_mask(enc_seq)\n        dec_pad_bool = tf.equal(dec_seq, self.pad_token_id)\n        look2d = tf.cast(self.create_look_ahead_mask(dec_len), tf.bool)\n        dec_pad_3d = dec_pad_bool[:, :, None]  \n        look3d     = look2d[None, :, :]         \n\n        self_attn  = tf.logical_or(dec_pad_3d, look3d)  \n\n        enc_x = self.pos_emb_enc(self.token_emb(enc_seq))\n        x = self.encoder1(enc_x, padding_mask=enc_pad, training=training)\n        x = self.encoder2(x, padding_mask=enc_pad, training=training)\n\n        dec_x = self.pos_emb_dec(self.token_emb(dec_seq))\n        y = self.decoder1(\n            dec_x,\n            x,\n            look_ahead_mask=self_attn,\n            padding_mask=enc_pad,\n            training=training\n        )\n        y = self.decoder2(\n            y,\n            x,\n            look_ahead_mask=self_attn,\n            padding_mask=enc_pad,\n            training=training\n        )\n        return self.final_dense(y)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"vocab_size\": self.vocab_size,\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"ff_dim\": self.ff_dim,\n            \"max_enc_in_len\": self.max_enc_in_len,\n            \"max_a_len\": self.max_a_len,\n            \"pad_token_id\": self.pad_token_id,\n            \"embedding_layer\": serialize_keras_object(self.token_emb),\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        emb_conf = config.pop(\"embedding_layer\")\n        embedding_layer = deserialize_keras_object(\n            emb_conf,\n            module_objects=globals(),\n            custom_objects={\"Embedding\": Embedding}\n        )\n        return cls(embedding_layer=embedding_layer, **config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:27:38.306716Z","iopub.execute_input":"2025-04-23T17:27:38.307201Z","iopub.status.idle":"2025-04-23T17:27:38.318501Z","shell.execute_reply.started":"2025-04-23T17:27:38.307178Z","shell.execute_reply":"2025-04-23T17:27:38.317927Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"model = Seq2SeqTransformer(\n    vocab_size=tokenizer_phase_2.num_words,\n    embed_dim=100,\n    num_heads=8,\n    ff_dim=512,\n    max_enc_in_len=MAX_ENCODER_LEN,\n    max_a_len=MAX_A_LEN, \n    embedding_layer=embedding_layer,\n    pad_token_id=0,\n)\n\ndummy_enc = tf.zeros((1, MAX_ENCODER_LEN), dtype=tf.int32)\ndummy_dec = tf.zeros((1,MAX_A_LEN  ), dtype=tf.int32)\n_ = model((dummy_enc, dummy_dec), training=False)\nmodel.summary()\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=[]    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:28:04.272211Z","iopub.execute_input":"2025-04-23T17:28:04.272522Z","iopub.status.idle":"2025-04-23T17:28:05.660495Z","shell.execute_reply.started":"2025-04-23T17:28:04.272501Z","shell.execute_reply":"2025-04-23T17:28:05.659986Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"seq2_seq_transformer_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2_seq_transformer_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                           │       \u001b[38;5;34m6,670,600\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_12              │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_13              │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder_12 (\u001b[38;5;33mEncoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m425,912\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder_13 (\u001b[38;5;33mEncoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m425,912\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_12 (\u001b[38;5;33mDecoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m748,612\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_13 (\u001b[38;5;33mDecoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m748,612\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50000\u001b[0m)              │       \u001b[38;5;34m5,050,000\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ pretrained_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,670,600</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_12              │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ positional_embedding_13              │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">425,912</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ encoder_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">425,912</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">748,612</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">748,612</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,000</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,069,648\u001b[0m (53.67 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,069,648</span> (53.67 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,399,048\u001b[0m (28.23 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,399,048</span> (28.23 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,670,600\u001b[0m (25.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,670,600</span> (25.45 MB)\n</pre>\n"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"\ndef decode_batch(seqs, tokenizer, pad_id=0, sos_id=None, eos_id=None):\n    texts = tokenizer.sequences_to_texts(seqs)\n    clean_texts = []\n    for txt in texts:\n        tokens = txt.split()\n        tokens = [\n            t for t in tokens\n            if t not in {tokenizer.index_word.get(pad_id, \"\"), \n                         tokenizer.index_word.get(sos_id, \"\"), \n                         tokenizer.index_word.get(eos_id, \"\")}\n        ]\n        clean_texts.append(\" \".join(tokens))\n    return clean_texts\n\n\ndef greedy_decode_batch(model, enc_batch,\n                        max_len, bos_id, eos_id, pad_id):\n    \"\"\"\n    Autoregressive batch decoding.\n    enc_batch : (B, S) int32 – encoder tokens\n    returns   : np.ndarray (B, ≤max_len) without BOS\n    \"\"\"\n    dec = tf.fill([tf.shape(enc_batch)[0], 1], bos_id)\n\n    for _ in range(max_len):\n        logits  = model([enc_batch, dec], training=False)\n        next_id = tf.argmax(logits[:, -1, :],\n                            axis=-1,\n                            output_type=tf.int32)\n        dec = tf.concat([dec, next_id[:, None]], axis=1)\n\n        if tf.reduce_all(tf.equal(next_id, eos_id)):\n            break\n\n    return tf.where(dec[:, 1:] == eos_id, pad_id, dec[:, 1:]).numpy()\n\n\nclass QAEvalCallback(tf.keras.callbacks.Callback):\n    def __init__(self, val_ds, tokenizer,\n                 max_ans_len,\n                 pad_id=0, bos_id=None, eos_id=None):\n        super().__init__()\n        self.val_ds   = val_ds           \n        self.tok      = tokenizer\n        self.max_len  = max_ans_len\n        self.pad_id   = pad_id\n        self.bos_id   = bos_id\n        self.eos_id   = eos_id\n\n    def on_epoch_end(self, epoch, logs=None):\n        em_sum = f1_sum = n = 0\n\n        for (enc_batch, _), dec_tar_batch, _ in self.val_ds:\n            pred_ids = greedy_decode_batch(\n                self.model, enc_batch,\n                self.max_len, self.bos_id, self.eos_id, self.pad_id\n            )\n\n            preds = decode_batch(pred_ids, self.tok,\n                                 pad_id=self.pad_id,\n                                 sos_id=self.bos_id,\n                                 eos_id=self.eos_id)\n            reals = decode_batch(dec_tar_batch.numpy(), self.tok,\n                                 pad_id=self.pad_id,\n                                 sos_id=self.bos_id,\n                                 eos_id=self.eos_id)\n\n            for p, r in zip(preds, reals):\n                em_sum  += exact_match_score(p, r)\n                f1_sum  += f1_score(p, r)\n                n += 1\n\n        logs = logs or {}\n        logs[\"val_em\"] = 100 * em_sum / n\n        logs[\"val_f1\"] = 100 * f1_sum / n\n        print(f\" — val_EM: {logs['val_em']:.2f}%  — val_F1: {logs['val_f1']:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:28:25.279606Z","iopub.execute_input":"2025-04-23T17:28:25.280133Z","iopub.status.idle":"2025-04-23T17:28:25.290004Z","shell.execute_reply.started":"2025-04-23T17:28:25.280109Z","shell.execute_reply":"2025-04-23T17:28:25.289379Z"}},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":"### Model training","metadata":{}},{"cell_type":"code","source":"MODEL_DIR       = get_model_dir()\nCHECKPOINT_PATH = os.path.join(MODEL_DIR, 'best_transformer_1.keras')\n\ncheckpoint_cb = ModelCheckpoint(\n    filepath=CHECKPOINT_PATH,\n    monitor='val_f1',      \n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\nqa_eval_cb = QAEvalCallback(\n    val_ds,\n    tokenizer_phase_2,\n    max_ans_len=MAX_A_LEN,\n    pad_id=pad_id,\n    bos_id=sos_id,\n    eos_id=eos_id\n)\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[qa_eval_cb,checkpoint_cb]\n)\n\nprint(f\"Best model will be saved to: {CHECKPOINT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:28:32.333717Z","iopub.execute_input":"2025-04-23T17:28:32.333976Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745429331.623369     108 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m210/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 1.1710","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745429399.185486     109 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 1.1705","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1745429406.976747     107 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nW0000 00:00:1745429410.126107     106 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":" — val_EM: 0.00%  — val_F1: 6.45%\n\nEpoch 1: val_f1 improved from -inf to 6.44719, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 367ms/step - loss: 1.1700 - val_loss: 0.9986 - val_em: 0.0000e+00 - val_f1: 6.4472\nEpoch 2/20\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 0.9362 — val_EM: 0.00%  — val_F1: 6.45%\n\nEpoch 2: val_f1 did not improve from 6.44719\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 317ms/step - loss: 0.9362 - val_loss: 1.0145 - val_em: 0.0000e+00 - val_f1: 6.4472\nEpoch 3/20\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.9173 — val_EM: 0.00%  — val_F1: 6.45%\n\nEpoch 3: val_f1 did not improve from 6.44719\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 311ms/step - loss: 0.9173 - val_loss: 1.0322 - val_em: 0.0000e+00 - val_f1: 6.4472\nEpoch 4/20\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.9240 — val_EM: 0.00%  — val_F1: 6.45%\n\nEpoch 4: val_f1 did not improve from 6.44719\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 309ms/step - loss: 0.9240 - val_loss: 1.0401 - val_em: 0.0000e+00 - val_f1: 6.4472\nEpoch 5/20\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.9214 — val_EM: 0.00%  — val_F1: 6.45%\n\nEpoch 5: val_f1 did not improve from 6.44719\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 308ms/step - loss: 0.9214 - val_loss: 1.0495 - val_em: 0.0000e+00 - val_f1: 6.4472\nEpoch 6/20\n\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.9215","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"best_model = load_model(\n    '/kaggle/working/models/best_transformer_1.keras',\n    custom_objects={\n        \"Seq2SeqTransformer\": Seq2SeqTransformer,\n        \"PositionalEmbedding\": PositionalEmbedding,\n        \"Encoder\": Encoder,\n        \"Decoder\": Decoder,\n        \"Embedding\": Embedding,   \n    }\n)\n\nbest_model.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[]\n)\n\n\nhistory = best_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=4,\n    callbacks=[qa_eval_cb, checkpoint_cb]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:02:15.905754Z","iopub.execute_input":"2025-04-23T01:02:15.906004Z","iopub.status.idle":"2025-04-23T01:04:20.458650Z","shell.execute_reply.started":"2025-04-23T01:02:15.905979Z","shell.execute_reply":"2025-04-23T01:04:20.457878Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/4\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 0.0131 — val_EM: 0.00%  — val_F1: 10.74%\n\nEpoch 1: val_f1 did not improve from 11.21321\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 224ms/step - loss: 0.0132 - val_loss: 0.4020 - val_em: 0.0000e+00 - val_f1: 10.7399\nEpoch 2/4\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0126 — val_EM: 0.00%  — val_F1: 11.47%\n\nEpoch 2: val_f1 improved from 11.21321 to 11.46981, saving model to /kaggle/working/models/best_transformer_1.keras\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 155ms/step - loss: 0.0126 - val_loss: 0.4201 - val_em: 0.0000e+00 - val_f1: 11.4698\nEpoch 3/4\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0074 — val_EM: 0.00%  — val_F1: 11.02%\n\nEpoch 3: val_f1 did not improve from 11.46981\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 150ms/step - loss: 0.0074 - val_loss: 0.4338 - val_em: 0.0000e+00 - val_f1: 11.0240\nEpoch 4/4\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0109 — val_EM: 0.00%  — val_F1: 11.40%\n\nEpoch 4: val_f1 did not improve from 11.46981\n\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 151ms/step - loss: 0.0109 - val_loss: 0.4366 - val_em: 0.0000e+00 - val_f1: 11.3962\n","output_type":"stream"}],"execution_count":322},{"cell_type":"code","source":"\n\nprint(\"  • # train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\nprint(\"  • # val   batches:\", tf.data.experimental.cardinality(val_ds).numpy())\n\n\nfor (enc_batch, decin_batch), dec_tar_batch, weight_batch in train_ds.take(1):\n    print(\"Encoder shape:\", enc_batch.shape)       # (B, enc_len)\n    print(\"Decoder‑in shape:\", decin_batch.shape)  # (B, ans_len)\n    print(\"Decoder‑tar shape:\", dec_tar_batch.shape)\n    print(\"Weights shape:\", weight_batch.shape)\n    \n    enc_ids = enc_batch[0].numpy().tolist()\n    decin_ids = decin_batch[0].numpy().tolist()\n    detar_ids = dec_tar_batch[0].numpy().tolist()\n    \n    \n    enc_ids = [i for i in enc_ids if i != pad_id]\n    decin_ids = [i for i in decin_ids if i not in (pad_id, sos_id, eos_id)]\n    detar_ids = [i for i in detar_ids if i not in (pad_id, sos_id, eos_id)]\n    \n    q_c_text = tokenizer_phase_2.sequences_to_texts([enc_ids])[0]\n    inp_ans  = tokenizer_phase_2.sequences_to_texts([decin_ids])[0]\n    tar_ans  = tokenizer_phase_2.sequences_to_texts([detar_ids])[0]\n    \n    print(\"\\nSample #1\")\n    print(\"  Q+SEP+C →\", q_c_text[:200] + \"…\")\n    print(\"  Decoder‑in →\", inp_ans)\n    print(\"  Decoder‑tar→\", tar_ans)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:07:35.705787Z","iopub.status.idle":"2025-04-23T01:07:35.705996Z","shell.execute_reply.started":"2025-04-23T01:07:35.705899Z","shell.execute_reply":"2025-04-23T01:07:35.705908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef show_errors_greedy(model, dataset, tokenizer, max_encoder_len, max_answer_len, sep_id, pad_id, sos_id, eos_id, n=5):\n    it = dataset.as_numpy_iterator()\n    samples = random.sample(list(it), n)\n    for (enc, _), dec_tar, _ in samples:\n        enc_row = [i for i in enc[0].tolist() if i != pad_id]\n        sep_index = enc_row.index(sep_id)\n        q_ids = enc_row[:sep_index]\n        c_ids = enc_row[sep_index+1:]\n        q_txt = tokenizer.sequences_to_texts([q_ids])[0]\n        c_txt = tokenizer.sequences_to_texts([c_ids])[0]\n        dec_input = [sos_id]\n        for _ in range(max_answer_len):\n            dec_pad = pad_sequences([dec_input], maxlen=max_answer_len, padding=\"post\", value=pad_id)\n            logits = model((enc, dec_pad), training=False)\n            next_id = int(tf.argmax(logits[0, len(dec_input)-1]).numpy())\n            if next_id == eos_id:\n                break\n            dec_input.append(next_id)\n        pred_ids = [i for i in dec_input[1:] if i not in (pad_id, sos_id, eos_id)]\n        true_ids = [i for i in dec_tar[0].tolist() if i not in (pad_id, sos_id, eos_id)]\n        pred_txt = tokenizer.sequences_to_texts([pred_ids])[0]\n        true_txt = tokenizer.sequences_to_texts([true_ids])[0]\n        print(f\"Q:  {q_txt!r}\")\n        print(f\"C:  {c_txt[:200]!r}…\")\n        print(f\"GT → {true_txt!r}\")\n        print(f\"PR → {pred_txt!r}\")\n        print(\"-\"*80)\n\nshow_errors_greedy(model, val_ds, tokenizer_phase_2, MAX_ENCODER_LEN, MAX_A_LEN-1, sep_id, pad_id, sos_id, eos_id, n=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:35:03.054103Z","iopub.execute_input":"2025-04-23T00:35:03.054811Z","iopub.status.idle":"2025-04-23T00:35:03.659306Z","shell.execute_reply.started":"2025-04-23T00:35:03.054783Z","shell.execute_reply":"2025-04-23T00:35:03.658682Z"}},"outputs":[{"name":"stdout","text":"Q:  'which criteria is used to rank the clubs'\nC:  'there are 20 clubs in the premier league during the course of a season from august to may each club plays the others twice a double round robin system once at their home stadium and once at that of th'…\nGT → 't'\nPR → ''\n--------------------------------------------------------------------------------\nQ:  'who coordinates the study program of samskritam as a foreign language'\nC:  'st james junior school in london england offers sanskrit as part of the curriculum in the united states since september 2009 high school students have been able to receive credits as independent study'…\nGT → 's'\nPR → ''\n--------------------------------------------------------------------------------\nQ:  'when did taiwanese hokkien have a fast change in development'\nC:  'in the 1990s marked by the liberalization of language development and mother tongue movement in taiwan taiwanese hokkien had undergone a fast pace in its development in 1993 taiwan became the first re'…\nGT → 'i'\nPR → ''\n--------------------------------------------------------------------------------\nQ:  'what was the primary reason for india children being employed'\nC:  'on 23 june 1757 the english east india company defeated siraj ud daula the nawab of bengal in the battle of plassey the british thus became masters of east india bengal bihar orissa – a prosperous reg'…\nGT → 'l'\nPR → ''\n--------------------------------------------------------------------------------\nQ:  'who was elected as chief minister of the state of india'\nC:  'governments have seen alternates between bharatiya janata party bjp and indian national congress inc no third front ever has become significant in 2003 the state legislative assembly was won by the in'…\nGT → 'v'\nPR → ''\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":202},{"cell_type":"code","source":"all_answers = []\nfor seq in decoder_targets[:500]:\n    clean_ids = [i for i in seq if i not in (pad_id, sos_id, eos_id)]\n    text = tokenizer_phase_2.sequences_to_texts([clean_ids])[0]\n    all_answers.append(text)\n\nanswer_counts = Counter(all_answers)\ndf_top = pd.DataFrame(answer_counts.items(), columns=['answer', 'count']) \\\n    .sort_values('count', ascending=False) \\\n    .reset_index(drop=True)\n\nprint(df_top.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:08:57.976073Z","iopub.execute_input":"2025-04-23T01:08:57.976813Z","iopub.status.idle":"2025-04-23T01:08:58.025150Z","shell.execute_reply.started":"2025-04-23T01:08:57.976790Z","shell.execute_reply":"2025-04-23T01:08:58.024407Z"}},"outputs":[{"name":"stdout","text":"                                               answer  count\n0                                                  45      2\n1                                                1988      2\n2                                               1 000      2\n3                                               india      2\n4                                           the bible      2\n5                                                2010      2\n6                                               eight      2\n7                                            couplets      1\n8                digitize and offer nara video online      1\n9                          banking financial services      1\n10            pollen either fails to reach the stigma      1\n11            the communist party of the soviet union      1\n12                                               rich      1\n13                                      louis agassiz      1\n14                                                 56      1\n15                                    research center      1\n16  used it as part of the modern gay rights movem...      1\n17                                         10 550 350      1\n18                                            halifax      1\n19                          bolivia romania and italy      1\n","output_type":"stream"}],"execution_count":326},{"cell_type":"markdown","source":"## Post Processing","metadata":{}},{"cell_type":"code","source":"def greedy_decode_contextual(model,\n                             question: str,\n                             context: str,\n                             tokenizer,\n                             max_encoder_len: int,\n                             max_answer_len: int,\n                             sep_token_id: int,\n                             pad_token_id: int,\n                             sos_token_id: int,\n                             eos_token_id: int):\n    q_ids = tokenizer.texts_to_sequences([question])[0]\n    c_ids = tokenizer.texts_to_sequences([context])[0]\n    enc_seq = q_ids + [sep_token_id] + c_ids\n    enc_input = pad_sequences(\n        [enc_seq],\n        maxlen=max_encoder_len,\n        padding='post',\n        truncating='post',\n        value=pad_token_id\n    )\n\n    dec_input = [sos_token_id]\n\n    allowed_set = set(c_ids) | {pad_token_id}\n\n    for t in range(max_answer_len):\n        # after first token, allow EOS too\n        if t > 0:\n            allowed_set.add(eos_token_id)\n\n        # build decoder input\n        dec_pad = pad_sequences(\n            [dec_input],\n            maxlen=max_answer_len,\n            padding='post',\n            truncating='post',\n            value=pad_token_id\n        )\n\n        preds = model((enc_input, dec_pad), training=False)          # shape (1, T, V)\n        last_logits = preds[0, len(dec_input) - 1]                  # shape (V,)\n\n        probs = tf.nn.softmax(last_logits).numpy()\n        mask = np.zeros_like(probs, dtype=bool)\n        mask[list(allowed_set)] = True\n        masked_probs = probs * mask\n\n        # 5) pick next token\n        next_id = int(np.argmax(masked_probs))\n        if next_id == eos_token_id:\n            break\n\n        dec_input.append(next_id)\n\n    decoded = [tokenizer.index_word.get(i, \"\") for i in dec_input[1:]]\n    return \" \".join(decoded)\n\n\n\nsos_id = tokenizer_phase_2.word_index[\"[SOS]\"]\neos_id = tokenizer_phase_2.word_index[\"[EOS]\"]\nsep_id = tokenizer_phase_2.word_index[\"[SEP]\"]\npad_id = 0\n\nquestion = \"What is the capital of France?\"\ncontext  = (\"France is a country in Western Europe. \"\n            \"Its capital city is Paris, known for the Eiffel Tower.\")\n\nanswer = greedy_decode_contextual(\n    best_model,\n    question,\n    context,\n    tokenizer_phase_2,\n    max_encoder_len=MAX_ENCODER_LEN,\n    max_answer_len=MAX_A_LEN-1,\n    sep_token_id=sep_id,\n    pad_token_id=pad_id,\n    sos_token_id=sos_id,\n    eos_token_id=eos_id\n)\n\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T02:00:04.772225Z","iopub.execute_input":"2025-04-23T02:00:04.772778Z","iopub.status.idle":"2025-04-23T02:00:04.950285Z","shell.execute_reply.started":"2025-04-23T02:00:04.772755Z","shell.execute_reply":"2025-04-23T02:00:04.949669Z"}},"outputs":[{"name":"stdout","text":"Answer: a\n","output_type":"stream"}],"execution_count":352},{"cell_type":"code","source":"question = \"What is my name?\"\ncontext = \"My name is ahmed and please my name is ahmed\"\n\nanswer2 = greedy_decode(\n    best_model,\n    question,\n    context,\n    tokenizer_phase_2,\n    max_encoder_len=MAX_ENCODER_LEN,\n    max_answer_len=MAX_A_LEN-1,\n    sep_token_id=sep_id,\n    pad_token_id=pad_id,\n    sos_token_id=sos_id,\n    eos_token_id=eos_id\n)\nprint(\"Answer: \", answer2 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:09:05.270839Z","iopub.execute_input":"2025-04-23T01:09:05.271426Z","iopub.status.idle":"2025-04-23T01:09:05.453410Z","shell.execute_reply.started":"2025-04-23T01:09:05.271406Z","shell.execute_reply":"2025-04-23T01:09:05.452691Z"}},"outputs":[{"name":"stdout","text":"Answer:  bandurria\n","output_type":"stream"}],"execution_count":328},{"cell_type":"markdown","source":"## Inference using the best model","metadata":{}},{"cell_type":"code","source":"import evaluate\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\n\ndev_ds = load_dataset(\"squad_v2\", split=\"validation\")\n\nmetric = evaluate.load(\"squad_v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:13:53.128336Z","iopub.execute_input":"2025-04-23T01:13:53.128664Z","iopub.status.idle":"2025-04-23T01:14:04.001725Z","shell.execute_reply.started":"2025-04-23T01:13:53.128639Z","shell.execute_reply":"2025-04-23T01:14:04.001174Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c357eca672e44c1bc5b99a4ba47e739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/11.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31aeb948e26c49ce8b703624de690b9f"}},"metadata":{}}],"execution_count":337},{"cell_type":"code","source":"predictions = []\nreferences  = []\n\nfor ex in tqdm(dev_ds, desc=\"Decoding & collecting\"):\n    pred = greedy_decode(\n        best_model,\n        ex[\"question\"],\n        ex[\"context\"],\n        tokenizer_phase_2,\n        max_encoder_len=MAX_ENCODER_LEN,\n        max_answer_len=MAX_A_LEN-1,\n        sep_token_id=sep_id,\n        pad_token_id=pad_id,\n        sos_token_id=sos_id,\n        eos_token_id=eos_id,\n    )\n    predictions.append({\"id\": ex[\"id\"], \"prediction_text\": pred})\n    references.append({\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]})\n\nresults = metric.compute(predictions=predictions, references=references)\nprint(f\"Exact Match = {results['exact']:.2f}%\")\nprint(f\"F1 Score     = {results['f1']:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:14:26.410252Z","iopub.execute_input":"2025-04-23T01:14:26.411216Z","iopub.status.idle":"2025-04-23T01:48:24.992738Z","shell.execute_reply.started":"2025-04-23T01:14:26.411190Z","shell.execute_reply":"2025-04-23T01:48:24.991532Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Decoding & collecting:   0%|          | 0/11873 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c87a84847a845249bf3a9cb2626b637"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/512162432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exact Match = {results['exact']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"F1 Score     = {results['f1']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_enforce_nested_string_type\u001b[0;34m(self, schema, obj)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# Nested structures: we allow dict, list, tuples, sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# Nested structures: we allow dict, list, tuples, sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mzip_dict\u001b[0;34m(*dicts)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'no_answer_probability'"],"ename":"KeyError","evalue":"'no_answer_probability'","output_type":"error"}],"execution_count":338},{"cell_type":"code","source":"for pred in predictions:\n    pred.setdefault(\"no_answer_probability\", 0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:51:15.011485Z","iopub.execute_input":"2025-04-23T01:51:15.012033Z","iopub.status.idle":"2025-04-23T01:51:15.018561Z","shell.execute_reply.started":"2025-04-23T01:51:15.012008Z","shell.execute_reply":"2025-04-23T01:51:15.017808Z"}},"outputs":[],"execution_count":341},{"cell_type":"code","source":"results = metric.compute(predictions=predictions, references=references)\nprint(f\"Exact Match = {results['exact']:.2f}%\")\nprint(f\"F1 Score    = {results['f1']:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T01:51:21.501499Z","iopub.execute_input":"2025-04-23T01:51:21.502008Z","iopub.status.idle":"2025-04-23T01:51:22.534542Z","shell.execute_reply.started":"2025-04-23T01:51:21.501983Z","shell.execute_reply":"2025-04-23T01:51:22.533769Z"}},"outputs":[{"name":"stdout","text":"Exact Match = 0.00%\nF1 Score    = 0.00%\n","output_type":"stream"}],"execution_count":342},{"cell_type":"code","source":"predictions[0:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T02:00:42.362295Z","iopub.execute_input":"2025-04-23T02:00:42.362874Z","iopub.status.idle":"2025-04-23T02:00:42.367467Z","shell.execute_reply.started":"2025-04-23T02:00:42.362852Z","shell.execute_reply":"2025-04-23T02:00:42.366833Z"}},"outputs":[{"execution_count":355,"output_type":"execute_result","data":{"text/plain":"[{'id': '56ddde6b9a695914005b9628',\n  'prediction_text': 'bandurria',\n  'no_answer_probability': 0.0},\n {'id': '56ddde6b9a695914005b9629',\n  'prediction_text': 'bandurria',\n  'no_answer_probability': 0.0},\n {'id': '56ddde6b9a695914005b962a',\n  'prediction_text': 'bandurria',\n  'no_answer_probability': 0.0},\n {'id': '56ddde6b9a695914005b962b',\n  'prediction_text': 'bandurria',\n  'no_answer_probability': 0.0},\n {'id': '56ddde6b9a695914005b962c',\n  'prediction_text': 'bandurria',\n  'no_answer_probability': 0.0}]"},"metadata":{}}],"execution_count":355},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}