{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNMvLH9BB7pR"
   },
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:07:36.186240Z",
     "iopub.status.busy": "2025-04-23T01:07:36.185968Z",
     "iopub.status.idle": "2025-04-23T01:07:36.191503Z",
     "shell.execute_reply": "2025-04-23T01:07:36.190763Z",
     "shell.execute_reply.started": "2025-04-23T01:07:36.186220Z"
    },
    "id": "kFomRI3xB9d8",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (c:\\Users\\Ahmed Labib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ahmed Labib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (c:\\Users\\Ahmed Labib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import os, zipfile , json , random, requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import  Sequential\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zzFTlcTCEc7"
   },
   "source": [
    "## Explorting dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:44:01.356367Z",
     "iopub.status.busy": "2025-04-23T00:44:01.356105Z",
     "iopub.status.idle": "2025-04-23T00:44:01.370238Z",
     "shell.execute_reply": "2025-04-23T00:44:01.369579Z",
     "shell.execute_reply.started": "2025-04-23T00:44:01.356345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     12\u001b[0m             drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_root\u001b[39m(start: \u001b[43mPath\u001b[49m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[0;32m     16\u001b[0m     p \u001b[38;5;241m=\u001b[39m Path(start \u001b[38;5;129;01mor\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mcwd())\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_kaggle():\n",
    "    # Kaggle kernels always set this env var\n",
    "    return 'KAGGLE_URL_BASE' in os.environ\n",
    "\n",
    "def is_colab():\n",
    "    return (not is_kaggle()) and os.path.exists('/content')\n",
    "\n",
    "def maybe_mount_drive():\n",
    "    if is_colab():\n",
    "        from google.colab import drive\n",
    "        if not os.path.isdir('/content/drive'):\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "def project_root(start: Path | None = None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / \".git\").exists() or any((p / d).exists() for d in [\"milestone1\", \"milestone2\", \"milestone3\"]):\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "def get_data_path():\n",
    "    if is_kaggle():\n",
    "        return '/kaggle/input/squad-2-0/'\n",
    "    elif is_colab():\n",
    "        return '/content/drive/MyDrive/SQuAD'\n",
    "    else:\n",
    "        return str(project_root() / 'data') + os.sep\n",
    "    \n",
    "\n",
    "def get_model_dir():\n",
    "    if is_colab():\n",
    "        model_dir = '/content/drive/MyDrive/models'\n",
    "    elif is_kaggle():\n",
    "        model_dir = '/kaggle/working/models'\n",
    "    else:\n",
    "        model_dir = str(project_root() / 'models')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:44:01.371057Z",
     "iopub.status.busy": "2025-04-23T00:44:01.370847Z",
     "iopub.status.idle": "2025-04-23T00:44:01.394936Z",
     "shell.execute_reply": "2025-04-23T00:44:01.394475Z",
     "shell.execute_reply.started": "2025-04-23T00:44:01.371035Z"
    },
    "id": "e0NuNGE435ad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = get_data_path()\n",
    "maybe_mount_drive()\n",
    "os.makedirs(dataset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:44:01.396638Z",
     "iopub.status.busy": "2025-04-23T00:44:01.396443Z",
     "iopub.status.idle": "2025-04-23T00:44:01.399840Z",
     "shell.execute_reply": "2025-04-23T00:44:01.399121Z",
     "shell.execute_reply.started": "2025-04-23T00:44:01.396606Z"
    },
    "id": "TSDZT50PX9sj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(dataset_dir, 'train-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:44:01.400648Z",
     "iopub.status.busy": "2025-04-23T00:44:01.400426Z",
     "iopub.status.idle": "2025-04-23T00:44:01.870088Z",
     "shell.execute_reply": "2025-04-23T00:44:01.869122Z",
     "shell.execute_reply.started": "2025-04-23T00:44:01.400608Z"
    },
    "id": "s_tEP-KyHMFT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:39.073613Z",
     "iopub.status.busy": "2025-04-23T00:50:39.073369Z",
     "iopub.status.idle": "2025-04-23T00:50:39.415173Z",
     "shell.execute_reply": "2025-04-23T00:50:39.414397Z",
     "shell.execute_reply.started": "2025-04-23T00:50:39.073594Z"
    },
    "id": "JixYG4s6saGU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for article in squad['data']:\n",
    "    for para in article['paragraphs']:\n",
    "        ctx = para['context']\n",
    "        for qa in para['qas']:\n",
    "            answers = [a['text'] for a in qa.get('answers', [])]\n",
    "            starts  = [a['answer_start'] for a in qa.get('answers', [])]\n",
    "            ends    = [s + len(t) for s,t in zip(starts, answers)]\n",
    "            records.append({\n",
    "                'question': qa['question'],\n",
    "                'answers': answers,\n",
    "                'context': ctx,\n",
    "                'answer_start': starts,\n",
    "                'answer_end': ends\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:40.295574Z",
     "iopub.status.busy": "2025-04-23T00:50:40.295298Z",
     "iopub.status.idle": "2025-04-23T00:50:40.434191Z",
     "shell.execute_reply": "2025-04-23T00:50:40.433529Z",
     "shell.execute_reply.started": "2025-04-23T00:50:40.295554Z"
    },
    "id": "NhZlSX4qtmtj",
    "outputId": "96567ffd-6526-4218-c322-84c26cd2e7e6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>[in the late 1990s]</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>[269]</td>\n",
       "      <td>[286]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>[singing and dancing]</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>[207]</td>\n",
       "      <td>[226]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>[2003]</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>[526]</td>\n",
       "      <td>[530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>[Houston, Texas]</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>[166]</td>\n",
       "      <td>[180]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>[late 1990s]</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>[276]</td>\n",
       "      <td>[286]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                answers  \\\n",
       "0           When did Beyonce start becoming popular?    [in the late 1990s]   \n",
       "1  What areas did Beyonce compete in when she was...  [singing and dancing]   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 [2003]   \n",
       "3      In what city and state did Beyonce  grow up?        [Houston, Texas]   \n",
       "4         In which decade did Beyonce become famous?           [late 1990s]   \n",
       "\n",
       "                                             context answer_start answer_end  \n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [269]      [286]  \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [207]      [226]  \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [526]      [530]  \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [166]      [180]  \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...        [276]      [286]  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:41.791933Z",
     "iopub.status.busy": "2025-04-23T00:50:41.791234Z",
     "iopub.status.idle": "2025-04-23T00:50:41.795693Z",
     "shell.execute_reply": "2025-04-23T00:50:41.794868Z",
     "shell.execute_reply.started": "2025-04-23T00:50:41.791911Z"
    },
    "id": "D72WTTHOupEF",
    "outputId": "6686b957-2f37-44bc-c831-47864e7260dd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs: 130319\n"
     ]
    }
   ],
   "source": [
    "print(\"Total QA pairs:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:42.628020Z",
     "iopub.status.busy": "2025-04-23T00:50:42.627441Z",
     "iopub.status.idle": "2025-04-23T00:50:42.706394Z",
     "shell.execute_reply": "2025-04-23T00:50:42.705680Z",
     "shell.execute_reply.started": "2025-04-23T00:50:42.627996Z"
    },
    "id": "rSzBSJjsx1MU",
    "outputId": "97a3a6d3-c217-4ab8-a1b7-edd234dca397",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: (15000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What year did the global recession that follow...</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>It threatened the collapse of large financial ...</td>\n",
       "      <td>[481]</td>\n",
       "      <td>[485]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what was a popular club in ibiza that started ...</td>\n",
       "      <td>[Amnesia]</td>\n",
       "      <td>But house was also being developed on Ibiza,[c...</td>\n",
       "      <td>[251]</td>\n",
       "      <td>[258]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In what century did Martin Luther honor Mary a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Although Calvin and Huldrych Zwingli honored M...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the climate like?</td>\n",
       "      <td>[varies from hot and subhumid tropical]</td>\n",
       "      <td>Due to extreme variation in elevation, great v...</td>\n",
       "      <td>[115]</td>\n",
       "      <td>[152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many times has the Queen toured Canada?</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Queen addressed the United Nations for a s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What year did the global recession that follow...   \n",
       "1  what was a popular club in ibiza that started ...   \n",
       "2  In what century did Martin Luther honor Mary a...   \n",
       "3                          What is the climate like?   \n",
       "4        How many times has the Queen toured Canada?   \n",
       "\n",
       "                                   answers  \\\n",
       "0                                   [2012]   \n",
       "1                                [Amnesia]   \n",
       "2                                       []   \n",
       "3  [varies from hot and subhumid tropical]   \n",
       "4                                       []   \n",
       "\n",
       "                                             context answer_start answer_end  \n",
       "0  It threatened the collapse of large financial ...        [481]      [485]  \n",
       "1  But house was also being developed on Ibiza,[c...        [251]      [258]  \n",
       "2  Although Calvin and Huldrych Zwingli honored M...           []         []  \n",
       "3  Due to extreme variation in elevation, great v...        [115]      [152]  \n",
       "4  The Queen addressed the United Nations for a s...           []         []  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# only working on subset of 15k row\n",
    "df_subset = df.head(15000).copy().reset_index(drop=True)\n",
    "\n",
    "print(\"Subset size:\", df_subset.shape)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EdBrPmqbBvA"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_Cb25_x11aT"
   },
   "source": [
    "Dropping rows where answers are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:45.054769Z",
     "iopub.status.busy": "2025-04-23T00:50:45.054189Z",
     "iopub.status.idle": "2025-04-23T00:50:45.069780Z",
     "shell.execute_reply": "2025-04-23T00:50:45.069056Z",
     "shell.execute_reply.started": "2025-04-23T00:50:45.054744Z"
    },
    "id": "JCoQdBYJ19ON",
    "outputId": "98bfb707-f5f5-4339-8879-dd7c8426a6d9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining after drop: 10020\n"
     ]
    }
   ],
   "source": [
    "df_subset = df_subset[df_subset['answers'].map(len) > 0].reset_index(drop=True)\n",
    "print(\"Rows remaining after drop:\", len(df_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBSLXMvASpe0"
   },
   "source": [
    "Removing Extra Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:46.600884Z",
     "iopub.status.busy": "2025-04-23T00:50:46.600597Z",
     "iopub.status.idle": "2025-04-23T00:50:46.605107Z",
     "shell.execute_reply": "2025-04-23T00:50:46.604103Z",
     "shell.execute_reply.started": "2025-04-23T00:50:46.600863Z"
    },
    "id": "FZAnlrcyStOE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collapse_whitespace(s):\n",
    "    if isinstance(s, str):\n",
    "        return re.sub(r'\\s+', ' ', s.strip())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:46.783173Z",
     "iopub.status.busy": "2025-04-23T00:50:46.782965Z",
     "iopub.status.idle": "2025-04-23T00:50:47.239025Z",
     "shell.execute_reply": "2025-04-23T00:50:47.238236Z",
     "shell.execute_reply.started": "2025-04-23T00:50:46.783156Z"
    },
    "id": "fnVh34G5VBAm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in ['question', 'context', 'answers']:\n",
    "    if col in df_subset.columns:\n",
    "        df_subset[col] = df_subset[col].apply(collapse_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFeXiO2dzf8Q"
   },
   "source": [
    "**Lets explore the length of the sequences which will determine some hyperparameters in training the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:47.240403Z",
     "iopub.status.busy": "2025-04-23T00:50:47.240206Z",
     "iopub.status.idle": "2025-04-23T00:50:47.248503Z",
     "shell.execute_reply": "2025-04-23T00:50:47.247951Z",
     "shell.execute_reply.started": "2025-04-23T00:50:47.240388Z"
    },
    "id": "Lc530Af3zoTV",
    "outputId": "f93ee1cc-11ca-4548-ea54-98ddafec6e05",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset['question'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:47.347889Z",
     "iopub.status.busy": "2025-04-23T00:50:47.347346Z",
     "iopub.status.idle": "2025-04-23T00:50:47.355750Z",
     "shell.execute_reply": "2025-04-23T00:50:47.355093Z",
     "shell.execute_reply.started": "2025-04-23T00:50:47.347872Z"
    },
    "id": "dDvLl2vx0a4o",
    "outputId": "103fac31-6acf-45c6-cf69-bb1656c1fe4c",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset['context'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hWOoWFg1GZk"
   },
   "source": [
    "We just turn the array of the answers to a string since none have multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:47.704846Z",
     "iopub.status.busy": "2025-04-23T00:50:47.704118Z",
     "iopub.status.idle": "2025-04-23T00:50:47.715262Z",
     "shell.execute_reply": "2025-04-23T00:50:47.714483Z",
     "shell.execute_reply.started": "2025-04-23T00:50:47.704821Z"
    },
    "id": "rh00m5Jw0lWm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_subset['answers']= df_subset['answers'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:47.879390Z",
     "iopub.status.busy": "2025-04-23T00:50:47.879132Z",
     "iopub.status.idle": "2025-04-23T00:50:47.889116Z",
     "shell.execute_reply": "2025-04-23T00:50:47.888565Z",
     "shell.execute_reply.started": "2025-04-23T00:50:47.879372Z"
    },
    "id": "WoL1ZTTx0z8b",
    "outputId": "e4736878-4cb3-4c68-df14-41efa24a60e8",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset['answers'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_-pDASk5eOa"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:48.996267Z",
     "iopub.status.busy": "2025-04-23T00:50:48.996010Z",
     "iopub.status.idle": "2025-04-23T00:50:51.976520Z",
     "shell.execute_reply": "2025-04-23T00:50:51.975474Z",
     "shell.execute_reply.started": "2025-04-23T00:50:48.996250Z"
    },
    "id": "gBEVZxrJ5gav",
    "outputId": "e51f32d7-1a78-4015-9a21-d4a580224374",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer for phase 1 only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:51.978729Z",
     "iopub.status.busy": "2025-04-23T00:50:51.978415Z",
     "iopub.status.idle": "2025-04-23T00:50:52.364005Z",
     "shell.execute_reply": "2025-04-23T00:50:52.363435Z",
     "shell.execute_reply.started": "2025-04-23T00:50:51.978706Z"
    },
    "id": "l2U-kh4D5mdK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer_phase_1 = Tokenizer(\n",
    "    num_words=20000,\n",
    "    oov_token='[UNK]',\n",
    "    filters='''!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'''\n",
    ")\n",
    "tokenizer_phase_1.fit_on_texts(df_subset[\"question\"].tolist()+df_subset[\"answers\"].tolist())\n",
    "\n",
    "q_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['question'])\n",
    "a_seqs = tokenizer_phase_1.texts_to_sequences(df_subset['answers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:52.365028Z",
     "iopub.status.busy": "2025-04-23T00:50:52.364787Z",
     "iopub.status.idle": "2025-04-23T00:50:52.369251Z",
     "shell.execute_reply": "2025-04-23T00:50:52.368448Z",
     "shell.execute_reply.started": "2025-04-23T00:50:52.365011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 18733\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer_phase_1.word_index)\n",
    "print(\"Total unique tokens:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer for phase 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:52.474153Z",
     "iopub.status.busy": "2025-04-23T00:50:52.473972Z",
     "iopub.status.idle": "2025-04-23T00:50:52.481044Z",
     "shell.execute_reply": "2025-04-23T00:50:52.480390Z",
     "shell.execute_reply.started": "2025-04-23T00:50:52.474134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def truncate_context(context: str, ans_start: int, ans_end: int, max_len: int) -> str:\n",
    "    \"\"\"\n",
    "    Return a substring of `context` of length up to max_len characters,\n",
    "    centered on the character span [ans_start:ans_end], adjusted to word boundaries.\n",
    "    \"\"\"\n",
    "    ans_len = ans_end - ans_start\n",
    "    extra   = max_len - ans_len\n",
    "    pre     = extra // 2\n",
    "    post    = extra - pre\n",
    "\n",
    "    # ideal window\n",
    "    start = ans_start - pre\n",
    "    end   = ans_end   + post\n",
    "\n",
    "    # shift if off left edge\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "        end   = min(max_len, len(context))\n",
    "\n",
    "    # shift if off right edge\n",
    "    if end > len(context):\n",
    "        end   = len(context)\n",
    "        start = max(0, len(context) - max_len)\n",
    "\n",
    "    # adjust start backward to nearest whitespace to avoid cutting a word\n",
    "    if start > 0 and not context[start].isspace():\n",
    "        m = re.search(r'\\s', context[:start][::-1])\n",
    "        if m:\n",
    "            # position of last whitespace before start\n",
    "            start = start - m.start()\n",
    "\n",
    "    # adjust end forward to nearest whitespace to avoid cutting a word\n",
    "    if end < len(context) and not context[end].isspace():\n",
    "        m = re.search(r'\\s', context[end:])\n",
    "        if m:\n",
    "            end = end + m.start()\n",
    "\n",
    "    # final slice\n",
    "    return context[start:end]\n",
    "\n",
    "def build_truncated_context(df, max_len: int):\n",
    "    \"\"\"\n",
    "    Returns a list of truncated context strings for each row in df,\n",
    "    preserving at least the answer span and cutting only at word boundaries.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    for ctx, starts, ends in zip(df['context'], df['answer_start'], df['answer_end']):\n",
    "        # pick the first span\n",
    "        s = starts[0]\n",
    "        e = ends[0]\n",
    "\n",
    "        window = truncate_context(ctx, s, e, max_len)\n",
    "        contexts.append(window)\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:53.914717Z",
     "iopub.status.busy": "2025-04-23T00:50:53.914349Z",
     "iopub.status.idle": "2025-04-23T00:50:55.102358Z",
     "shell.execute_reply": "2025-04-23T00:50:55.101646Z",
     "shell.execute_reply.started": "2025-04-23T00:50:53.914693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "questions = df_subset['question'].astype(str).tolist()\n",
    "answers   = df_subset['answers'].astype(str).tolist()\n",
    "contexts  = build_truncated_context(df_subset, 2000)\n",
    "tokenizer_phase_2 = Tokenizer(\n",
    "    num_words=50000,\n",
    "    oov_token='[UNK]',\n",
    "    filters='''!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'''\n",
    ")\n",
    "tokenizer_phase_2.fit_on_texts(questions + answers + contexts + ['[SEP]'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding sep sos and eos tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:55.103944Z",
     "iopub.status.busy": "2025-04-23T00:50:55.103634Z",
     "iopub.status.idle": "2025-04-23T00:50:55.111464Z",
     "shell.execute_reply": "2025-04-23T00:50:55.110663Z",
     "shell.execute_reply.started": "2025-04-23T00:50:55.103922Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SEP_TOKEN id = 49999\n",
      "✅ [SOS] injected at id 49998\n",
      "✅ [EOS] injected at id 49997\n",
      "Special IDs: {'SEP': 49999, 'SOS': 49998, 'EOS': 49997, 'PAD': 0}\n"
     ]
    }
   ],
   "source": [
    "sep_token = \"[SEP]\"\n",
    "sep_id = tokenizer_phase_2.word_index.get(sep_token)\n",
    "if sep_id is None:\n",
    "    sep_id = tokenizer_phase_2.num_words - 1\n",
    "    occupant = tokenizer_phase_2.index_word.get(sep_id)\n",
    "    if occupant:\n",
    "        del tokenizer_phase_2.word_index[occupant] \n",
    "    tokenizer_phase_2.word_index[sep_token] = sep_id\n",
    "    tokenizer_phase_2.index_word[sep_id]  = sep_token\n",
    "print(\"✅ SEP_TOKEN id =\", sep_id)\n",
    "\n",
    "\n",
    "for token, offset in {\"[SOS]\":1, \"[EOS]\":2}.items():\n",
    "    tok_id = tokenizer_phase_2.word_index.get(token)\n",
    "    if tok_id is None:\n",
    "        new_id = tokenizer_phase_2.num_words - offset - 1\n",
    "        occupant = tokenizer_phase_2.index_word.get(new_id)\n",
    "        if occupant:\n",
    "            del tokenizer_phase_2.word_index[occupant]\n",
    "        tokenizer_phase_2.word_index[token]      = new_id\n",
    "        tokenizer_phase_2.index_word[new_id]     = token\n",
    "        print(f\"✅ {token} injected at id {new_id}\")\n",
    "    else:\n",
    "        print(f\"✅ {token} already at id {tok_id}\")\n",
    "\n",
    "sep_id = tokenizer_phase_2.word_index['[SEP]']\n",
    "sos_id = tokenizer_phase_2.word_index['[SOS]']\n",
    "eos_id = tokenizer_phase_2.word_index['[EOS]']\n",
    "pad_id = 0\n",
    "\n",
    "print(\"Special IDs:\", {'SEP':sep_id, 'SOS':sos_id, 'EOS':eos_id, 'PAD':pad_id})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:55.591428Z",
     "iopub.status.busy": "2025-04-23T00:50:55.590761Z",
     "iopub.status.idle": "2025-04-23T00:50:56.378818Z",
     "shell.execute_reply": "2025-04-23T00:50:56.378099Z",
     "shell.execute_reply.started": "2025-04-23T00:50:55.591405Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: {'Q': 33, 'C': 374, 'A': 32, 'Enc': 408}\n"
     ]
    }
   ],
   "source": [
    "q_seqs = tokenizer_phase_2.texts_to_sequences(questions)\n",
    "c_seqs = tokenizer_phase_2.texts_to_sequences(contexts)\n",
    "a_raw  = tokenizer_phase_2.texts_to_sequences(answers)\n",
    "a_seqs = [[sos_id] + seq + [eos_id] for seq in a_raw]\n",
    "\n",
    "MAX_Q_LEN  = max(len(s) for s in q_seqs)\n",
    "MAX_C_LEN  = max(len(s) for s in c_seqs)\n",
    "MAX_A_LEN  = max(len(s) for s in a_seqs)\n",
    "MAX_ENCODER_LEN = MAX_Q_LEN + 1 + MAX_C_LEN\n",
    "\n",
    "print(\"Lengths:\", {'Q':MAX_Q_LEN, 'C':MAX_C_LEN, 'A':MAX_A_LEN, 'Enc':MAX_ENCODER_LEN})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add padding to the encoder and decoder inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:56.655205Z",
     "iopub.status.busy": "2025-04-23T00:50:56.654944Z",
     "iopub.status.idle": "2025-04-23T00:50:56.792353Z",
     "shell.execute_reply": "2025-04-23T00:50:56.791583Z",
     "shell.execute_reply.started": "2025-04-23T00:50:56.655184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enc_seqs = [q + [sep_id] + c for q, c in zip(q_seqs, c_seqs)]\n",
    "encoder_inputs = pad_sequences(\n",
    "    enc_seqs,\n",
    "    maxlen=MAX_ENCODER_LEN,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=pad_id\n",
    ")\n",
    "\n",
    "a_padded = pad_sequences(\n",
    "    a_seqs,\n",
    "    maxlen=MAX_A_LEN,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:57.943475Z",
     "iopub.status.busy": "2025-04-23T00:50:57.942790Z",
     "iopub.status.idle": "2025-04-23T00:50:57.955756Z",
     "shell.execute_reply": "2025-04-23T00:50:57.955129Z",
     "shell.execute_reply.started": "2025-04-23T00:50:57.943450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs: (10020, 408)\n",
      "decoder_inputs: (10020, 31)\n",
      "decoder_targets: (10020, 31)\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs  = a_padded[:, :-1]  # begins with SOS\n",
    "decoder_targets = a_padded[:,  1:]  # ends with EOS\n",
    "\n",
    "print(\"encoder_inputs:\", encoder_inputs.shape)\n",
    "print(\"decoder_inputs:\", decoder_inputs.shape)\n",
    "print(\"decoder_targets:\", decoder_targets.shape)\n",
    "assert decoder_inputs.shape == decoder_targets.shape\n",
    "\n",
    "(enc_tr, enc_val,\n",
    " decin_tr, decin_val,\n",
    " dectar_tr, dectar_val) = train_test_split(\n",
    "    encoder_inputs,\n",
    "    decoder_inputs,\n",
    "    decoder_targets,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:50:59.079201Z",
     "iopub.status.busy": "2025-04-23T00:50:59.078960Z",
     "iopub.status.idle": "2025-04-23T00:50:59.113208Z",
     "shell.execute_reply": "2025-04-23T00:50:59.112611Z",
     "shell.execute_reply.started": "2025-04-23T00:50:59.079185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token sequence:\n",
      "['who', 'joining', 'that', 'the', 'second', 'songs', 'deemed', 'bird', 'influenced', 'change', 'flow', 'species', 'signal', 'at', 'the', 'european', 'that', 'the', 'second', 'songs', 'capital', 'several', '[SEP]', 'the', 'capital', 'several', 'had', 'have', 'response', 'birds', 'most', 'afro', 'phanerozoic', 'to', 'mw', 'unit', 'of', 'benjamin', 'ice', '1950s', 'the', 'bird', 'influenced', 'their', 'system', 'threat', 'birds', 'most', 'grotius', 'and', 'd–log', 'hugo', 'prior', 'from', 'the', 'although', 'on', 'locations', 'of', 'benjamin', 'adjacent', 'to', 'its', 'important', 'follow', 'on', 'its', \"darwin's\", 'are', 'continental', 'to', 'the', 'jesus', 'of', 'census', 'and', 'the', '1', 'building', 'some', 'the', 'administration', 'paris', 'mm', 'of', 'the', 'bhaktapur', 'capital', 'or', 'was', 'basic', 'to', 'last', '2', 'a', 'steel', 'hymns', 'that', 'would', 'cover', 'as', 'a', 'appear', 'diffusive', 'arline', 'for', 'the', 'capital', 'certain', 'on', 'from', '85', 'in', 'the', 'bhaktapur', 'capital', 'the', '1935', 'of', 'the', 'bhaktapur', 'deemed', 'bird', 'influenced', 'greenbaum', 'operons', 'their', 'significantly', 'that', 'the', 'named', 'change', 'hewlett', 'the', 'bhaktapur', 'curated', 'european']\n",
      "\n",
      "Reconstructed text:\n",
      "who joining that the second songs deemed bird influenced change flow species signal at the european that the second songs capital several [SEP] the capital several had have response birds most afro phanerozoic to mw unit of benjamin ice 1950s the bird influenced their system threat birds most grotius and d–log hugo prior from the although on locations of benjamin adjacent to its important follow on its darwin's are continental to the jesus of census and the 1 building some the administration paris mm of the bhaktapur capital or was basic to last 2 a steel hymns that would cover as a appear diffusive arline for the capital certain on from 85 in the bhaktapur capital the 1935 of the bhaktapur deemed bird influenced greenbaum operons their significantly that the named change hewlett the bhaktapur curated european\n"
     ]
    }
   ],
   "source": [
    "# for (enc_batch, _), _, _ in train_ds.take(1):\n",
    "#     ids    = enc_batch[0].numpy()  # first example in the batch\n",
    "#     tokens = [tokenizer_phase_2.index_word.get(i, \"\") for i in ids]\n",
    "#     tokens = [t for t in tokens if t]\n",
    "#     print(\"Token sequence:\")\n",
    "#     print(tokens)\n",
    "#     print(\"\\nReconstructed text:\")\n",
    "#     print(\" \".join(tokens))\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEwBdMqoGBO_"
   },
   "source": [
    "**Load gloVe dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:21.674587Z",
     "iopub.status.busy": "2025-04-23T00:51:21.673988Z",
     "iopub.status.idle": "2025-04-23T00:51:21.682351Z",
     "shell.execute_reply": "2025-04-23T00:51:21.681676Z",
     "shell.execute_reply.started": "2025-04-23T00:51:21.674565Z"
    },
    "id": "sHHCMrjhGG4V",
    "outputId": "cc629348-918c-4ab8-85a2-0297f924a578",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe file: /kaggle/working/glove/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "def prepare_glove(target_dim=100, work_subdir='glove',\n",
    "                  input_dataset_slug='glove6b',\n",
    "                  download_url='http://nlp.stanford.edu/data/glove.6B.zip'):\n",
    "    maybe_mount_drive()\n",
    "\n",
    "    if is_kaggle():\n",
    "        work_dir = f'/kaggle/working/{work_subdir}'\n",
    "        uploaded_zip = f'/kaggle/input/{input_dataset_slug}/glove.6B.zip'\n",
    "    elif is_colab():\n",
    "        work_dir = f'/content/drive/MyDrive/{work_subdir}'\n",
    "        uploaded_zip = None\n",
    "    else:\n",
    "        work_dir = f'./data/{work_subdir}'\n",
    "        uploaded_zip = None\n",
    "\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "    target_file = f'glove.6B.{target_dim}d.txt'\n",
    "    txt_path = os.path.join(work_dir, target_file)\n",
    "    zip_path = os.path.join(work_dir, os.path.basename(download_url))\n",
    "\n",
    "    if os.path.exists(txt_path):\n",
    "        return txt_path\n",
    "\n",
    "    if is_kaggle() and uploaded_zip and os.path.exists(uploaded_zip):\n",
    "        zip_path = uploaded_zip\n",
    "    else:\n",
    "        if requests is None:\n",
    "            raise RuntimeError(\"`requests` not available; offline mode\")\n",
    "        with requests.get(download_url, stream=True) as r, open(zip_path, 'wb') as f:\n",
    "            r.raise_for_status()\n",
    "            for chunk in r.iter_content(8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extract(target_file, path=work_dir)\n",
    "\n",
    "    if not os.path.exists(txt_path):\n",
    "        raise RuntimeError(f\"Failed to extract {target_file}\")\n",
    "\n",
    "    return txt_path\n",
    "\n",
    "# Usage\n",
    "glove_path = prepare_glove()\n",
    "print(\"GloVe file:\", glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:23.621420Z",
     "iopub.status.busy": "2025-04-23T00:51:23.621166Z",
     "iopub.status.idle": "2025-04-23T00:51:23.628872Z",
     "shell.execute_reply": "2025-04-23T00:51:23.628103Z",
     "shell.execute_reply.started": "2025-04-23T00:51:23.621402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_layer(\n",
    "    tokenizer,\n",
    "    glove_path: str,\n",
    "    embedding_dim: int,\n",
    "    mask_zero: bool = True,\n",
    "    trainable: bool = False,\n",
    "    oov_token: str = '[UNK]'\n",
    ") -> Embedding:\n",
    "    \"\"\"\n",
    "    Build a Keras Embedding layer from a fitted tokenizer and a GloVe file.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: a fitted keras.preprocessing.text.Tokenizer\n",
    "        glove_path: path to a GloVe‑style file (word + embedding_dim floats)\n",
    "        max_num_words: max vocabulary size (typically tokenizer.num_words)\n",
    "        embedding_dim: dimensionality of the GloVe vectors\n",
    "        mask_zero: if True, reserve index 0 for padding (and mask it)\n",
    "        trainable: if False, freeze the embedding weights\n",
    "        oov_token: the out‑of‑vocab token (must match tokenizer.oov_token)\n",
    "\n",
    "    Returns:\n",
    "        A tf.keras.layers.Embedding instance with pretrained weights.\n",
    "    \"\"\"\n",
    "   \n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            word = parts[0]\n",
    "            coefs = np.asarray(parts[1:], dtype='float32')\n",
    "            if coefs.shape[0] != embedding_dim:\n",
    "                continue  # skip any lines that don't match expected dim\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    \n",
    "    vocab_size =  len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=0.01,\n",
    "        size=(vocab_size, embedding_dim)\n",
    "    ).astype('float32')\n",
    "\n",
    "    \n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == 0 or idx >= vocab_size:\n",
    "            continue\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        mask_zero=mask_zero,\n",
    "        trainable=trainable,\n",
    "        name='pretrained_embedding'\n",
    "    )\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci0NXaY-UBOi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.857583Z",
     "iopub.status.idle": "2025-04-23T00:30:00.857890Z",
     "shell.execute_reply": "2025-04-23T00:30:00.857754Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.857742Z"
    },
    "id": "XSOJYRpOKaqa",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_Q_LEN = max(len(s) for s in q_seqs)\n",
    "MAX_A_LEN = max(len(s) for s in a_seqs)\n",
    "MAX_C_LEN   = max(len(s) for s in trunc_c_seqs)\n",
    "MAX_ENCODER_LEN = max(len(s) for s in q_seqs) + 1 + MAX_C_TRUNC\n",
    "VOCAB_SIZE  = 20000\n",
    "EMB_DIM     = 100\n",
    "UNITS       = 128\n",
    "BATCH_SIZE  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.858973Z",
     "iopub.status.idle": "2025-04-23T00:30:00.859261Z",
     "shell.execute_reply": "2025-04-23T00:30:00.859127Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.859115Z"
    },
    "id": "JERoPRt-RHlC",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "q_padded = pad_sequences(q_seqs, maxlen=MAX_Q_LEN, padding='post', truncating='post')\n",
    "a_padded = pad_sequences(a_seqs, maxlen=MAX_A_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.860887Z",
     "iopub.status.idle": "2025-04-23T00:30:00.861195Z",
     "shell.execute_reply": "2025-04-23T00:30:00.861061Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.861048Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMB_DIM      = 100\n",
    "GLOVE_PATH   = prepare_glove()\n",
    "\n",
    "embedding_layer = create_embedding_layer(\n",
    "    tokenizer=tokenizer_phase_1,\n",
    "    glove_path=GLOVE_PATH,\n",
    "    embedding_dim=EMB_DIM,\n",
    "    mask_zero=True,      \n",
    "    trainable=False      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.861823Z",
     "iopub.status.idle": "2025-04-23T00:30:00.862111Z",
     "shell.execute_reply": "2025-04-23T00:30:00.861975Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.861962Z"
    },
    "id": "zaezBfWvSmaO",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decoder_input  = a_padded[:, :-1]\n",
    "decoder_target = a_padded[:, 1:]\n",
    "\n",
    "Xq_tr, Xq_val, Din_tr, Din_val, Dt_tr, Dt_val = train_test_split(\n",
    "    q_padded, decoder_input, decoder_target,\n",
    "    test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "def make_ds(q, d_in, d_tar, batch_size=64):\n",
    "    mask = tf.cast(tf.not_equal(d_tar, 0), tf.float32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((q, d_in), d_tar, mask)\n",
    "    )\n",
    "    return ds.shuffle(2000).batch(batch_size).prefetch(1)\n",
    "\n",
    "train_ds = make_ds(Xq_tr, Din_tr, Dt_tr, batch_size=BATCH_SIZE)\n",
    "val_ds   = make_ds(Xq_val, Din_val, Dt_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.863124Z",
     "iopub.status.idle": "2025-04-23T00:30:00.863348Z",
     "shell.execute_reply": "2025-04-23T00:30:00.863262Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.863252Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"Val batches:\", tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-DjaB2H1448"
   },
   "source": [
    "**Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.864321Z",
     "iopub.status.idle": "2025-04-23T00:30:00.864533Z",
     "shell.execute_reply": "2025-04-23T00:30:00.864441Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.864432Z"
    },
    "id": "XONLiGcUxUgA",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,emb_dim,units,max_q_len,max_a_len,embedding_matrix=None,pad_token_id=0,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = Embedding(vocab_size, emb_dim,weights=[embedding_matrix],trainable=False,mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = Embedding(vocab_size, emb_dim,mask_zero=True)\n",
    "\n",
    "        #units is the vector size of the hidden state\n",
    "        #return_state if true returns the final h and c\n",
    "        self.encoder_lstm = LSTM(units, return_state=True, name='encoder_lstm')\n",
    "\n",
    "        #return sequence returns all the hidden states from h_1 to h_n\n",
    "        #return sequence is for evaluation\n",
    "        #return state is for inference because after each token generated we need to feed the model the states again\n",
    "        self.decoder_lstm = LSTM(units,return_sequences=True,return_state=True, name='decoder_lstm')\n",
    "\n",
    "        #the layer needed to predict the next word\n",
    "        self.dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        x_enc = self.embedding(encoder_inputs)\n",
    "        _, state_h, state_c = self.encoder_lstm(x_enc, training=training)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        x_dec = self.embedding(decoder_inputs)\n",
    "        dec_outputs, _, _ = self.decoder_lstm(x_dec, initial_state=encoder_states, training=training)\n",
    "        return self.dense(dec_outputs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.865198Z",
     "iopub.status.idle": "2025-04-23T00:30:00.865474Z",
     "shell.execute_reply": "2025-04-23T00:30:00.865370Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.865357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqLSTM( vocab_size=vocab_size,\n",
    "                    emb_dim=100,units=128,\n",
    "                     max_q_len=MAX_Q_LEN,max_a_len=MAX_A_LEN,embedding_matrix=embedding_matrix,pad_token_id=0)\n",
    "\n",
    "dummy_q = tf.zeros((1, MAX_Q_LEN), dtype=tf.int32)\n",
    "dummy_a = tf.zeros((1, MAX_A_LEN-1), dtype=tf.int32)\n",
    "_ = model((dummy_q, dummy_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.866234Z",
     "iopub.status.idle": "2025-04-23T00:30:00.866541Z",
     "shell.execute_reply": "2025-04-23T00:30:00.866388Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.866374Z"
    },
    "id": "WY8ot7KK74I-",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "b3a84da1-e1f8-44e5-8cdf-ba203079b9be",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.867851Z",
     "iopub.status.idle": "2025-04-23T00:30:00.868047Z",
     "shell.execute_reply": "2025-04-23T00:30:00.867963Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.867955Z"
    },
    "id": "36vdUH2M4TGY",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "28b9f184-420b-4feb-f9fb-ce4118b6f98c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !mkdir /content/drive/MyDrive/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.status.busy": "2025-04-23T00:30:00.868692Z",
     "iopub.status.idle": "2025-04-23T00:30:00.868921Z",
     "shell.execute_reply": "2025-04-23T00:30:00.868834Z",
     "shell.execute_reply.started": "2025-04-23T00:30:00.868826Z"
    },
    "id": "C9i5PQ0q8AhI",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "32a5b44e-efa3-4617-cb3a-fac519494bfc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MODEL_DIR = get_model_dir()\n",
    "# CHECKPOINT_PATH = os.path.join(MODEL_DIR, 'seq2seq_lstm_best.keras')\n",
    "\n",
    "# checkpoint_cb = ModelCheckpoint(\n",
    "#     filepath=CHECKPOINT_PATH,\n",
    "#     monitor='val_sparse_categorical_accuracy',\n",
    "#     save_best_only=True,\n",
    "#     mode='max',\n",
    "#     verbose=1\n",
    "# )\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks=[checkpoint_cb]\n",
    "# )\n",
    "# print(f\"Best model will be saved to: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 using a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:25.548718Z",
     "iopub.status.busy": "2025-04-23T00:51:25.548099Z",
     "iopub.status.idle": "2025-04-23T00:51:33.973847Z",
     "shell.execute_reply": "2025-04-23T00:51:33.973008Z",
     "shell.execute_reply.started": "2025-04-23T00:51:25.548692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMB_DIM      = 100\n",
    "GLOVE_PATH   = prepare_glove()\n",
    "\n",
    "embedding_layer = create_embedding_layer(\n",
    "    tokenizer=tokenizer_phase_2,\n",
    "    glove_path=GLOVE_PATH,\n",
    "    embedding_dim=EMB_DIM,\n",
    "    mask_zero=False,      \n",
    "    trainable=False      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:33.976572Z",
     "iopub.status.busy": "2025-04-23T00:51:33.976344Z",
     "iopub.status.idle": "2025-04-23T00:51:34.032505Z",
     "shell.execute_reply": "2025-04-23T00:51:34.031894Z",
     "shell.execute_reply.started": "2025-04-23T00:51:33.976556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • train batches: 141\n",
      "   • val batches:   16\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = 0  \n",
    "BATCH_SIZE= 64\n",
    "def make_ds(enc, decin, dectar, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(((enc, decin), dectar))\n",
    "    \n",
    "    \n",
    "    def add_sample_weight(inputs, target):\n",
    "        weights = tf.cast(tf.not_equal(target, PAD_ID), tf.float32)\n",
    "        return inputs, target, weights\n",
    "    \n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(2000)\n",
    "        .map(add_sample_weight, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "train_ds = make_ds(enc_tr, decin_tr, dectar_tr)\n",
    "val_ds   = make_ds(enc_val, decin_val, dectar_val)\n",
    "\n",
    "print(\"   • train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"   • val batches:  \", tf.data.experimental.cardinality(val_ds).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:34.033371Z",
     "iopub.status.busy": "2025-04-23T00:51:34.033155Z",
     "iopub.status.idle": "2025-04-23T00:51:34.039843Z",
     "shell.execute_reply": "2025-04-23T00:51:34.039083Z",
     "shell.execute_reply.started": "2025-04-23T00:51:34.033355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len: int, embed_dim: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # self.supports_masking = True\n",
    "        self.max_len   = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        pos = np.arange(max_len)[:, np.newaxis]                 \n",
    "        dim = np.arange(embed_dim)[np.newaxis, :]                \n",
    "        angle_rates = 1.0 / np.power(10000.0, (2 * (dim//2)) / embed_dim)\n",
    "        angle_rads  = pos * angle_rates                          \n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])        \n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])        \n",
    "        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_len\": self.max_len,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:34.041909Z",
     "iopub.status.busy": "2025-04-23T00:51:34.041662Z",
     "iopub.status.idle": "2025-04-23T00:51:34.056551Z",
     "shell.execute_reply": "2025-04-23T00:51:34.055895Z",
     "shell.execute_reply.started": "2025-04-23T00:51:34.041884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,embed_dim: int, num_heads: int, ff_dim: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim\n",
    "        )\n",
    "        self.layer1 = layers.LayerNormalization()\n",
    "        self.layer2 = layers.LayerNormalization()\n",
    "        self.ffn =  tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def call(self,pos_matrix, padding_mask, **kwargs):\n",
    "        att_out= self.attention(query=pos_matrix, value=pos_matrix, key=pos_matrix,attention_mask=padding_mask)\n",
    "        norm1 = self.layer1(att_out+pos_matrix)\n",
    "        ff_out = self.ffn(norm1)\n",
    "        return self.layer2(ff_out)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:34.057449Z",
     "iopub.status.busy": "2025-04-23T00:51:34.057203Z",
     "iopub.status.idle": "2025-04-23T00:51:34.071677Z",
     "shell.execute_reply": "2025-04-23T00:51:34.070982Z",
     "shell.execute_reply.started": "2025-04-23T00:51:34.057434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # no supports_masking = True\n",
    "        self.self_mha  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.cross_mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn       = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.norm3 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, enc_out,\n",
    "             look_ahead_mask=None,\n",
    "             padding_mask=None,\n",
    "             training=False):\n",
    "        # 1) Decoder self‑attention with your 3D look‑ahead+pad mask\n",
    "        att1 = self.self_mha(\n",
    "            query=x, value=x, key=x,\n",
    "            attention_mask=look_ahead_mask,\n",
    "            training=training\n",
    "        )\n",
    "        out1 = self.norm1(x + att1, training=training)\n",
    "\n",
    "        \n",
    "        att2 = self.cross_mha(\n",
    "            query=out1, value=enc_out, key=enc_out,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "        out2 = self.norm2(out1 + att2, training=training)\n",
    "\n",
    "        \n",
    "        ffn_out = self.ffn(out2, training=training)\n",
    "        return self.norm3(out2 + ffn_out, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"embedding_layer\": serialize_keras_object(self.token_emb)\n",
    "            \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \n",
    "        emb_conf = config.pop(\"embedding_layer\")\n",
    "        \n",
    "        embedding_layer = deserialize_keras_object(\n",
    "            emb_conf,\n",
    "            module_objects=globals(),       # globals() now contains Embedding\n",
    "            custom_objects=None\n",
    "        )\n",
    "        return cls(embedding_layer=embedding_layer, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:34.073220Z",
     "iopub.status.busy": "2025-04-23T00:51:34.072571Z",
     "iopub.status.idle": "2025-04-23T00:51:34.092224Z",
     "shell.execute_reply": "2025-04-23T00:51:34.091616Z",
     "shell.execute_reply.started": "2025-04-23T00:51:34.073200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import serialize_keras_object, deserialize_keras_object\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "class Seq2SeqTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        ff_dim,\n",
    "        max_enc_in_len,\n",
    "        max_a_len,\n",
    "        embedding_layer,\n",
    "        pad_token_id=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.max_enc_in_len = max_enc_in_len\n",
    "        self.max_a_len = max_a_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        if embedding_layer is None:\n",
    "            raise ValueError(\"`embedding_layer` must be provided\")\n",
    "        self.token_emb = embedding_layer\n",
    "        self.pos_emb_enc = PositionalEmbedding(max_enc_in_len, embed_dim)\n",
    "        self.pos_emb_dec = PositionalEmbedding(max_a_len, embed_dim)\n",
    "        self.encoder1 = Encoder(embed_dim, num_heads, ff_dim)\n",
    "        self.encoder2 = Encoder(embed_dim, num_heads, ff_dim)\n",
    "        self.decoder1 = Decoder(embed_dim, num_heads, ff_dim)\n",
    "        self.decoder2 = Decoder(embed_dim, num_heads, ff_dim)\n",
    "        self.final_dense = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.equal(seq, self.pad_token_id)\n",
    "        return mask[:, None, None, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        return 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_seq, dec_seq = inputs\n",
    "        dec_len = tf.shape(dec_seq)[1]\n",
    "        enc_pad = self.create_padding_mask(enc_seq)\n",
    "        dec_pad_bool = tf.equal(dec_seq, self.pad_token_id)\n",
    "        look2d = tf.cast(self.create_look_ahead_mask(dec_len), tf.bool)\n",
    "        dec_pad_3d = dec_pad_bool[:, None, :]\n",
    "        look3d = look2d[None, :, :]\n",
    "        self_attn = tf.logical_or(dec_pad_3d, look3d)\n",
    "\n",
    "        enc_x = self.pos_emb_enc(self.token_emb(enc_seq))\n",
    "        x = self.encoder1(enc_x, padding_mask=enc_pad, training=training)\n",
    "        x = self.encoder2(x, padding_mask=enc_pad, training=training)\n",
    "\n",
    "        dec_x = self.pos_emb_dec(self.token_emb(dec_seq))\n",
    "        y = self.decoder1(\n",
    "            dec_x,\n",
    "            x,\n",
    "            look_ahead_mask=self_attn,\n",
    "            padding_mask=enc_pad,\n",
    "            training=training\n",
    "        )\n",
    "        y = self.decoder2(\n",
    "            y,\n",
    "            x,\n",
    "            look_ahead_mask=self_attn,\n",
    "            padding_mask=enc_pad,\n",
    "            training=training\n",
    "        )\n",
    "        return self.final_dense(y)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"max_enc_in_len\": self.max_enc_in_len,\n",
    "            \"max_a_len\": self.max_a_len,\n",
    "            \"pad_token_id\": self.pad_token_id,\n",
    "            \"embedding_layer\": serialize_keras_object(self.token_emb),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        emb_conf = config.pop(\"embedding_layer\")\n",
    "        embedding_layer = deserialize_keras_object(\n",
    "            emb_conf,\n",
    "            module_objects=globals(),\n",
    "            custom_objects={\"Embedding\": Embedding}\n",
    "        )\n",
    "        return cls(embedding_layer=embedding_layer, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:36.647283Z",
     "iopub.status.busy": "2025-04-23T00:51:36.647013Z",
     "iopub.status.idle": "2025-04-23T00:51:36.674414Z",
     "shell.execute_reply": "2025-04-23T00:51:36.673680Z",
     "shell.execute_reply.started": "2025-04-23T00:51:36.647263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=tokenizer_phase_2.num_words,\n",
    "    embed_dim=100,\n",
    "    num_heads=8,\n",
    "    ff_dim=512,\n",
    "    max_enc_in_len=MAX_ENCODER_LEN,\n",
    "    max_a_len=MAX_A_LEN - 1, \n",
    "    embedding_layer=embedding_layer,\n",
    "    pad_token_id=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:49.245580Z",
     "iopub.status.busy": "2025-04-23T00:51:49.245255Z",
     "iopub.status.idle": "2025-04-23T00:51:49.252434Z",
     "shell.execute_reply": "2025-04-23T00:51:49.251666Z",
     "shell.execute_reply.started": "2025-04-23T00:51:49.245558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower, strip punctuation/extra whitespace, collapse to tokens.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def exact_match_score(pred, truth):\n",
    "    return int(normalize_answer(pred) == normalize_answer(truth))\n",
    "\n",
    "def f1_score(pred, truth):\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    truth_tokens = normalize_answer(truth).split()\n",
    "    if not pred_tokens or not truth_tokens:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "  \n",
    "    common = {}\n",
    "    for t in pred_tokens:\n",
    "        common[t] = common.get(t, 0) + 1\n",
    "    same = 0\n",
    "    for t in truth_tokens:\n",
    "        if common.get(t, 0) > 0:\n",
    "            same += 1\n",
    "            common[t] -= 1\n",
    "    if same == 0:\n",
    "        return 0.0\n",
    "    prec = same / len(pred_tokens)\n",
    "    rec  = same / len(truth_tokens)\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:50.613533Z",
     "iopub.status.busy": "2025-04-23T00:51:50.613009Z",
     "iopub.status.idle": "2025-04-23T00:51:50.621949Z",
     "shell.execute_reply": "2025-04-23T00:51:50.621038Z",
     "shell.execute_reply.started": "2025-04-23T00:51:50.613513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode_batch(seqs, tokenizer, pad_id=0, sos_id=None, eos_id=None):\n",
    "    texts = tokenizer.sequences_to_texts(seqs)\n",
    "    clean_texts = []\n",
    "    for txt in texts:\n",
    "        tokens = txt.split()\n",
    "        tokens = [\n",
    "            t for t in tokens\n",
    "            if t not in {tokenizer.index_word.get(pad_id, \"\"), \n",
    "                         tokenizer.index_word.get(sos_id, \"\"), \n",
    "                         tokenizer.index_word.get(eos_id, \"\")}\n",
    "        ]\n",
    "        clean_texts.append(\" \".join(tokens))\n",
    "    return clean_texts\n",
    "\n",
    "class QAEvalCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, tokenizer, pad_id=0, sos_id=None, eos_id=None):\n",
    "        super().__init__()\n",
    "        self.val_ds    = val_ds\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_id    = pad_id\n",
    "        self.sos_id    = sos_id\n",
    "        self.eos_id    = eos_id\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        total_em = 0.0\n",
    "        total_f1 = 0.0\n",
    "        count    = 0\n",
    "\n",
    "        for (enc_batch, dec_in_batch), dec_tar_batch, _ in self.val_ds:\n",
    "            logits = self.model((enc_batch, dec_in_batch), training=False)\n",
    "            pred_ids = tf.argmax(logits, axis=-1).numpy()\n",
    "\n",
    "            preds = decode_batch(pred_ids,\n",
    "                                 tokenizer=self.tokenizer,\n",
    "                                 pad_id=self.pad_id,\n",
    "                                 sos_id=self.sos_id,\n",
    "                                 eos_id=self.eos_id)\n",
    "            reals = decode_batch(dec_tar_batch.numpy(),\n",
    "                                 tokenizer=self.tokenizer,\n",
    "                                 pad_id=self.pad_id,\n",
    "                                 sos_id=self.sos_id,\n",
    "                                 eos_id=self.eos_id)\n",
    "\n",
    "            for p_str, r_str in zip(preds, reals):\n",
    "                total_em += exact_match_score(p_str, r_str)\n",
    "                total_f1 += f1_score(p_str, r_str)\n",
    "                count   += 1\n",
    "\n",
    "        val_em = 100.0 * total_em / count\n",
    "        val_f1 = 100.0 * total_f1 / count\n",
    "        logs[\"val_em\"] = val_em\n",
    "        logs[\"val_f1\"] = val_f1\n",
    "        print(f\" — val_EM: {val_em:.2f}%  — val_F1: {val_f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:51:54.052399Z",
     "iopub.status.busy": "2025-04-23T00:51:54.051847Z",
     "iopub.status.idle": "2025-04-23T00:51:55.444569Z",
     "shell.execute_reply": "2025-04-23T00:51:55.444078Z",
     "shell.execute_reply.started": "2025-04-23T00:51:54.052376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2_seq_transformer_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2_seq_transformer_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ positional_embedding_34              │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ positional_embedding_35              │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)                │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">425,912</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">425,912</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ decoder_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">748,612</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ decoder_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)                 │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">748,612</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_161 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,000</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                           │       \u001b[38;5;34m5,729,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ positional_embedding_34              │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ positional_embedding_35              │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)                │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_34 (\u001b[38;5;33mEncoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m425,912\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ encoder_35 (\u001b[38;5;33mEncoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m425,912\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ decoder_34 (\u001b[38;5;33mDecoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m748,612\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ decoder_35 (\u001b[38;5;33mDecoder\u001b[0m)                 │ ?                           │         \u001b[38;5;34m748,612\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_161 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m50000\u001b[0m)              │       \u001b[38;5;34m5,050,000\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,128,248</span> (50.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,128,248\u001b[0m (50.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,399,048</span> (28.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,399,048\u001b[0m (28.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,729,200</span> (21.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,729,200\u001b[0m (21.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_enc = tf.zeros((1, MAX_ENCODER_LEN), dtype=tf.int32)\n",
    "dummy_dec = tf.zeros((1,MAX_A_LEN - 1 ), dtype=tf.int32)\n",
    "_ = model((dummy_enc, dummy_dec), training=False)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[]    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T01:07:35.702924Z",
     "iopub.status.idle": "2025-04-23T01:07:35.703238Z",
     "shell.execute_reply": "2025-04-23T01:07:35.703095Z",
     "shell.execute_reply.started": "2025-04-23T01:07:35.703082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_DIR       = get_model_dir()\n",
    "CHECKPOINT_PATH = os.path.join(MODEL_DIR, 'best_transformer_1.keras')\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=CHECKPOINT_PATH,\n",
    "    monitor='val_f1',      \n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "qa_eval_cb = QAEvalCallback(\n",
    "    val_ds,\n",
    "    tokenizer_phase_2,\n",
    "    pad_id=pad_id,\n",
    "    sos_id=sos_id,\n",
    "    eos_id=eos_id\n",
    ")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[qa_eval_cb,checkpoint_cb]\n",
    ")\n",
    "\n",
    "print(f\"Best model will be saved to: {CHECKPOINT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:02:15.906004Z",
     "iopub.status.busy": "2025-04-23T01:02:15.905754Z",
     "iopub.status.idle": "2025-04-23T01:04:20.458650Z",
     "shell.execute_reply": "2025-04-23T01:04:20.457878Z",
     "shell.execute_reply.started": "2025-04-23T01:02:15.905979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 0.0131 — val_EM: 0.00%  — val_F1: 10.74%\n",
      "\n",
      "Epoch 1: val_f1 did not improve from 11.21321\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 224ms/step - loss: 0.0132 - val_loss: 0.4020 - val_em: 0.0000e+00 - val_f1: 10.7399\n",
      "Epoch 2/4\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0126 — val_EM: 0.00%  — val_F1: 11.47%\n",
      "\n",
      "Epoch 2: val_f1 improved from 11.21321 to 11.46981, saving model to /kaggle/working/models/best_transformer_1.keras\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 155ms/step - loss: 0.0126 - val_loss: 0.4201 - val_em: 0.0000e+00 - val_f1: 11.4698\n",
      "Epoch 3/4\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.0074 — val_EM: 0.00%  — val_F1: 11.02%\n",
      "\n",
      "Epoch 3: val_f1 did not improve from 11.46981\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 150ms/step - loss: 0.0074 - val_loss: 0.4338 - val_em: 0.0000e+00 - val_f1: 11.0240\n",
      "Epoch 4/4\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0109 — val_EM: 0.00%  — val_F1: 11.40%\n",
      "\n",
      "Epoch 4: val_f1 did not improve from 11.46981\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 151ms/step - loss: 0.0109 - val_loss: 0.4366 - val_em: 0.0000e+00 - val_f1: 11.3962\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(\n",
    "    '/kaggle/working/models/best_transformer_1.keras',\n",
    "    custom_objects={\n",
    "        \"Seq2SeqTransformer\": Seq2SeqTransformer,\n",
    "        \"PositionalEmbedding\": PositionalEmbedding,\n",
    "        \"Encoder\": Encoder,\n",
    "        \"Decoder\": Decoder,\n",
    "        \"Embedding\": Embedding,   \n",
    "    }\n",
    ")\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[]\n",
    ")\n",
    "\n",
    "\n",
    "history = best_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=4,\n",
    "    callbacks=[qa_eval_cb, checkpoint_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-23T01:07:35.705787Z",
     "iopub.status.idle": "2025-04-23T01:07:35.705996Z",
     "shell.execute_reply": "2025-04-23T01:07:35.705908Z",
     "shell.execute_reply.started": "2025-04-23T01:07:35.705899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"  • # train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"  • # val   batches:\", tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "\n",
    "for (enc_batch, decin_batch), dec_tar_batch, weight_batch in train_ds.take(1):\n",
    "    print(\"Encoder shape:\", enc_batch.shape)       # (B, enc_len)\n",
    "    print(\"Decoder‑in shape:\", decin_batch.shape)  # (B, ans_len)\n",
    "    print(\"Decoder‑tar shape:\", dec_tar_batch.shape)\n",
    "    print(\"Weights shape:\", weight_batch.shape)\n",
    "    \n",
    "    enc_ids = enc_batch[0].numpy().tolist()\n",
    "    decin_ids = decin_batch[0].numpy().tolist()\n",
    "    detar_ids = dec_tar_batch[0].numpy().tolist()\n",
    "    \n",
    "    \n",
    "    enc_ids = [i for i in enc_ids if i != pad_id]\n",
    "    decin_ids = [i for i in decin_ids if i not in (pad_id, sos_id, eos_id)]\n",
    "    detar_ids = [i for i in detar_ids if i not in (pad_id, sos_id, eos_id)]\n",
    "    \n",
    "    q_c_text = tokenizer_phase_2.sequences_to_texts([enc_ids])[0]\n",
    "    inp_ans  = tokenizer_phase_2.sequences_to_texts([decin_ids])[0]\n",
    "    tar_ans  = tokenizer_phase_2.sequences_to_texts([detar_ids])[0]\n",
    "    \n",
    "    print(\"\\nSample #1\")\n",
    "    print(\"  Q+SEP+C →\", q_c_text[:200] + \"…\")\n",
    "    print(\"  Decoder‑in →\", inp_ans)\n",
    "    print(\"  Decoder‑tar→\", tar_ans)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:35:03.054811Z",
     "iopub.status.busy": "2025-04-23T00:35:03.054103Z",
     "iopub.status.idle": "2025-04-23T00:35:03.659306Z",
     "shell.execute_reply": "2025-04-23T00:35:03.658682Z",
     "shell.execute_reply.started": "2025-04-23T00:35:03.054783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  'which criteria is used to rank the clubs'\n",
      "C:  'there are 20 clubs in the premier league during the course of a season from august to may each club plays the others twice a double round robin system once at their home stadium and once at that of th'…\n",
      "GT → 't'\n",
      "PR → ''\n",
      "--------------------------------------------------------------------------------\n",
      "Q:  'who coordinates the study program of samskritam as a foreign language'\n",
      "C:  'st james junior school in london england offers sanskrit as part of the curriculum in the united states since september 2009 high school students have been able to receive credits as independent study'…\n",
      "GT → 's'\n",
      "PR → ''\n",
      "--------------------------------------------------------------------------------\n",
      "Q:  'when did taiwanese hokkien have a fast change in development'\n",
      "C:  'in the 1990s marked by the liberalization of language development and mother tongue movement in taiwan taiwanese hokkien had undergone a fast pace in its development in 1993 taiwan became the first re'…\n",
      "GT → 'i'\n",
      "PR → ''\n",
      "--------------------------------------------------------------------------------\n",
      "Q:  'what was the primary reason for india children being employed'\n",
      "C:  'on 23 june 1757 the english east india company defeated siraj ud daula the nawab of bengal in the battle of plassey the british thus became masters of east india bengal bihar orissa – a prosperous reg'…\n",
      "GT → 'l'\n",
      "PR → ''\n",
      "--------------------------------------------------------------------------------\n",
      "Q:  'who was elected as chief minister of the state of india'\n",
      "C:  'governments have seen alternates between bharatiya janata party bjp and indian national congress inc no third front ever has become significant in 2003 the state legislative assembly was won by the in'…\n",
      "GT → 'v'\n",
      "PR → ''\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_errors_greedy(model, dataset, tokenizer, max_encoder_len, max_answer_len, sep_id, pad_id, sos_id, eos_id, n=5):\n",
    "    it = dataset.as_numpy_iterator()\n",
    "    samples = random.sample(list(it), n)\n",
    "    for (enc, _), dec_tar, _ in samples:\n",
    "        enc_row = [i for i in enc[0].tolist() if i != pad_id]\n",
    "        sep_index = enc_row.index(sep_id)\n",
    "        q_ids = enc_row[:sep_index]\n",
    "        c_ids = enc_row[sep_index+1:]\n",
    "        q_txt = tokenizer.sequences_to_texts([q_ids])[0]\n",
    "        c_txt = tokenizer.sequences_to_texts([c_ids])[0]\n",
    "        dec_input = [sos_id]\n",
    "        for _ in range(max_answer_len):\n",
    "            dec_pad = pad_sequences([dec_input], maxlen=max_answer_len, padding=\"post\", value=pad_id)\n",
    "            logits = model((enc, dec_pad), training=False)\n",
    "            next_id = int(tf.argmax(logits[0, len(dec_input)-1]).numpy())\n",
    "            if next_id == eos_id:\n",
    "                break\n",
    "            dec_input.append(next_id)\n",
    "        pred_ids = [i for i in dec_input[1:] if i not in (pad_id, sos_id, eos_id)]\n",
    "        true_ids = [i for i in dec_tar[0].tolist() if i not in (pad_id, sos_id, eos_id)]\n",
    "        pred_txt = tokenizer.sequences_to_texts([pred_ids])[0]\n",
    "        true_txt = tokenizer.sequences_to_texts([true_ids])[0]\n",
    "        print(f\"Q:  {q_txt!r}\")\n",
    "        print(f\"C:  {c_txt[:200]!r}…\")\n",
    "        print(f\"GT → {true_txt!r}\")\n",
    "        print(f\"PR → {pred_txt!r}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "show_errors_greedy(model, val_ds, tokenizer_phase_2, MAX_ENCODER_LEN, MAX_A_LEN-1, sep_id, pad_id, sos_id, eos_id, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:08:57.976813Z",
     "iopub.status.busy": "2025-04-23T01:08:57.976073Z",
     "iopub.status.idle": "2025-04-23T01:08:58.025150Z",
     "shell.execute_reply": "2025-04-23T01:08:58.024407Z",
     "shell.execute_reply.started": "2025-04-23T01:08:57.976790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               answer  count\n",
      "0                                                  45      2\n",
      "1                                                1988      2\n",
      "2                                               1 000      2\n",
      "3                                               india      2\n",
      "4                                           the bible      2\n",
      "5                                                2010      2\n",
      "6                                               eight      2\n",
      "7                                            couplets      1\n",
      "8                digitize and offer nara video online      1\n",
      "9                          banking financial services      1\n",
      "10            pollen either fails to reach the stigma      1\n",
      "11            the communist party of the soviet union      1\n",
      "12                                               rich      1\n",
      "13                                      louis agassiz      1\n",
      "14                                                 56      1\n",
      "15                                    research center      1\n",
      "16  used it as part of the modern gay rights movem...      1\n",
      "17                                         10 550 350      1\n",
      "18                                            halifax      1\n",
      "19                          bolivia romania and italy      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Suppose decoder_targets is your unbatched numpy array of shape (N, seq_len)\n",
    "# You padded to MAX_A_LEN, so decoder_targets exists as detar before train_test_split.\n",
    "all_answers = []\n",
    "for seq in decoder_targets[:500]:\n",
    "    clean_ids = [i for i in seq if i not in (pad_id, sos_id, eos_id)]\n",
    "    text = tokenizer_phase_2.sequences_to_texts([clean_ids])[0]\n",
    "    all_answers.append(text)\n",
    "\n",
    "answer_counts = Counter(all_answers)\n",
    "df_top = pd.DataFrame(answer_counts.items(), columns=['answer', 'count']) \\\n",
    "    .sort_values('count', ascending=False) \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "print(df_top.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:00:04.772778Z",
     "iopub.status.busy": "2025-04-23T02:00:04.772225Z",
     "iopub.status.idle": "2025-04-23T02:00:04.950285Z",
     "shell.execute_reply": "2025-04-23T02:00:04.949669Z",
     "shell.execute_reply.started": "2025-04-23T02:00:04.772755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode_contextual(model,\n",
    "                             question: str,\n",
    "                             context: str,\n",
    "                             tokenizer,\n",
    "                             max_encoder_len: int,\n",
    "                             max_answer_len: int,\n",
    "                             sep_token_id: int,\n",
    "                             pad_token_id: int,\n",
    "                             sos_token_id: int,\n",
    "                             eos_token_id: int):\n",
    "    # 1) build encoder input\n",
    "    q_ids = tokenizer.texts_to_sequences([question])[0]\n",
    "    c_ids = tokenizer.texts_to_sequences([context])[0]\n",
    "    enc_seq = q_ids + [sep_token_id] + c_ids\n",
    "    enc_input = pad_sequences(\n",
    "        [enc_seq],\n",
    "        maxlen=max_encoder_len,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=pad_token_id\n",
    "    )\n",
    "\n",
    "    # 2) start decoding\n",
    "    dec_input = [sos_token_id]\n",
    "\n",
    "    # only context tokens + PAD are allowed at first step\n",
    "    allowed_set = set(c_ids) | {pad_token_id}\n",
    "\n",
    "    for t in range(max_answer_len):\n",
    "        # after first token, allow EOS too\n",
    "        if t > 0:\n",
    "            allowed_set.add(eos_token_id)\n",
    "\n",
    "        # build decoder input\n",
    "        dec_pad = pad_sequences(\n",
    "            [dec_input],\n",
    "            maxlen=max_answer_len,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=pad_token_id\n",
    "        )\n",
    "\n",
    "        # 3) run model and grab the logits for the last position\n",
    "        preds = model((enc_input, dec_pad), training=False)          # shape (1, T, V)\n",
    "        last_logits = preds[0, len(dec_input) - 1]                  # shape (V,)\n",
    "\n",
    "        # 4) mask out everything but allowed_set\n",
    "        probs = tf.nn.softmax(last_logits).numpy()\n",
    "        mask = np.zeros_like(probs, dtype=bool)\n",
    "        mask[list(allowed_set)] = True\n",
    "        masked_probs = probs * mask\n",
    "\n",
    "        # 5) pick next token\n",
    "        next_id = int(np.argmax(masked_probs))\n",
    "        if next_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "        dec_input.append(next_id)\n",
    "\n",
    "    # 6) convert IDs back to tokens\n",
    "    decoded = [tokenizer.index_word.get(i, \"\") for i in dec_input[1:]]\n",
    "    return \" \".join(decoded)\n",
    "\n",
    "\n",
    "# usage\n",
    "sos_id = tokenizer_phase_2.word_index[\"[SOS]\"]\n",
    "eos_id = tokenizer_phase_2.word_index[\"[EOS]\"]\n",
    "sep_id = tokenizer_phase_2.word_index[\"[SEP]\"]\n",
    "pad_id = 0\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "context  = (\"France is a country in Western Europe. \"\n",
    "            \"Its capital city is Paris, known for the Eiffel Tower.\")\n",
    "\n",
    "answer = greedy_decode_contextual(\n",
    "    best_model,\n",
    "    question,\n",
    "    context,\n",
    "    tokenizer_phase_2,\n",
    "    max_encoder_len=MAX_ENCODER_LEN,\n",
    "    max_answer_len=MAX_A_LEN-1,\n",
    "    sep_token_id=sep_id,\n",
    "    pad_token_id=pad_id,\n",
    "    sos_token_id=sos_id,\n",
    "    eos_token_id=eos_id\n",
    ")\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:09:05.271426Z",
     "iopub.status.busy": "2025-04-23T01:09:05.270839Z",
     "iopub.status.idle": "2025-04-23T01:09:05.453410Z",
     "shell.execute_reply": "2025-04-23T01:09:05.452691Z",
     "shell.execute_reply.started": "2025-04-23T01:09:05.271406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  bandurria\n"
     ]
    }
   ],
   "source": [
    "question = \"What is my name?\"\n",
    "context = \"My name is ahmed and please my name is ahmed\"\n",
    "\n",
    "answer2 = greedy_decode(\n",
    "    best_model,\n",
    "    question,\n",
    "    context,\n",
    "    tokenizer_phase_2,\n",
    "    max_encoder_len=MAX_ENCODER_LEN,\n",
    "    max_answer_len=MAX_A_LEN-1,\n",
    "    sep_token_id=sep_id,\n",
    "    pad_token_id=pad_id,\n",
    "    sos_token_id=sos_id,\n",
    "    eos_token_id=eos_id\n",
    ")\n",
    "print(\"Answer: \", answer2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:13:53.128664Z",
     "iopub.status.busy": "2025-04-23T01:13:53.128336Z",
     "iopub.status.idle": "2025-04-23T01:14:04.001725Z",
     "shell.execute_reply": "2025-04-23T01:14:04.001174Z",
     "shell.execute_reply.started": "2025-04-23T01:13:53.128639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c357eca672e44c1bc5b99a4ba47e739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31aeb948e26c49ce8b703624de690b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dev_ds = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:14:26.411216Z",
     "iopub.status.busy": "2025-04-23T01:14:26.410252Z",
     "iopub.status.idle": "2025-04-23T01:48:24.992738Z",
     "shell.execute_reply": "2025-04-23T01:48:24.991532Z",
     "shell.execute_reply.started": "2025-04-23T01:14:26.411190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c87a84847a845249bf3a9cb2626b637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding & collecting:   0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'no_answer_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/512162432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exact Match = {results['exact']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"F1 Score     = {results['f1']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_enforce_nested_string_type\u001b[0;34m(self, schema, obj)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# Nested structures: we allow dict, list, tuples, sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# Nested structures: we allow dict, list, tuples, sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mzip_dict\u001b[0;34m(*dicts)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'no_answer_probability'"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references  = []\n",
    "\n",
    "for ex in tqdm(dev_ds, desc=\"Decoding & collecting\"):\n",
    "    pred = greedy_decode(\n",
    "        best_model,\n",
    "        ex[\"question\"],\n",
    "        ex[\"context\"],\n",
    "        tokenizer_phase_2,\n",
    "        max_encoder_len=MAX_ENCODER_LEN,\n",
    "        max_answer_len=MAX_A_LEN-1,\n",
    "        sep_token_id=sep_id,\n",
    "        pad_token_id=pad_id,\n",
    "        sos_token_id=sos_id,\n",
    "        eos_token_id=eos_id,\n",
    "    )\n",
    "    predictions.append({\"id\": ex[\"id\"], \"prediction_text\": pred})\n",
    "    references.append({\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]})\n",
    "\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(f\"Exact Match = {results['exact']:.2f}%\")\n",
    "print(f\"F1 Score     = {results['f1']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:51:15.012033Z",
     "iopub.status.busy": "2025-04-23T01:51:15.011485Z",
     "iopub.status.idle": "2025-04-23T01:51:15.018561Z",
     "shell.execute_reply": "2025-04-23T01:51:15.017808Z",
     "shell.execute_reply.started": "2025-04-23T01:51:15.012008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for pred in predictions:\n",
    "    pred.setdefault(\"no_answer_probability\", 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:51:21.502008Z",
     "iopub.status.busy": "2025-04-23T01:51:21.501499Z",
     "iopub.status.idle": "2025-04-23T01:51:22.534542Z",
     "shell.execute_reply": "2025-04-23T01:51:22.533769Z",
     "shell.execute_reply.started": "2025-04-23T01:51:21.501983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match = 0.00%\n",
      "F1 Score    = 0.00%\n"
     ]
    }
   ],
   "source": [
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(f\"Exact Match = {results['exact']:.2f}%\")\n",
    "print(f\"F1 Score    = {results['f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T02:00:42.362874Z",
     "iopub.status.busy": "2025-04-23T02:00:42.362295Z",
     "iopub.status.idle": "2025-04-23T02:00:42.367467Z",
     "shell.execute_reply": "2025-04-23T02:00:42.366833Z",
     "shell.execute_reply.started": "2025-04-23T02:00:42.362852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56ddde6b9a695914005b9628',\n",
       "  'prediction_text': 'bandurria',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'id': '56ddde6b9a695914005b9629',\n",
       "  'prediction_text': 'bandurria',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'id': '56ddde6b9a695914005b962a',\n",
       "  'prediction_text': 'bandurria',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'id': '56ddde6b9a695914005b962b',\n",
       "  'prediction_text': 'bandurria',\n",
       "  'no_answer_probability': 0.0},\n",
       " {'id': '56ddde6b9a695914005b962c',\n",
       "  'prediction_text': 'bandurria',\n",
       "  'no_answer_probability': 0.0}]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7201839,
     "sourceId": 11489407,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7210129,
     "sourceId": 11500742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
